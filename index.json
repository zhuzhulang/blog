[{"authors":["码力全开"],"categories":["AI"],"content":"下面来学习Llama3.2的视觉多模态模型,\n首先来看其预处理过程,Llama3.2 Vision中1个tile大小为560x560。1个图片最多可以占4个tile。根据不同的长宽比,一共有8种不同的组合方式,对应不同的图像分辨率。\n同一张图片不同尺寸下适配到的tile是不一样的,我们使用掩码进行表示,占用该tile块则对应值为1,反之为0。\n对于4x3x560x560的图片,将其按照尺寸40x40划分为14x14的patch,通过线性层投射3x14x14展平为1280维,之后通过4x1280x1600,再为图片添加POS尾部嵌入编码,其一共有8种组合方式,对应8种不同可学习的方法,从而学习到不同patch的位置关系。紧接着添加1个分类的头,从而其形状变为4x1601x1280。\n再为其添加位置编码,将4个tile的token一字排开成为一个6404x1280的序列,输入32层视觉编码器模块。输出的张量形状不变,再添加尾部POS嵌入,再经过8层视觉编码器。\n将其最后一层输出作为全局特征,合并前面3,7,15,23,30层局部特征,得到最终形状4x1601x7680。其中4为tile数量,1601为每个tile里的patch数量,7680为每个patch的特征维度。\n参考视频:\nhttps://www.bilibili.com/video/BV1g47nzTEAY?p=7\n","date":1768361216,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1768397332,"objectID":"ad51f54cb75c07edf22f3472d365d6c0","permalink":"https://zhuzhulang.github.io/blog/posts/llama3.2-vision/","publishdate":"2026-01-14T11:26:56+08:00","relpermalink":"/posts/llama3.2-vision/","section":"posts","summary":"对Llama3.2视觉模型进行简单的介绍","tags":["AI"],"title":"Llama3.2视觉模型","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"LLaVA模型使用最简单的模型结构,最少的数据却达到最好的效果。该模型名称是Large Language and Vision Assistant的缩写。\n作者通过Prompt工程,通过提供图片的描述信息和目标检测信息,用纯文本输入,用GPT 4生成更丰富更真实的数据,通过COCO数据集构造对话(58K)+详细描述(23K)+复杂推理)77K)共158K的数据集。\n其图像编码器使用的是ViT-L/14,而LLM使用的是Vicuna。其网络结构如下图所示:\n而多模态融合采用最简单的线性映射层。图片经过ViT后得到图片特征,经过线性投射后得到视觉token,再与文本token拼接从而形成多模态的输入。\n其训练过程可以分为两阶段:\n特征对齐的预训练 端到端微调 在第1阶段,冻结视觉编码器和LLM,仅训练线性投射层。采用CC3M数据集筛选至595K对图像-文本对,进行简单描述图片问答。 而在第2阶段,冻结视觉编码器,训练线性投影层和LLM,使用GPT 4生成158K视觉指令跟随数据。\n而LLaVA-1.5在LLaVA基础上进行更进一步的改进,其实1个13B模型,采用120万公开数据集,使用8个A100训练1天,在11个基准任务取得SOTA。在网络结构方面,将一层的线性层替换为了2层。\n参考视频:\nhttps://www.bilibili.com/video/BV1g47nzTEAY?p=6\n","date":1768353539,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1768355905,"objectID":"d4a71a219dd99cec6e2d493e76caafe2","permalink":"https://zhuzhulang.github.io/blog/posts/llava-network/","publishdate":"2026-01-14T09:18:59+08:00","relpermalink":"/posts/llava-network/","section":"posts","summary":"对LLaVA模型进行简单的介绍","tags":["AI"],"title":"LLaVA网络简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"BLIP是Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation中首字母的组合,即通过自举方式预训练的语言-图像模型,它统一了视觉-文本的理解和生成。\n它与之前ALBEF的同一个作者,因此其继承后者的很多思想。其解决了2个问题:\n训练1个模型既可以做检索又可以做生成 解决网络收集图文对数据中的噪声问题 怎么设计一种网络结构既可以有编码器又可以有解码器,可以同时做图文检索又可以做基于图片的文本生成任务。\n下面是其网络结构:\n其中左侧是基于ViT结构的图像编码器,最终获取图像的特征。接着是标准ITC任务架构,也是CLIP采用的架构。其是由一个图像编码器和单模态文本编码器构成。\n之后是标准ITM网络架构,其是由图像编码器和多模态文本编码器组成。最后是基于多模态文本解码器的LM架构。\nBLIP将上述3个模型同时进行训练,但部分参数是共享的。每个batch训练时,图像编码器训练时前向传播1次,单模态文本编码器、多模态文本编码器和多模态文本解码器分别在文本前面添加不同任务的token,各自前向传播1次。而同一个颜色模块和共享的参数是一样的。\n对于互联网噪声数据,BLIP使用预训练的多模态解码器来判断图文是否匹配。训练的数据包含两部分,包括网络采集的图文对(质量较差)以及人工标注的图文对(质量较好)从而预训练1个模型。之后用ITC和ITM在人工标注的高质量数据集上训练1个Filter模型。同时运用上述这2个数据集训练1个Captioner模型。\n参考视频:\nhttps://www.bilibili.com/video/BV1hwLEzZEnS?p=17\n","date":1768289323,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1768306103,"objectID":"bbc3d10ce9042c65e27bc0685f4ac169","permalink":"https://zhuzhulang.github.io/blog/posts/blip-network/","publishdate":"2026-01-13T15:28:43+08:00","relpermalink":"/posts/blip-network/","section":"posts","summary":"对BLIP网络进行简单的介绍","tags":["AI"],"title":"BLIP网络简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"下面对ALBEF网络进行简单的介绍,ALBEF是Align before Fuse的缩写,其是在融合前对齐并用动量蒸馏的办法进行视觉和语言的表示学习。\n多模态学习有什么下游任务,比如:\n文本-图像检索 图像问答 多模态推理 图像和文本多模态主要有3个部分组成:\n图像编码器 文本编码器 融合编码器 其可以根据各自的比重分为4种结构。\n其中ALBEF模型左侧是12层的图片编码器,而右侧是6层的文本编码器,通过提取特征后计算余弦相似度及图片与文本对比loss。\nALBEF借鉴了MoCo的思想引入了动量模型,其既有图片Embedding的参数也有文本Embedding的参数。\n以下是其网络结构图:\n可以看到,其融合部分,其在文本编码器6层的基础上又增加了6层,其中包括Cross Attention。而query来自文本编码器,K,V来自图片编码器的输出。\nITM用于文本与图片匹配任务,其是二分类的,MLM用于带掩码的语言模型,用于预测被遮住的标签是什么。\n在ALBEF模型中有3个loss,分别为ITC、ITM及MLM的Loss。另外其还提出了动量蒸馏的概念。\n参考视频:\nhttps://www.bilibili.com/video/BV1uMUDYnEKu\n","date":1768224604,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1768226198,"objectID":"62216d11cc9c336bbf20284a2f54c4d6","permalink":"https://zhuzhulang.github.io/blog/posts/albef-network/","publishdate":"2026-01-12T21:30:04+08:00","relpermalink":"/posts/albef-network/","section":"posts","summary":"对ALBEF网络进行简单的介绍","tags":["AI"],"title":"ALBEF网络简述","type":"posts"},{"authors":["码力全开"],"categories":[],"content":"Vision Transformer是用Transformer架构解决视觉问题。其可以在不做改动来解决计算机视觉问题。\n其在小规模数据上略输卷积神经网络,而在中等或者大规模数据集上,表现相当或者优于卷积神经网络。在计算效率上,训练同等精度的模型,Transformer模型比卷积神经网络模型更有优势。\n首先将图片按照固定大小分为一个个patch作为token。一个patch如何作为Embedding向量呢?\n可以将这个图像的长乘以宽再乘以通道数的多维矩阵表示展平,再通过一个共享线性层投射到Transformer模型特征维度就完成1个图片转换为向量序列。\n图片切片相当于文本中分词,而线性层相当于Embedding。接下来需要考虑位置编码。\n在ViT中把位置编码为可学习向量,直接加在图像Token向量上。其参考BERT模型,在前面添加1个可学习的分类头。后面用的是Transformer Encoder,因此每个Token都可以看到所有Token。\n其网络结构如下图所示:\n其模型类型可分为3种:\nViT-Base ViT-Large ViT-Huge 其中ViT-L/16表示使用的是ViT-Large模型,patch尺寸为16x16。其中patch size越小,序列越长,计算代价越大。\n下面介绍下图像转化为Embedding序列的两种实现方式:\n训练图片大小为224x224,patch大小为16x16,patch数量为14x14。而Transformer里的特征维度(Hidden Size)为1024。\n第一种方式是采用线性映射,将原始图片拆分为多个patch,对于每个patch,shape为(16,16,3),展开为1个长度为768的一维向量,然后通过一个共享的(768,1024)的线性层进行编码。\n第二种方式是卷积操作,直接对原始图片,定义1024个卷积核,每个卷积核大小为patch大小(16,16),步长为16,padding为valid。\n这两种操作是完全等价的。\n另外Encoder输出分类也有两种方法:\n增加一个分类头[cls]token,用最后的Encoder对这个token的输出提取全局信息 不增加token,用最后Encoder的所有token的向量的全局平均池化GAP提取全局信息 最后,原作者也在该网络上尝试自监督学习。其借鉴BERT,对50%的patch进行标记,其中的80%标记为可学习的[mask]标签,10%替换为其他patch的Embedding,剩余10%维持不变。\n而预测像素值时,原来是RGB数量255x255x255=16581375色转换为预测8x8x8=256色,效果非常不错。\n参考视频:\nhttps://www.bilibili.com/video/BV15RDtYqE4r\n","date":1768204746,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1768207451,"objectID":"4f46e68c9a88197c0c70fc271ee3220b","permalink":"https://zhuzhulang.github.io/blog/posts/vision-transformer/","publishdate":"2026-01-12T15:59:06+08:00","relpermalink":"/posts/vision-transformer/","section":"posts","summary":"Vision Transformer是用Transformer架构解决视觉问题。其可以在不做改动来解决计算机视觉问题。\n其在小规模数据上略输卷积神经网络,而在中等或者大规模数据集上,表现相当或者优于卷积神经网络。在计算效率上,训练同等精度的模型,Transformer模型比卷积神经网络模型更有优势。\n首先将图片按照固定大小分为一个个patch作为token。一个patch如何作为Embedding向量呢?\n可以将这个图像的长乘以宽再乘以通道数的多维矩阵表示展平,再通过一个共享线性层投射到Transformer模型特征维度就完成1个图片转换为向量序列。\n图片切片相当于文本中分词,而线性层相当于Embedding。接下来需要考虑位置编码。\n在ViT中把位置编码为可学习向量,直接加在图像Token向量上。其参考BERT模型,在前面添加1个可学习的分类头。后面用的是Transformer Encoder,因此每个Token都可以看到所有Token。\n其网络结构如下图所示:\n其模型类型可分为3种:\nViT-Base ViT-Large ViT-Huge 其中ViT-L/16表示使用的是ViT-Large模型,patch尺寸为16x16。其中patch size越小,序列越长,计算代价越大。\n下面介绍下图像转化为Embedding序列的两种实现方式:\n训练图片大小为224x224,patch大小为16x16,patch数量为14x14。而Transformer里的特征维度(Hidden Size)为1024。\n第一种方式是采用线性映射,将原始图片拆分为多个patch,对于每个patch,shape为(16,16,3),展开为1个长度为768的一维向量,然后通过一个共享的(768,1024)的线性层进行编码。\n第二种方式是卷积操作,直接对原始图片,定义1024个卷积核,每个卷积核大小为patch大小(16,16),步长为16,padding为valid。\n这两种操作是完全等价的。\n另外Encoder输出分类也有两种方法:\n增加一个分类头[cls]token,用最后的Encoder对这个token的输出提取全局信息 不增加token,用最后Encoder的所有token的向量的全局平均池化GAP提取全局信息 最后,原作者也在该网络上尝试自监督学习。其借鉴BERT,对50%的patch进行标记,其中的80%标记为可学习的[mask]标签,10%替换为其他patch的Embedding,剩余10%维持不变。\n而预测像素值时,原来是RGB数量255x255x255=16581375色转换为预测8x8x8=256色,效果非常不错。\n参考视频:\nhttps://www.bilibili.com/video/BV15RDtYqE4r\n","tags":[],"title":"视觉Transformer网络ViT简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"MoCo是Momentum Contrast for Unsupervised Visual Representation Learning的缩写,即动量对比,其是一种对比学习算法。\n对比学习是一种无监督学习,通过正例对,负例对学习样本的特征表示。在特征空间里正例对的特征尽可能的靠近,负例对的特征尽可能远离。学到好的特征表示后,在下游任务上简单微调就可以取得好的结果。\n对于监督学习是有固定Label的,而对比学习是没有固定Label,随着模型训练,同一个样本生成的特征一直在变。因此对比学习有两个要求:\n负例要尽可能的多 负例要尽可能一致 MoCo中动量来自于股票中的移动平均EMA。其将图片对比学习看成字典检索问题,每次有1个样本称为query,字典里有1个正样本其他均为负样本。其目标是从字典中找出唯一与query的正样本。其Loss函数选择的是InfoNCE。\n参考视频:\nhttps://www.bilibili.com/video/BV1dtSuY7Evj https://www.bilibili.com/video/BV1hwLEzZEnS?p=9\n","date":1768187029,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1768262823,"objectID":"03f87e8fcef45a40feeca8aeddf196e8","permalink":"https://zhuzhulang.github.io/blog/posts/moco-network/","publishdate":"2026-01-12T11:03:49+08:00","relpermalink":"/posts/moco-network/","section":"posts","summary":"对MoCo网络进行简单的介绍","tags":["AI"],"title":"MoCo网络简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"一般的神经网络模型只能预测固定的类型从而造成难迁移的问题。CLIP网络通过对比学习(图文配对)实现自监督学习。\n以下是CLIP的网络结构:\n其中文本通过1个Text Encoder,而图片通过1个Image Encoder分别得到文本向量表示和图片向量表示。再通过一个线性投射层,投射在一个多模态向量空间中,尽量拉近配对文本与图片的向量距离,而不配对的文本与图片向量尽可能的远。\n实验发现在模型深度、宽度及图像分辨率上同时增加计算量,模型提升比仅在一个维度增加计算力要好。其损失函数一般为InfoNCE。\n而模型推理时先从数据集分类器中创建标签文本,然后就可以使用zero-shot进行预测。\n而相关利用CLIP的分割任务可以参考LSeg及GroupViT。\n参考视频:\nhttps://www.bilibili.com/video/BV1pYmDYgEDW/ https://www.bilibili.com/video/BV1hwLEzZEnS?p=17 https://blog.csdn.net/jiaoyangwm/article/details/132252010\n","date":1768184238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1768306627,"objectID":"1688ae68ccecc1a1cf3f4f82efa08061","permalink":"https://zhuzhulang.github.io/blog/posts/clip-network/","publishdate":"2026-01-12T10:17:18+08:00","relpermalink":"/posts/clip-network/","section":"posts","summary":"对CLIP网络进行简单的介绍","tags":["AI"],"title":"CLIP网络简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"视觉基础模型的迁移方法主要有4种:\nLinear Probing Finetune Adapter Prompt 其中线性探测(Linear Probing)在预训练模型参数冻结情况下训练新的线性分类器。只适用于大模型学习到的特征非常好、特别容易迁移的情况下才有效。\n而微调是指全参数化的微调,允许对预训练模型修改其所有层从而适配特定的任务。但这种方式可能会产生灾难遗忘的问题且需要的显存需要与训练的模型相同。\n第三种适配器的方法是在预训练模型上插入微小可训练的模块。\n而视觉Prompt可以不改变原模型,迁移到下游任务。具备few及zero-shot能力且更少的参数量。通过给模型提供额外的信息从而帮助其执行特定任务。其主要分为两种方式:\n基于标注的Prompt 基于学习的Prompt 参考视频:\nhttps://www.bilibili.com/video/BV1hwLEzZEnS?p=21\n","date":1768055917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1768116387,"objectID":"6c3d5f7b48c079d2b7fbdfb2aba7548d","permalink":"https://zhuzhulang.github.io/blog/posts/downstream-task-migration/","publishdate":"2026-01-10T22:38:37+08:00","relpermalink":"/posts/downstream-task-migration/","section":"posts","summary":"对视觉大模型下游任务迁移进行简单的介绍","tags":["AI"],"title":"视觉基础模型下游任务迁移","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"多模态网络并没有一些范式,但是存在一些共同点,将其概括为要素。\n下面对多模态网络的要素进行简单的介绍,主要包括:\nEncoder,针对每个模态的Encoder Align Strategy:不同模态的对齐/融合方式 LLM(可选):以大语言模型为核心的网络 多模态网络主要可以分为4种:\nDual-Encoder,双编码器,比如CLIP、GLIP、ALIGN Fusion,在双编码器与loss之间添加了一层Fusion Decoder,比如GLIP、SAM、CoCa Encoder-Decoder,在编码器与loss之间添加一层文本编码器与解码器 Adapted LLM,在编码器与loss之间添加一层LLM 参考视频:\nhttps://www.bilibili.com/video/BV1hwLEzZEnS?p=15\n","date":1767967309,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767969152,"objectID":"72b459e6d860741e0a213c72038fc5ed","permalink":"https://zhuzhulang.github.io/blog/posts/multimodel-elements/","publishdate":"2026-01-09T22:01:49+08:00","relpermalink":"/posts/multimodel-elements/","section":"posts","summary":"对多模态网络的要素进行简单的介绍","tags":["AI"],"title":"多模态网络的要素","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"下面对自监督学习的方法进行简单的介绍,主要包括:\n基于前置任务 基于对比学习 基于掩码重建 其中前置任务又可细分为如下几种子任务:\n位置预测,包括相对位置及拼图排序 旋转预测 图片上色,对灰度图片经过卷积层后对每个像素进行颜色填充 聚类预测,通过聚类对图片进行分类给出伪标签 而对比学习是通过创建一对pair样本,让正样本之间的距离尽可能地近,而负样本之间的距离尽可能地远。\n对于一张图像经过数据增强后可以得到两种数据增强后的正样本图像,经过网络后进行特征提取,此时可以通过1个loss让其尽可能小。但是一般情况是连接MLP网络后再添加loss从而得到更好的效果,这就是SimCLR的简化版本。\n对于SimCLR的网络结构可以参考下图:\n对比学习的另一种方式是MoCo(Momentum Contrast)模型。其中只有1个正样本与查询向量是正相关的,而其他负样本组成的队列与之不相关。这就是第1个版本,而在MoCo V2版本中,采用了更多的数据增强及将SimCLR中的映射层添加了进来,到了V3版本,则转向基于ViT架构并采用双向的对比loss,同时取消内存银行(Memory bank)而采用更大的batch。\n而掩码重建则是源自BERT的引申的BEiT模型,通过掩码的方式学习到上下文特征。\n基于对比学习的大模型主要是DINO,而基于掩码重建的大模型是MAE。\n参考视频:\nhttps://www.bilibili.com/video/BV1hwLEzZEnS https://www.bilibili.com/video/BV1hwLEzZEnS\u0026amp;page=8\n","date":1767927806,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767944967,"objectID":"22cf17f04cdca9ad6e67cc3b5ef4700d","permalink":"https://zhuzhulang.github.io/blog/posts/self-supervised-learning-method/","publishdate":"2026-01-09T11:03:26+08:00","relpermalink":"/posts/self-supervised-learning-method/","section":"posts","summary":"对自监督学习的方法进行简单的介绍","tags":["AI"],"title":"自监督学习的方法","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"卷积是通过数学的方式从图像中提取其相关特征。假设我们有所示的图片:\n0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 其中黑色地方值为0,而白色地方值为1。实际上就是个数字X的图片。\n而提取特征需要使用卷积核的工具,其尺寸一般是3x3或5x5的方块,比如下面用于提取右下线条的卷积核:\n1 0 0 0 1 0 0 0 1 于是得到了特征图:\n2 0 1 0 1 0 3 0 1 0 1 0 3 0 1 0 1 0 3 0 1 0 1 0 2 而卷积核中只有斜方向上的数字为1,因此卷积后对应位置的值特别大。而在特征图中,我们不妨找下哪里的值特别大。从而可以看到其满足右下线条的特征。\n通过不同的卷积核我们可以对图像进行不同的处理,得到不同的特征图,从而显示特征分布在图像的什么位置。\n之后我们通过池化技术简化特征图,将有特征的部分放大,比如使用最大池化,于是得到:\n3 1 1 1 3 1 1 1 2 而池化后这个比较小的特征图依然保持原有特征图的重要信息,就是右下线条。\n之后再经过激活过程,于是得到:\n0.95 0.73 0.73 0.73 0.95 0.73 0.73 0.73 0.88 这张图依然代表图像的特性,数值越接近1越满足卷积核的特性。\n参考视频:\nhttps://www.bilibili.com/video/BV1dhHpeiET1\n","date":1767769621,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767770899,"objectID":"0b43f4b7a98526daab8d56ea2474a62c","permalink":"https://zhuzhulang.github.io/blog/posts/convolution-network-introduction/","publishdate":"2026-01-07T15:07:01+08:00","relpermalink":"/posts/convolution-network-introduction/","section":"posts","summary":"将介绍卷积神经网络是如何从图像中提取特征","tags":["AI"],"title":"卷积神经网络特征提取","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"传感器融合的核心是整合来自多种传感器的数据,以提升环境感知的准确性、鲁棒性和完整性,从而支持自动驾驶系统做出可靠决策。\n在自动驾驶中,它主要应用于两个关键领域:\n感知融合(Perception Fusion),结合不同传感器的原始数据或处理结果,实现更精确的目标检测、识别和跟踪 定位融合(Localization Fusion),通过融合GPS、IMU等传感器信息,提高车辆自身定位的精度 以下是3种常见的传感器,分别为激光雷达、毫米波雷达及摄像头。其特点可以归纳如下表:\n激光雷达 毫米波雷达 摄像头 可用于定位 √ √ × 可用于夜晚 √ √ × 可3D建模 √ × × 雨雪天气可用 × √ × 可识别颜色 × × √ 而一般导航级地图精度是米级的,无法用于自动驾驶定位。此时需要借助高精度地图,其精度是厘米或分米级别。\n理论上4颗卫星即可实现定位,再结合基站进行地面校正。而自动驾驶汽车会与高精地图场景进行对比,从而确定自己的位置。\n而不同传感器会获取不同的信息,此时面临2个主要问题:\n空间定标,将不同传感器捕捉到的画面融合在一块 时间定标,将不同时间的信号融合在一起 除此之外,不同传感器彼此的信息是矛盾的,就会出现该相信谁的问题。此时想起马斯克吹嘘纯视觉端到端模型多好,而放弃激光雷达是对的,然后说到其星链中使用激光雷达会出现彼此信息矛盾的问题。对此可以使用卡尔曼滤波来解决。\n假设有两个误差符合高斯分布的传感器,其中GPS定位$Z_{1}$,误差为$\\sigma_{1}^{2}$。而另一个传感器$Z_{2}$,误差为$\\sigma_{2}^{2}$。\n而卡尔曼给出最优的估计\n$$ \\begin{cases} Z=\\cfrac{\\sigma_{2}^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}Z_{1}+\\cfrac{\\sigma_{1}^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}Z_{2}\\ \\cfrac{1}{\\sigma^{2}}=\\cfrac{1}{\\sigma_{1}^{2}}+\\cfrac{1}{\\sigma_{2}^{2}} \\end{cases} $$\n其中随更准确那么就更相信谁,对应的权重也更大一些。 参考视频:\nhttps://www.bilibili.com/video/BV1dhHpeiET1\n","date":1767750984,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767753999,"objectID":"9f603b00d3bdec559148bfbefaf4d331","permalink":"https://zhuzhulang.github.io/blog/posts/autonomous-driving-sensor-fusion/","publishdate":"2026-01-07T09:56:24+08:00","relpermalink":"/posts/autonomous-driving-sensor-fusion/","section":"posts","summary":"对自动驾驶中传感器融合进行简单介绍","tags":["AI"],"title":"自动驾驶与传感器融合","type":"posts"},{"authors":["码力全开"],"categories":["CV"],"content":"这里来看一个实例,基于YOLO实现对高速公路的车速检测。\n整个过程可以说非常简单,主要用到YOLO、ByteTrack及OpenCV。通过ByteTrack进行车辆目标跟踪,从而方便获取其位置信息。\n要实现车速计算,最简单的方法自然是根据识别框的像素位置进行估计,但是这种方式存在一种问题,无法从图像的像素值得到其实际物理的距离。为此,需要通过一种映射关系建立与真实世界的联系。\n在OpenCV中提供了getPerspectiveTransform函数,我们可以将图像中的四边形变换为一个矩形,从而建立其关系。因为龙门架的高度、车道线的距离一般是规定不变的,从而借助这些隐含信息作为参考,得到其真实的距离度量。\n问题在于如何得到图像中的坐标点,假设我们知道其坐标点信息,那么就可以建立联系了,从而解决车速检测的问题。\n最后为了避免车速出现一些异常值,可以对一秒钟内的速度取平均值,这样有助于系统更加稳健。\n最后我们还可以使用YOLO制造热力图,将其进行显示,详情可以参考使用YOLOv8创建交通热力图。\n参考文章:\nhttps://cloud.tencent.com/developer/article/2389959?policyId=1004\n","date":1767514066,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767515597,"objectID":"e73ed3cd6c4d49c7f82ed7c9d8b87b59","permalink":"https://zhuzhulang.github.io/blog/posts/yolo-speed-estimate/","publishdate":"2026-01-04T16:07:46+08:00","relpermalink":"/posts/yolo-speed-estimate/","section":"posts","summary":"这里来看一个实例,基于YOLO实现对高速公路的车速检测。\n整个过程可以说非常简单,主要用到YOLO、ByteTrack及OpenCV。通过ByteTrack进行车辆目标跟踪,从而方便获取其位置信息。\n要实现车速计算,最简单的方法自然是根据识别框的像素位置进行估计,但是这种方式存在一种问题,无法从图像的像素值得到其实际物理的距离。为此,需要通过一种映射关系建立与真实世界的联系。\n在OpenCV中提供了getPerspectiveTransform函数,我们可以将图像中的四边形变换为一个矩形,从而建立其关系。因为龙门架的高度、车道线的距离一般是规定不变的,从而借助这些隐含信息作为参考,得到其真实的距离度量。\n问题在于如何得到图像中的坐标点,假设我们知道其坐标点信息,那么就可以建立联系了,从而解决车速检测的问题。\n最后为了避免车速出现一些异常值,可以对一秒钟内的速度取平均值,这样有助于系统更加稳健。\n最后我们还可以使用YOLO制造热力图,将其进行显示,详情可以参考使用YOLOv8创建交通热力图。\n参考文章:\nhttps://cloud.tencent.com/developer/article/2389959?policyId=1004\n","tags":["CV"],"title":"基于YOLO的车速检测","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"这里对大模型并行计算中的策略进行简单的介绍,主要涵盖如下一些策略的内容:\n数据并行 模型并行 张量并行 流水线并行 其中数据并行(DP,Data Parallelism)是最简单的,就是将训练的数据集拆分为大小相同的若干份,模型对不同的数据子集进行训练,在各个分组训练完成后需要进行全局的参数同步。\n而张量并行(TP,Tensor Parallelism)通过将模型中的张量(如权重矩阵)按行或列切分到多个计算设备上,以解决单设备内存不足问题并提升大模型训练效率。其属于模型并行中的一种,核心思想是将单个模型层的参数或计算任务拆分到不同设备上执行,如矩阵乘法拆分到不同的GPU上运行。\n模型并行(Model Parallelism)是通过将模型不同的部分分配给多个设备上进行计算,比如将不同的层分配给不同的设备上,但这种朴素策略存在GPU利用率低的问题。与之类似的是流水线并行(PP,Pipeline Parallelism),通过将模型的不同层按顺序分配到不同设备上的方法。\n流水线并行通过将输入数据切分成多个微批次,使得每个设备可以在处理完当前批次后立即处理下一个批次,从而提高设备利用率。主要策略有Gpipe和PipeDream流水线并行。\n参考文章:\nhttps://developer.aliyun.com/article/1257832 https://blog.csdn.net/shizheng_Li/article/details/144138542 https://blog.csdn.net/m0_59164520/article/details/142731435 https://www.cnblogs.com/khronos0206/p/18606724 https://zhuanlan.zhihu.com/p/613196255\n","date":1767447187,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767449416,"objectID":"fbbd614b8f68b992d7a84f25c7a2a25e","permalink":"https://zhuzhulang.github.io/blog/posts/parallelism-strategy-introduction/","publishdate":"2026-01-03T21:33:07+08:00","relpermalink":"/posts/parallelism-strategy-introduction/","section":"posts","summary":"这里对大模型并行计算中的策略进行简单的介绍,主要涵盖如下一些策略的内容:\n数据并行 模型并行 张量并行 流水线并行 其中数据并行(DP,Data Parallelism)是最简单的,就是将训练的数据集拆分为大小相同的若干份,模型对不同的数据子集进行训练,在各个分组训练完成后需要进行全局的参数同步。\n而张量并行(TP,Tensor Parallelism)通过将模型中的张量(如权重矩阵)按行或列切分到多个计算设备上,以解决单设备内存不足问题并提升大模型训练效率。其属于模型并行中的一种,核心思想是将单个模型层的参数或计算任务拆分到不同设备上执行,如矩阵乘法拆分到不同的GPU上运行。\n模型并行(Model Parallelism)是通过将模型不同的部分分配给多个设备上进行计算,比如将不同的层分配给不同的设备上,但这种朴素策略存在GPU利用率低的问题。与之类似的是流水线并行(PP,Pipeline Parallelism),通过将模型的不同层按顺序分配到不同设备上的方法。\n流水线并行通过将输入数据切分成多个微批次,使得每个设备可以在处理完当前批次后立即处理下一个批次,从而提高设备利用率。主要策略有Gpipe和PipeDream流水线并行。\n参考文章:\nhttps://developer.aliyun.com/article/1257832 https://blog.csdn.net/shizheng_Li/article/details/144138542 https://blog.csdn.net/m0_59164520/article/details/142731435 https://www.cnblogs.com/khronos0206/p/18606724 https://zhuanlan.zhihu.com/p/613196255\n","tags":["AI"],"title":"大模型并行策略简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"Ring算法是一种基于环形拓扑结构的分布式计算算法,其核心思想是将系统中的节点(如进程或计算单元)组织成一个逻辑环,每个节点仅与环上的直接邻居节点(左邻和右邻)通信,信息沿环的固定方向依次传递,从而实现协作。\n百度在2018年提出一种在深度学习场景下高效的all reduce算法,称为Ring all reduce。整个通信过程可以分为reduce-scatter和all gather两个阶段。\n假设我们有如下5个节点:\nGPU 0: a0 b0 c0 d0 e0 GPU 1: a1 b1 c1 d1 e1 GPU 2: a2 b2 c2 d2 e2 GPU 3: a3 b3 c3 d3 e3 GPU 4: a4 b4 c4 d4 e4 其reduce-scatter过程如下,第1步为:\nGPU 0: a0 b0 c0 d0 e0+e4 GPU 1: a0+a1 b1 c1 d1 e1 GPU 2: a2 b1+b2 c2 d2 e2 GPU 3: a3 b3 c2+c3 d3 e3 GPU 4: a4 b4 c4 d3+d4 e4 而第2步为:\nGPU 0: a0 b0 c0 d0+d3+d4 e0+e4 GPU 1: a0+a1 b1 c1 d1 e1+e0+e4 GPU 2: a0+a1+a2 b1+b2 c2 d2 e2 GPU 3: a3 b1+b2+b3 c2+c3 d3 e3 GPU 4: a4 b4 c2+c3+c4 d3+d4 e4 而第3步为:\nGPU 0: a0 b0 c0+c2+c3+c4 d0+d3+d4 e0+e4 GPU 1: a0+a1 b1 c1 d1+d0+d3+d4 e1+e0+e4 GPU 2: a0+a1+a2 b1+b2 c2 d2 e2+e1+e0+e4 GPU 3: a0+a1+a2+a3 b1+b2+b3 c2+c3 d3 e3 GPU 4: a4 b1+b2+b3+b4 c2+c3+c4 d3+d4 e4 最后我们得到:\nGPU 0: a0 b0+b1+b2+b3+b4 c0+c2+c3+c4 d0+d3+d4 e0+e4 GPU 1: a0+a1 b1 c1+c0+c2+c3+c4 d1+d0+d3+d4 e1+e0+e4 GPU 2: a0+a1+a2 b1+b2 c2 d2+d1+d0+d3+d4 e2+e1+e0+e4 GPU 3: a0+a1+a2+a3 b1+b2+b3 c2+c3 d3 e3+e2+e1+e0+e4 GPU 4: a0+a1+a2+a3+a4 b1+b2+b3+b4 c2+c3+c4 d3+d4 e4 之后就要开始进行all-gather进行广播了。而执行完一个环后,所有节点均有完整的备份。因此,Ring All Reduce进行2次环就完成整个数据的同步。\n参考文章:\nhttps://www.zhihu.com/question/57799212 https://blog.csdn.net/weixin_44966641/article/details/149980684 https://www.bilibili.com/video/BV1fg41187rc/\n","date":1767427272,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767439222,"objectID":"4c06202349ff3575dee99d1e5f11ca90","permalink":"https://zhuzhulang.github.io/blog/posts/ring-algorithm-introduction/","publishdate":"2026-01-03T16:01:12+08:00","relpermalink":"/posts/ring-algorithm-introduction/","section":"posts","summary":"对分布式并行中Ring算法进行简单的介绍","tags":["AI"],"title":"Ring算法简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"在大模型分布式并行计算中主要有如下一些通信的原语:\nbroadcast scatter gather all-gather reduce all-reduce reduce-scatter all-all 其中最简单的就是前三者。broadcast与scatter都是进行分发,而gather是聚合的过程。\nBroadcast(广播)用于将数据从1个进程或节点发送到所有其他进程或节点,通常由一个源进程或节点发送数据,然后所有其他进程或节点都接收相同的数据。通常用于并行计算中将全局数据分发给所有参与者,以便它们可以共享相同的信息。\n与之类似的是scatter,其用于将数据从一个进程或节点发送到多个进程或节点。通常由一个源进程或节点发送数据,然后所有其他进程或节点都接收部分数据。可以看到,Broadcast中节点接收的是相同的信息,而在scatter中接收的只是部分数据,相当于Broadcast中是数据的复制,而scatter是数据拆分后的分片。\n而gather用于将来自多个进程或节点的数据收集到单个进程或节点中,结果是将来自各个进程或节点的数据聚合到一个目标进程或节点中。其可以看成是scatter的反过程。\n与之类似的是reduce,其用于将来自多个进程或节点的数据合并成单个值。通常由一个目标进程或节点执行归约操作,将来自其他进程或节点的数据集合成一个结果。可以看到,reduce相当于将gather中的数据合并成一个,而不再是gather中的一列。\n下面我们来看all-gather与all-reduce。\nall-gather将来自所有进程或节点的数据收集到所有其他进程或节点中,而不仅仅是一个目标进程或节点。其结果可以理解为其在gather基础上叠加broadcast的操作。主要用于分布式计算中共享数据,进行全局汇总或全局同步。\nall-reduce将来自所有进程或节点的数据合并成单个值,将结果分发给所有其他进程或节点。其结果可以理解其在reduce基础上叠加broadcast的操作,通常用于分布式计算中计算全局统计量,对全局计算结果进行合并或进行全局同步。\n除了以上组合方式外,reduce-scatter结合了归约和分散操作,首先将来自多个进程或节点的数据归约成单个值,然后将结果分散到所有进程或节点中。其主要用于将全局结果分发给各个参与者,以便它们可以在本地处理部分结果。\n最后,all-all是在所有进程或节点之间进行全局数据交换,每个进程或节点都向所有其他进程或节点发送数据,并接收来自所有其他进程或节点的数据。\n参考文章:\nhttps://blog.csdn.net/u010420283/article/details/139396719\n","date":1767405101,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767408867,"objectID":"a51e943abd416053e774b804da51a0ab","permalink":"https://zhuzhulang.github.io/blog/posts/parallel-primitives-introduction/","publishdate":"2026-01-03T09:51:41+08:00","relpermalink":"/posts/parallel-primitives-introduction/","section":"posts","summary":"对大模型分布式并行中各种通信原语进行简单的介绍","tags":["AI"],"title":"分布式并行通信原语简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"Switch Transformer由谷歌2021年提出,是一种基于Transformer架构的改进模型,其通过引入MoE(Mixture of Experts)机制提升模型的效率与扩展性。相关论文可以参考《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity》。\n在标准Transformer中,每个输入的token都会经过相同的前馈网络(FFN)层进行处理,而Switch Transformer将其中的FFN替换为多个独立的专家子网络。模型通过一个路由机制动态地将每个token分配给最合适的专家进行处理,这种设计允许模型在不显著增加单个样本推理计算量的情况下扩展参数规模,从而提升处理能力。\n需要注意的是,MoE中的专家并不会都全被激活,一般只会激活1-2个专家。而其训练过程需要使用数据并行、模型并行及专家并行方法,从而可以让模型支持万亿参数规模。\n数据并行过程将训练数据分片到多个设备,每个设备独立计算损失和梯度,随后通过All-Reduce操作聚合梯度并同步模型参数。\n专家并行将MoE结构中的专家子网络分布到不同设备,每个设备仅存储部分专家的参数。通过稀疏路由机制(Switch Routing)确保每个token仅由一个专家进行处理,通过All-to-All通信将token路由到对应设备,实现专家数量的线性扩展并减少通信开销。\n参考文章:\nhttps://zhuanlan.zhihu.com/p/705443412 https://developer.aliyun.com/article/1690017\n","date":1767361709,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767362645,"objectID":"535c9cd7016335557cd4983dde2c2bde","permalink":"https://zhuzhulang.github.io/blog/posts/switch-transformer-introduction/","publishdate":"2026-01-02T21:48:29+08:00","relpermalink":"/posts/switch-transformer-introduction/","section":"posts","summary":"对Switch Transformer进行简单的介绍","tags":["AI"],"title":"Switch Transformer简介","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"这里我们将对Transformer中各类位置编码技术进行简单的叙述,主要包括:\n相对位置编码,如T5 Relative 正余弦位置编码,如Sinusoidal 旋转弦位置编码,如RoPE 旋转位置编码,如YaRN 线性偏置项位置编码,如ALiBi 下面我们对相对位置与绝对位置编码进行总结:\n优势 劣势 绝对位置编码 计算速度较快 文字间相对位置信息不明显,推理窗口受训练长度限制 相对位置编码 可学习文字间相对信息 计算量增大,无法使用KV Cache 旋转位置编码与正余弦位置编码的区别在于,前者不再在词向量(字典查找表)中加入位置信息,而是通过旋转矩阵对向量(Q,K)进行角度旋转。\n通过旋转位置编码,可以保留token间相对信息及绝对位置信息。\n之前的位置编码都是在Q和K基础上操作的,而ALiBi是对Q@K乘积结果进行操作。\n参考视频:\nhttps://www.bilibili.com/video/BV1ErPkeSEHn\n","date":1767165855,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767167707,"objectID":"be8b1c28246458163c1fbb6b37f778ab","permalink":"https://zhuzhulang.github.io/blog/posts/transformer-position-encoder-introduction/","publishdate":"2025-12-31T15:24:15+08:00","relpermalink":"/posts/transformer-position-encoder-introduction/","section":"posts","summary":"对Transformer各类位置编码技术进行简单的叙述","tags":["AI"],"title":"Transformer各类位置编码技术简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"在这里我们对注意力的各种变种进行简单的介绍,主要包括:\nMHA(Multi-Head Attention) MQA(Multi-Query Attention) GQA(Grouped-Query Attention) 其中MHA是多头注意力,其网络结构如下图所示:\n而MQA是将后面的K与V合并为1个,而为多个Q,其网络结构如下:\n对于GQA,则将QKV分为多个组并共用,其网络结构如下:\n参考视频:\nhttps://www.bilibili.com/video/BV17CPkeEE5d\n","date":1767145466,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767146186,"objectID":"ef40ae095b6764306982e5fd3f7b339a","permalink":"https://zhuzhulang.github.io/blog/posts/mqa-and-gqa-introduction/","publishdate":"2025-12-31T09:44:26+08:00","relpermalink":"/posts/mqa-and-gqa-introduction/","section":"posts","summary":"对MHA、MQA及GQA进行简单的介绍","tags":["AI"],"title":"MQA与GQA简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"之前介绍了FlashAttention,现在来看PageAttention的相关内容。\n在介绍PageAttention之前,需要对KV Cache有一定的了解。我们知道Attention的计算过程可以使用类似如下的公式进行计算:\n$$ \\text{Attention}(Q,K,V)=\\text{Softmax}(QK^{T})@V $$\n在上述过程中我们会计算K与V乘积,而对于序列来说,前面n个的值计算一次就不变了,为了加快计算,于是引入了缓存机制,这就是KV Cache的由来。\n需要注意的是,KV Cache只应用于推理阶段,且只存在于Decoder解码器中,其会加大显卡内存占用。由于其需要提前申请过多空间从而造成内部碎片,而过小的空间无法分配给其他请求,从而造成外部碎片。\n为了解决KV Cache随着token不断增长而占用的内存也线性增长的问题,于是就引入了PageAttention的概念。\n在计算机操作系统中,页(Page)是可以按需加载的,通过动态的方式解决内存占用的问题。同理,对于LLM中的Attention,也可以利用类似的思想按需进行加载,从而解决KV Cache的问题。\nPageAttention将连续的键值缓存KV Cache分割为固定大小的物理块,并动态管理,从而显著提升显存利用率和推理吞吐量。其主要分为3个步骤:\n分块存储:将输入序列的KV Cache划分为固定大小的逻辑块,每个逻辑块通过块表映射到非连续的物理显存块上 动态分配:在推理过程,仅按需加载当前计算所需的物理块到显存,避免为每个请求预分配最大长度空间 写时复制:在并行采样(如生成多个候选输出)时,多个请求共享相同物理块。当某个请求需修改块内存时,系统自动复制该块并更新映射,确保数据隔离性。 类似地,在SGLang中还有Radix Attention,也是通过对KV Cache进行复用。此时,是不是觉得有些熟悉感,于是我们将大模型的问题在计算机基础课程中找到了解决的方法。\n参考视频:\nhttps://www.bilibili.com/video/BV17CPkeEEzk/\n","date":1767100020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767101954,"objectID":"fda2b3e697200f3281d66b34482c8afd","permalink":"https://zhuzhulang.github.io/blog/posts/pageattention-introduction/","publishdate":"2025-12-30T21:07:00+08:00","relpermalink":"/posts/pageattention-introduction/","section":"posts","summary":"关于PageAttention的简单介绍","tags":["AI"],"title":"PageAttention简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"Transformer中模型的核心是自注意力机制(self-attention),其时间和存储复杂度均为$O(n^{2})$。于是有人提出了近似注意力的方法来降低注意力计算和内存需求,而FlashAttention在此基础上考虑了内存访问(IO)的开销。\n关于FlashAttention的详细介绍,可以参考文章《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》。\n通过减少GPU内存读取和写入,FlashAttention的运行速度比PyTorch标准注意力快2-4倍,而所需的内存减少了5-20倍。\n我们知道GPU的SRAM(静态随机存取内存)的IO读写速度大概为19TB/s,而GPU的HBM高带宽显存的读写速度为1.5TB/s,但是它们的存储容量分别是20MB和40GB。\nFlashAttention在运行注意力机制算法时,需要从HBM中读取Q、K、V这3个矩阵,并在SRAM中进行计算,并在计算完成后写回到HBM中。因此就得考虑如何减少这样的IO开销,从而得到更好的效果。\n在标准注意力机制实现中,假设HBM中矩阵$Q,K,V\\in\\mathbb{R}^{N\\times d}$。其过程如下:\n从HBM中加载Q,K,计算$S=QK^{T}$,并将S写入HBM 从HBM中读取S,计算$P=\\text{Softmax}(S)$,将P写入HBM 从HBM中加载P和V,计算$O=PV$,将O写入HBM 返回O 从上面步骤可以看到$$ S=QK^{T}\\in\\mathbb{R}^{N\\times N},\\quad P=\\text{Softmax}(S)\\in\\mathbb{R}^{N\\times N},O=PV\\in\\mathbb{R}^{N\\times d} $$\n在计算过程中需要存储中间值S和P到HBM中,这会极大占用HBM(高带宽显存)。\n而FlashAttention希望可以避免从HBM中读取和写入注意力矩阵,从而对如下方面进行优化:\n在不访问整个输入的情况下计算softmax函数的缩减 在后向传播中不能存储中间注意力矩阵S和P 对于第1点,FlashAttention将输入分割成块,并在输入块上进行多次传递,从而以增量方式进行softmax缩减,从而大大加快运行的速度。 对于第2点,FlashAttention存储一个softmax函数的归一化因子,通过这个归一化因子在反向传播过程中再重新计算这2个矩阵,从而大大节省所占用的内存\n参考视频:\nhttps://www.bilibili.com/video/BV1zs4y1J7tb/ https://zhuanlan.zhihu.com/p/669926191\n","date":1767080817,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767083083,"objectID":"ffae9f5b008e66b78ab0e1e6c41b7195","permalink":"https://zhuzhulang.github.io/blog/posts/flashattention-introduction/","publishdate":"2025-12-30T15:46:57+08:00","relpermalink":"/posts/flashattention-introduction/","section":"posts","summary":"对FlashAttention进行简单的介绍","tags":["AI"],"title":"FlashAttention简述","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"这里介绍下低比特量化的原理,之后再看下如何将fp32转换为int8。\n模型量化是通过减少权重表示或激活所需的比特数来压缩模型,而网络剪枝研究模型权重中的冗余,并尝试删除/修剪冗余和非关键的权重。\n其实模型量化的原理非常简单,我们知道FP32可以表示的范围为$$ \\pm 1.4\\times 10^{-45}\\sim\\pm 3.4\\times 10^{38} $$\n而int8可以表示的范围在[-128,127]。可以看到int8中一位数如果对应到fp32中将是很大一段区域。\n模型量化是一种将浮点计算转换成低比特定点计算的技术,可以有效的降低模型计算强度、参数大小和内存消耗,但往往带来巨大的精度损失。\n模型之所以会引入量化的原因在于神经网络一般对噪声(精度损失)不太敏感。\n模型量化通过在定点和浮点数之间建立一种有效的数据映射关系,使得以较小的精度损失代价获取较好的收益。\n而线性量化主要分为两种:\n对称量化 非对称量化 其中对于int8来说,对称量化的取值范围是[-128,127],而非对称量化的取值范围是[0,255]。\n模型量化的原理就是找到这种数据映射关系,其中浮点与定点数据的转换公式如下:\n$$ Q=\\frac{R}{S}+Z\\ \\Rightarrow Q-Z=\\frac{R}{S}\\ \\Rightarrow R=(Q-Z)\\times S $$\n其中R表示输入的浮点数据,Q表示量化后的定点数据,Z表示零点的数值,S表示缩放因子的数值。\n求解S和Z有很多种方法,比如下面最简单的一种线性量化求解方式(MinMax):\n$$ S=\\frac{R_{max}-R_{min}}{Q_{max}-Q_{min}}\\ Z=Q_{max}-\\frac{R_{max}}{S} $$\n其中$R_{max}$表示输入浮点数数据中最大值,$R_{min}$表示输入浮点数数据中最小值。同理$Q_{max}$是最大的定点值。\n下面我们来看一个例子加深其印象。假设神经网络某层权重的取值范围是[-1.0,1.0],使用对称量化方式,则将其代入上述公式从而有$$ S=\\frac{1.0-(-1.0)}{127-(-128)}=\\frac{2}{255}\\ Z=127-\\frac{1.0}{\\frac{2}{255}}=127-127.5=-0.5 $$\n当输入的FP32的值为0.5时有$$ Q=\\frac{0.5}{\\frac{2}{255}}-0.5=\\frac{255-2}{4}=\\frac{253}{4}\\approx 63 $$\n而对应的反量化过程为$$ R=(63+0.5)\\times\\frac{2}{255}=\\frac{127}{255}\\approx 0.498 $$\n可以看到反量化后的值与输入的值0.5相差不大。\n有了量化原理后,接下来就可以进行一些实际的应用了,比如QAT(感知量化训练,Aware Quantization Training)。其通过插入伪量化节点(fake quant)来模拟量化引入的误差。并在端侧推理的过程直接使用tensor中带有量化属性参数。\n通过伪量化节点可以找到输入数据的分布,即找到min和max值。同时,模拟量化到低比特操作的时候的精度损失,把该损失作用到网络模型中,传递给损失函数,让优化器在训练过程中对该损失值进行优化。\n而PTQ Dynamic(动态离线量化,Post Training Quantization Dynamic)仅将模型中特定算子的权重从FP32类型映射成INT8/16类型,其缩放因子是动态计算的,因此动态量化是几种量化方法中性能最差的。\n另外一种PTQ Static也称为校正量化或数据集量化。其使用少量无标签校准数据,核心是计算量化比例因子,使用静态量化后的模型进行预测。在此过程中量化模型的缩放因子会根据输入数据的分布进行调整。\n参考视频:\nhttps://www.bilibili.com/video/BV1VD4y1n7AR https://blog.csdn.net/2502_92631100/article/details/149351615 https://www.bilibili.com/video/BV1s8411w7b9 https://www.bilibili.com/video/BV1HD4y1n7E1\n","date":1766046566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1766126471,"objectID":"dcac57084621b437cb9c13a25b269ffa","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-int8-quantization/","publishdate":"2025-12-18T16:29:26+08:00","relpermalink":"/posts/how-to-int8-quantization/","section":"posts","summary":"如何实现int8量化","tags":["AI"],"title":"int8量化概述","type":"posts"},{"authors":["码力全开"],"categories":["AI","CV"],"content":"关于MobileViT网络,是在MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer中提出的。\nMobileViT将CNN与Transformer的优势成功组合在一起,从而给移动设备带来轻量级、低延迟及通用视觉的网络。其相关代码实现可以参考ml-cvnets。\n在标准ViT(visual transformer)中,对于输入的图片需要将其进行patch后再展平,这个过程可以通过1个卷积来实现。之后通过Linear后得到embedding,添加位置编码信息后送入Transformer层中,最后再经过Linear得到的最终输出,从而实现分类。\nMobileViT整个网络结构如下图所示:\n其中Conv-n x n表示标准的n x n卷积,而MV2表示MobileNetv2块。而标注↓2是对块进行下采样。\n最后介绍下如何在YOLO中集成MobileViT模块,可以参考YOLOv8添加MobileViTv3模块。相关代码为yolov8_vit。\n参考文章:\nhttps://keras.io/examples/vision/mobilevit/\n","date":1765094733,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1765096994,"objectID":"6d8c1daab8437a44243fd0ca969091f5","permalink":"https://zhuzhulang.github.io/blog/posts/mobilevit-introduction/","publishdate":"2025-12-07T16:05:33+08:00","relpermalink":"/posts/mobilevit-introduction/","section":"posts","summary":"对MobileViT网络进行简单的介绍","tags":["AI","CV"],"title":"MobileViT简述","type":"posts"},{"authors":["码力全开"],"categories":["AI","CV"],"content":"默认情况下YOLO模型采用的都是单Backbone的设计,但是这种架构存在其内在的局限,特别对于小物体的识别往往力不从心。而引入双Backbone架构可以从多维度特征融合方面增强其能力。\n双Backbone架构主要分为共享输入和双输入这两种典型结构。其中共享输入允许模型处理同一个输入时通过并行的特征对特征进行提取,从不同的角度和尺度理解输入的图像内容。其极大增强了模型对目标的判别能力,在训练和部署过程中,由于共享同一输入图像,参数相对稳定,减少模型不稳定的情况的发生。 而双输入结构允许两个Backbone分别处理不同来源的输入,这些输入可以是不同模态的数据,比如RGB与红外图像输入的结合,一般情况下说的多模态融合都属于这种类型。\n下面介绍下双Backbone架构的一些组合方式:\nCNN + CNN: 将两个不同的CNN进行组合,可以选择一个轻量级CNN快速捕捉图像中的浅层特征,再使用另一个相对较重且语义建模能力更强的CNN来理解图像中目标的内在关系。 CNN + Transformer: 让CNN处理图像的低级特征,并将特征传递给Transformer,由Transformer来理解其语义关联 CNN + Mamba: CNN提取图像的静态特征,Mamba捕捉图像中跨通道、区域以及时间的动态信息 参考文章:\nhttps://blog.csdn.net/qq_64693987/article/details/148086687\n","date":1765074340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1765077560,"objectID":"39a25c674e09aebaec94c2ae7e30fc9c","permalink":"https://zhuzhulang.github.io/blog/posts/double-backbone-in-yolo/","publishdate":"2025-12-07T10:25:40+08:00","relpermalink":"/posts/double-backbone-in-yolo/","section":"posts","summary":"介绍YOLO中双Backbone架构的相关设计","tags":["AI","CV"],"title":"YOLO双Backbone架构设计","type":"posts"},{"authors":["码力全开"],"categories":["AI","CV"],"content":"之前介绍了如何将YOLO11中Backbone网络替换为ShuffleNet,而今天尝试将其替换为MobileNet V3。\n首先是代码的实现:\nimport torch from torch import nn class h_sigmoid(nn.Module): def __init__(self, inplace=True): super(h_sigmoid, self).__init__() self.relu = nn.ReLU6(inplace=inplace) def forward(self, x): return self.relu(x + 3) / 6 class h_swish(nn.Module): def __init__(self, inplace=True): super(h_swish, self).__init__() self.sigmoid = h_sigmoid(inplace=inplace) def forward(self, x): return x * self.sigmoid(x) class SELayer(nn.Module): def __init__(self, channel, reduction=4): super(SELayer, self).__init__() # Squeeze操作 self.avg_pool = nn.AdaptiveAvgPool2d(1) # Excitation操作(FC+ReLU+FC+Sigmoid) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel), h_sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x) y = y.view(b, c) y = self.fc(y).view(b, c, 1, 1) # 学习到的每一channel的权重 return x * y class conv_bn_hswish(nn.Module): \u0026#34;\u0026#34;\u0026#34; This equals to def conv_3x3_bn(inp, oup, stride): return nn.Sequential( nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), h_swish() ) \u0026#34;\u0026#34;\u0026#34; def __init__(self, c1, c2, stride): super(conv_bn_hswish, self).__init__() self.conv = nn.Conv2d(c1, c2, 3, stride, 1, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = h_swish() def forward(self, x): return self.act(self.bn(self.conv(x))) def fuseforward(self, x): return self.act(self.conv(x)) class MobileNetV3(nn.Module): def __init__(self, inp, oup, hidden_dim, kernel_size, stride, use_se, use_hs): super(MobileNetV3, self).__init__() assert stride in [1, 2] self.identity = stride == 1 and inp == oup # 输入通道数=扩张通道数 则不进行通道扩张 if inp == hidden_dim: self.conv = nn.Sequential( # dw nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False), nn.BatchNorm2d(hidden_dim), h_swish() if use_hs else nn.ReLU(inplace=True), # Squeeze-and-Excite SELayer(hidden_dim) if use_se else nn.Sequential(), # pw-linear nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), ) else: # 否则 先进行通道扩张 self.conv = nn.Sequential( # pw nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False), nn.BatchNorm2d(hidden_dim), h_swish() if use_hs else nn.ReLU(inplace=True), # dw nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False), nn.BatchNorm2d(hidden_dim), # Squeeze-and-Excite SELayer(hidden_dim) if use_se else nn.Sequential(), h_swish() if use_hs else nn.ReLU(inplace=True), # pw-linear nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), ) def forward(self, x): y = self.conv(x) if self.identity: return x + y else: return y 关于该网络的介绍这里不赘述,可以参考文章最后的链接。这里主要关注网络的结构图:\n可以看到网络输入先经过3x3卷积后才进入bneck模块。其中输出通道为16,不使用SE模块,激活函数NL为h-swich,而stride为2。\n根据上述网络结构,可以定义出YOLO的yaml中的内容如下,这里新建1个配置文件yolo11n-mobilenetV3-small.yaml:\nbackbone: # [from, repeats, module, args] - [-1, 1, conv_bn_hswish, [16, 2]] # 0-P1/2 320*320 - [-1, 1, MobileNetV3, [16,16, 3, 2, True, False]] # 1-P2/4 160*160 - [-1, 1, MobileNetV3, [24,72, 3, 2, False, False]] # 2 80*80 - [-1, 1, MobileNetV3, [24,88,3,1,False,False]] # 3-P3/8 80*80 - [-1, 1, MobileNetV3, [40, 96, 5,2,True,True]] # 4 40*40 - [-1, 1, MobileNetV3, [40, 240, 5, 1, True, True]] # 5-P4/16 40*40 - [-1, 1, MobileNetV3, [40, 240, 5, 1, True, True]] # 6 40*40 - [-1, 1, MobileNetV3, [48, 120, 5, 1, True, True]] # 7-P5/32 40*40 - [-1, 1, MobileNetV3, [48, 144, 5, 1, True, True]] # 8 40*40 - [-1, 1, MobileNetV3, [96,288,5,2,True,True]] # 9 20*20 - [-1, 1, MobileNetV3, [96,576,5,1,True,True]] # 10 20*20 - [-1, 1, MobileNetV3, [96,576,5,1,True,True]] # 11 20*20 # YOLO11n head head: - [-1, 1, nn.Upsample, [None, 2, \u0026#34;nearest\u0026#34;]] # 40*40 - [[-1, 6], 1, Concat, [1]] # cat backbone P4 - [-1, 2, C3k2, [512, False]] # 13 - [-1, 1, nn.Upsample, [None, 2, \u0026#34;nearest\u0026#34;]] - [[-1, 3], 1, Concat, [1]] # cat backbone P3 - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 13], 1, Concat, [1]] # cat head P4 - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 10], 1, Concat, [1]] # cat head P5 - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large) - [[17, 20, 23], 1, Detect, [nc]] # Detect(P3, P4, P5) 将其对应之前的网络结构图,其中exp size对应MobileNetV3类中hidden_dim。\n最后就是将上述模块的代码在tasks.py中进行调入,这里就不再详细赘述其过程了:\nfrom ultralytics.nn.modules import ( AIFI, C1, ... conv_bn_hswish, MobileNetV3 ) def parse_model(d, ch, verbose=True): ... base_modules = frozenset( { Classify, Conv, ... conv_bn_hswish, MobileNetV3 } ) 最后其计算量为4.0 GFLops,而原来YOLO11n的计算量为6.6 GFLops,一下子就减少了39%的计算量。\n参考文章:\nhttps://zhuanlan.zhihu.com/p/365119654 https://www.cnblogs.com/ZOMI/articles/18561132 https://www.bilibili.com/video/BV17QhAzPE9V\n","date":1764833826,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1764838369,"objectID":"c560a31912433da8f68d108e45f5c40d","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-replace-backbone-to-mobilenetv3-small-in-yolo/","publishdate":"2025-12-04T15:37:06+08:00","relpermalink":"/posts/how-to-replace-backbone-to-mobilenetv3-small-in-yolo/","section":"posts","summary":"将YOLO 11中的Backbone网络替换为MobileNet V3 Small","tags":["AI","CV"],"title":"YOLO11中替换Backbone为MobileNetV3-Small","type":"posts"},{"authors":["码力全开"],"categories":["AI","CV"],"content":"关于ShuffleNetV2网络结构的介绍可以参考。\n下面是对应的实战部分,首先在ultralytics源码目录nn/modules下创建1个shufflenetv2模块,其代码如下:\n#encoding:utf-8 import torch import torch.nn as nn def channel_shuffle(x, groups): batchsize, num_channels, height, width = x.data.size() channels_per_group = num_channels // groups # reshape x = x.view(batchsize, groups, channels_per_group, height, width) x = torch.transpose(x, 1, 2).contiguous() # flatten x = x.view(batchsize, -1, height, width) return x class CBRM(nn.Module): #conv BN ReLU Maxpool2d def __init__(self, c1, c2): # ch_in, ch_out super(CBRM, self).__init__() self.conv = nn.Sequential( nn.Conv2d(c1, c2, kernel_size=3, stride=2, padding=1, bias=False), nn.BatchNorm2d(c2), nn.ReLU(inplace=True), ) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) def forward(self, x): return self.maxpool(self.conv(x)) class Shuffle_Block(nn.Module): def __init__(self, ch_in, ch_out, stride): super(Shuffle_Block, self).__init__() if not (1 \u0026lt;= stride \u0026lt;= 2): raise ValueError(\u0026#39;illegal stride value\u0026#39;) self.stride = stride branch_features = ch_out // 2 assert (self.stride != 1) or (ch_in == branch_features \u0026lt;\u0026lt; 1) if self.stride \u0026gt; 1: self.branch1 = nn.Sequential( self.depthwise_conv(ch_in, ch_in, kernel_size=3, stride=self.stride, padding=1), nn.BatchNorm2d(ch_in), nn.Conv2d(ch_in, branch_features, kernel_size=1, stride=1, padding=0, bias=False), nn.BatchNorm2d(branch_features), nn.ReLU(inplace=True), ) self.branch2 = nn.Sequential( nn.Conv2d(ch_in if (self.stride \u0026gt; 1) else branch_features, branch_features, kernel_size=1, stride=1, padding=0, bias=False), nn.BatchNorm2d(branch_features), nn.ReLU(inplace=True), self.depthwise_conv(branch_features, branch_features, kernel_size=3, stride=self.stride, padding=1), nn.BatchNorm2d(branch_features), nn.Conv2d(branch_features, branch_features, kernel_size=1, stride=1, padding=0, bias=False), nn.BatchNorm2d(branch_features), nn.ReLU(inplace=True), ) @staticmethod def depthwise_conv(i, o, kernel_size, stride=1, padding=0, bias=False): return nn.Conv2d(i, o, kernel_size, stride, padding, bias=bias, groups=i) def forward(self, x): if self.stride == 1: x1, x2 = x.chunk(2, dim=1) # 按照维度1进行split out = torch.cat((x1, self.branch2(x2)), dim=1) else: out = torch.cat((self.branch1(x), self.branch2(x)), dim=1) out = channel_shuffle(out, 2) return out 然后在nn/modules/__init__.py中导入上述类:\nfrom .transformer import ( AIFI, ... TransformerLayer, ) from .shufflenetv2 import CBRM, Shuffle_Block __all__ = ( \u0026#34;AIFI\u0026#34;, ... \u0026#34;CBRM\u0026#34;, \u0026#34;Shuffle_Block\u0026#34; ) 之后修改nn/tasks.py中的代码:\nfrom ultralytics.nn.modules import ( AIFI, ... CBRM, Shuffle_Block ) def parse_model(d, ch, verbose=True): ... base_modules = frozenset( { Classify, Conv, ... CBRM, Shuffle_Block } ) 除了将其放置在base_modules变量中,还可以使用如下的方式:\n... elif m in frozenset({TorchVision, Index}): c2 = args[0] c1 = ch[f] args = [*args[1:]] elif m in (CBRM,Shuffle_Block): c1, c2 = ch[f], args[0] args = [c1,c2,*args[1:]] 我们构建参数让其可以满足对应模块的参数,分别为输入通道数、输出通道数,其中输入参数的值是根据上一层传入的。\n对此可以使用如下的代码查看每层的输入与输出维度:\nimport torch from ultralytics import YOLO model = YOLO(\u0026#34;yolo11.yaml\u0026#34;).model model.eval() x = torch.randn(1,3,640,640) print(\u0026#34;{:\u0026lt;4} {:\u0026lt;25} {:\u0026gt;25} {:\u0026gt;25}\u0026#34;.format(\u0026#34;Idx\u0026#34;,\u0026#34;Module\u0026#34;,\u0026#34;Input Shape\u0026#34;,\u0026#34;Output Shape\u0026#34;)) print(\u0026#34;-\u0026#34;*80) def hook_fn(module,input,output): if module in model.model: def extract_shape(o): if isinstance(o, torch.Tensor): return list(o.shape) elif isinstance(o,(list,tuple)): return [extract_shape(x) for x in o] else: return str(type(o)) input_shape = extract_shape(input)[0] output_shape = extract_shape(output) idx = list(model.model).index(module) print(\u0026#34;{:\u0026lt;4} {:\u0026lt;25} {:\u0026lt;25} {:\u0026gt;25}\u0026#34;.format(idx,module.__class__.__name__,str(input_shape), str(output_shape))) hooks = [m.register_forward_hook(hook_fn) for m in model.model] _ = model(x) for h in hooks: h.remove() 之后在cfg/models/11目录下新建1个yolo11n-shufflenetV2.yaml的配置文件,其内容如下:\nnc: 80 # number of classes scales: # model compound scaling constants, i.e. \u0026#39;model=yolo11n.yaml\u0026#39; will call yolo11.yaml with scale \u0026#39;n\u0026#39; # [depth, width, max_channels] n: [0.50, 0.25, 1024] # summary: 181 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs s: [0.50, 0.50, 1024] # summary: 181 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs m: [0.50, 1.00, 512] # summary: 231 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs l: [1.00, 1.00, 512] # summary: 357 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs x: [1.00, 1.50, 512] # summary: 357 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs # YOLO11n backbone backbone: # [from, repeats, module, args] - [-1, 1, CBRM, [32]] # 0-P2/4 160*160 - [-1, 1, Shuffle_Block, [128, 2]] # 1-P3/8 80*80 - [-1, 3, Shuffle_Block, [128, 1]] # 2 80*80 - [-1, 1, Shuffle_Block, [256, 2]] # 3-P4/16 40*40 - [-1, 7, Shuffle_Block, [256, 1]] # 4 40*40 - [-1, 1, Shuffle_Block, [512, 2]] # 5-P5/32 20*20 - [-1, 3, Shuffle_Block, [512, 1]] # 6 20*20 # YOLO11n head head: - [-1, 1, nn.Upsample, [None, 2, \u0026#34;nearest\u0026#34;]] # 11 40*40 - [[-1, 4], 1, Concat, [1]] # 12 …","date":1764331251,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1764386657,"objectID":"b0c77dd198fb250b8c6bea0af1e8afd3","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-replace-backbone-to-shufflenetv2-in-yolo11/","publishdate":"2025-11-28T20:00:51+08:00","relpermalink":"/posts/how-to-replace-backbone-to-shufflenetv2-in-yolo11/","section":"posts","summary":"如何替换YOLO11中Backbone为ShuffleNetV2","tags":["AI","CV"],"title":"YOLO11更换Backbone为ShuffleNetV2","type":"posts"},{"authors":["码力全开"],"categories":["AI","CV"],"content":"这里对GhostNet网络模型进行简单的介绍,其作为轻量级网络结构可用于边缘设备上。其中关于G-GhostNet的介绍可以参考GhostNets on Heterogeneous Devices via Cheap Operations。而第1个版本的介绍可以参考GhostNet: More Features from Cheap Operations。第2个版本的介绍可以参考GhostNetV2: Enhance Cheap Operation with Long-Range Attention。最后第3个版本的介绍可以参考GhostNetV3: Exploring the Training Strategies for Compact Models。\n该网络由Ghost模块组成,其思想是从第二层开始所有的卷积层的通道数是输出通道数的1/2,剩余的1/2通道的输出特征则由第一层卷积层的输出经过廉价操作产生。\n最后相关代码可以参考huawei-noah/Efficient-AI-Backbones ,查看代码比论文更容易理解。\n参考文章:\nhttps://zhuanlan.zhihu.com/p/109467941 https://cloud.tencent.com.cn/developer/article/2353617 https://blog.csdn.net/lishanlu136/article/details/142107037 https://zhuanlan.zhihu.com/p/109325275\n","date":1764207109,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1764331373,"objectID":"675bd1f9b0427dcffe999464db0b37ce","permalink":"https://zhuzhulang.github.io/blog/posts/ghostnet-introduction/","publishdate":"2025-11-27T09:31:49+08:00","relpermalink":"/posts/ghostnet-introduction/","section":"posts","summary":"关于GhostNet网络简单的叙述","tags":["AI","CV"],"title":"GhostNet网络简述","type":"posts"},{"authors":["码力全开"],"categories":["CV","AI"],"content":"这里介绍如何对YOLOv5网络进行修改,让其可以在边缘设备,例如RK3588上较好的运行。\n由于默认YOLOv5的输出形状为[1,3,25200,85],对于边缘设备而言,此时需要将其拆分为3个输出比较妥当。\n而优化后网络第17层如下:\n而对应网络第23层如下:\n可以看到,其中的SiLU被替换为ReLU,从而可以在边缘上较好的运行。另外原始网络中Detect模块的输出被拆分为3个输出,从而简化了其计算过程。\n","date":1763173322,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1763173579,"objectID":"b42de5a994b180f3108ba5db361a4479","permalink":"https://zhuzhulang.github.io/blog/posts/yolov5-in-edge-device/","publishdate":"2025-11-15T10:22:02+08:00","relpermalink":"/posts/yolov5-in-edge-device/","section":"posts","summary":"如何让YOLOv5可以在边缘设备上较好的运行","tags":["CV","AI"],"title":"YOLOv5在边缘设备上的优化","type":"posts"},{"authors":["码力全开"],"categories":["CV","AI"],"content":"下面介绍下如何通过YOLO进行图片中性别与年龄估计。首先是种族分类,可以参考。\n如果想检测图片中人物的性别可以考虑使用iic/cv_resnet34_face-attribute-recognition_fairface模型。如果想使用onnx,则可以分别考虑onnx-community/fairface_gender_image_detection-ONNX和onnx-community/fairface_age_image_detection-ONNX。其他模型可以参考。\n其相关代码类似如下:\n\u0026gt;\u0026gt;\u0026gt; from modelscope.pipelines import pipeline \u0026gt;\u0026gt;\u0026gt; from modelscope.utils.constant import Tasks \u0026gt;\u0026gt;\u0026gt; fair_face_attribute_func = pipeline(Tasks.face_attribute_recognition, \u0026#39;iic/cv_resnet34_face-attribute-recognition_fairface\u0026#39;) 在这个过程中会下载相关的人脸识别模型。之后开始人物的检测:\n\u0026gt;\u0026gt;\u0026gt; src_img_path = \u0026#39;cfa0234006d2b95630a4e026ea6ea721.jpeg\u0026#39; \u0026gt;\u0026gt;\u0026gt; raw_result = fair_face_attribute_func(src_img_path) 对应的检测结果类似如下:\n\u0026gt;\u0026gt;\u0026gt; raw_result {\u0026#39;scores\u0026#39;: [[1.0, 5.469796349188982e-08], [6.387997331103179e-08, 0.00021145127539057285, 0.23208525776863098, 0.738937258720398, 0.028579212725162506, 0.00018610214465297759, 5.655915629176889e-07, 4.1816949902795386e-08, 2.805190246757405e-10]], \u0026#39;labels\u0026#39;: [[\u0026#39;Male\u0026#39;, \u0026#39;Female\u0026#39;], [\u0026#39;0-2\u0026#39;, \u0026#39;3-9\u0026#39;, \u0026#39;10-19\u0026#39;, \u0026#39;20-29\u0026#39;, \u0026#39;30-39\u0026#39;, \u0026#39;40-49\u0026#39;, \u0026#39;50-59\u0026#39;, \u0026#39;60-69\u0026#39;, \u0026#39;70+\u0026#39;]]} 我们对其结果按照最大值获取其结果:\n\u0026gt;\u0026gt;\u0026gt; labels = raw_result[\u0026#34;labels\u0026#34;] \u0026gt;\u0026gt;\u0026gt; scores = raw_result[\u0026#34;scores\u0026#34;] \u0026gt;\u0026gt;\u0026gt; [max(x) for x in scores] [1.0, 0.738937258720398] \u0026gt;\u0026gt;\u0026gt; [x.index(max(x)) for x in scores] [0, 3] \u0026gt;\u0026gt;\u0026gt; labels[0][0] \u0026#39;Male\u0026#39; \u0026gt;\u0026gt;\u0026gt; labels[1][3] \u0026#39;20-29\u0026#39; 可以看出其结果准确率还是蛮高的。\n参考文章:\nhttps://www.cnblogs.com/odesey/p/16800014.html https://talhassner.github.io/home/publication/2015_CVPR https://zhuanlan.zhihu.com/p/111721652\n","date":1762573157,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1762783666,"objectID":"bb635c44cd2d7a455b144aac103d8611","permalink":"https://zhuzhulang.github.io/blog/posts/use-yolo-for-humman-age-and-gender-estimiation/","publishdate":"2025-11-08T11:39:17+08:00","relpermalink":"/posts/use-yolo-for-humman-age-and-gender-estimiation/","section":"posts","summary":"如何使用YOLO模型进行性别和年龄估计","tags":["CV","AI"],"title":"YOLO性别与年龄估计","type":"posts"},{"authors":["码力全开"],"categories":["AI实战","CV"],"content":"这里介绍下如何在RK3588的开发板中,使用Docker进行模型的推理。主要还是要开启相关的权限,并将相关文件映射到容器中。这样在容器内就可以调用RK3588主板上的NPU进行推理。\n其主要命令如下:\ndocker run -it --privileged -v /dev/drm:/dev/drm [容器ID] /bin/bash 而在容器内安装相应的软件包:\npip install rknn-toolkit-lite2==2.3.2 这样就可以以最小化的形式将程序运行起来。\n","date":1761225565,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1761226237,"objectID":"79f67ce2d7762cfc3c78d7a56b6aad21","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-rk3588-npu-in-docker/","publishdate":"2025-10-23T21:19:25+08:00","relpermalink":"/posts/how-to-use-rk3588-npu-in-docker/","section":"posts","summary":"如何在Docker中调用主板上的NPU","tags":["AI实战","CV"],"title":"在RK3588主板上使用Docker进行推理","type":"posts"},{"authors":["码力全开"],"categories":["CV"],"content":"帧率可以简单理解为1秒内播放了多少张图片。一般来说,视频中相邻的两帧之间有相同的时间间隔,而修改帧率则需要在响应的间隔中插入相同的帧。\n对于构造不存在的帧,我们称之为插帧(Frame Interpolation),以下是一些常见的方法:\nDuplication,复制相近的帧 Blend,用相邻的两帧进行混合 Motion Interpolation,结合图像运动来构造中间帧,类似于运动补偿 对于第1种方式,在生成当前的中间帧的过程中涉及到两个输入帧,分别为小于输出时间并且最接近该输出时间的输入帧以及大于输出时间并且最接近输出时间的输入帧。我们可以称之为参考帧。而从这两个参考帧中选取时间上最接近中间帧输出时间的一帧,对这一帧进行复制,作为当前的中间帧进行输出。 而第2种方法是将两帧进行混合,其代码类似如下:\nframe = cv2.addWeighted( prev_frame, 0.7, next_frame, 0.3, 0 ) 而对于第3种方式可以使用类似如下的代码:\nprev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY) # 计算光流（运动矢量） flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0) # 生成中间帧（动态补偿） h, w = flow.shape[:2] map_x = np.tile(np.arange(w), (h, 1)) + flow[...,0] map_y = np.tile(np.arange(h), (w, 1)).T + flow[...,1] # map_y = np.swapaxes(np.tile(np.arange(h), (w, 1)), 0, 1) + flow[...,1] mapped_frame = cv2.remap(next_frame, map_x.astype(np.float32), map_y.astype(np.float32), cv2.INTER_LINEAR) 这里主要采用光流法,其适用于中等运行复杂度的场景,而对于大型运行则可以考虑使用帧间对准技术。此时可以考虑python_video_stab项目。\n对于帧率的变化,下面举一个简单的例子。对于25帧的视频,那么每隔1/25=40ms就是一帧,如果将其修改为15帧,那么每帧的间隔为1/15=66.67ms。此时就可以使用如下的代码进行视频流的读取:\nwhile cap.isOpened(): start_time = time.time() ret,frame = cap.read() if not ret: print(\u0026#34;no frame\u0026#34;) break # 跳帧 if frame_count % skip_frame == 0: # height,width = frame.shape[:2] frame = cv2.resize(frame,(width//2,height//2)) # 补偿帧 compensated_frame = frame_interpolation(prev_frame, frame, 0.5) cv2.imshow(\u0026#34;Frame\u0026#34;, compensated_frame) cv2.waitKey(20) cv2.imshow(\u0026#34;Frame\u0026#34;,frame) prev_frame = frame frame_count += 1 elapsed = time.time() - start_time # if elapsed \u0026lt; frame_interval: # time.sleep(frame_interval-elapsed) remaining = max(1, int((frame_interval - elapsed) * 1000)) if cv2.waitKey(remaining) \u0026amp; 0xFF == ord(\u0026#34;q\u0026#34;): break 参考文章:\nhttps://blog.csdn.net/zhying719/article/details/121709862\n","date":1758443641,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1758717431,"objectID":"d4c63c5539fdeb03cff488c7a3bd0399","permalink":"https://zhuzhulang.github.io/blog/posts/something-about-interpolate-frame/","publishdate":"2025-09-21T16:34:01+08:00","relpermalink":"/posts/something-about-interpolate-frame/","section":"posts","summary":"关于视频插帧的一些常用方法","tags":["CV"],"title":"视频插帧方案简述","type":"posts"},{"authors":["码力全开"],"categories":["AI","CV"],"content":"这里介绍下如何为YOLO模型添加注意力机制,主要在于卷积神经网络CNN劣势在于无法很好关注重要的位置。而通过引入注意力机制,从而加强其对一些物品的注意。\n之前的文章已经介绍过YOLO模型的结构,要为模型添加注意力机制,可以从如下3个方面下手:\n选择相应的注意力模块 为模型添加注意力模块 对模型进行训练 对模型进行推理 下面从第一步开始介绍。相应的注意力机制模块主要有:\nSENet,让网络自己学会哪些通道重要,对于不重要的通道进行抑制 CBAM,可以看成通道注意力+空间注意力的集成,在上一步基础上添加看图中哪些位置的能力 BAM,瓶颈注意力模块,与CBAM不同,其使用并联结构 Non-Local Attention或Non-Local Neural Network,是一种全局建模方法,通过计算任意两个位置之间的关系,实现信息的长距离传递 Transformer风格注意力,如Contextual Transformer,兼顾局部细节提取和上下文建模。而ECA(Efficient Channel Attention)是对SENet的改进,计算更快。而Coordinate Attention更适用于移动设备和轻量网络 而引入注意力模块,可以在ultralytics/nn目录下新建对应的模块,并编写对应网络结构代码。而在ultralytics/nn/tasks.py中导入之前的模块,修改parse_model函数中内容,添加相应加载方式。\n接着为了避免与之前模型结构冲突,在ultralytics/cfg/models/对应YOLO版本目录下新建1个yaml文件,其中backbone中添加相应的注意力机制模块,并修改head中网络的索引。\n一切准确就绪就可以加载模型并调用其train方法进行训练了,此时配置文件设置为之前新建的模型yaml文件。\n可以看到整个流程还是比较简单,而可视化注意力可以考虑使用visualiz包,通过以下方式进行安装:\npip install visualize==0.5.0 参考文章:\nhttps://blog.csdn.net/shuai_beibei/article/details/142798188\nhttps://blog.csdn.net/2403_88150975/article/details/148263735\nhttps://www.cnblogs.com/Brisling/p/16838987.html\nhttps://blog.csdn.net/weixin_52603404/article/details/145930505\nhttps://zhuanlan.zhihu.com/p/356798637\n","date":1757855761,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757857148,"objectID":"0ca646dae9474639314cda6ee9141872","permalink":"https://zhuzhulang.github.io/blog/posts/add-attention-module-in-yolo/","publishdate":"2025-09-14T21:16:01+08:00","relpermalink":"/posts/add-attention-module-in-yolo/","section":"posts","summary":"如何为YOLO添加注意力机制","tags":["AI","CV"],"title":"YOLO添加注意力机制","type":"posts"},{"authors":["码力全开"],"categories":["AI","CV"],"content":"这里将对YOLO版本11的网络结构进行简单的介绍,了解其大体结构,有一个整体的认识即可,当前没必要过于深入了解每个模块。\n首先是yolo11.yaml中的内容:\n# YOLO11n backbone backbone: # [from, repeats, module, args] - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2 - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4 - [-1, 2, C3k2, [256, False, 0.25]] - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8 - [-1, 2, C3k2, [512, False, 0.25]] - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16 - [-1, 2, C3k2, [512, True]] - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32 - [-1, 2, C3k2, [1024, True]] - [-1, 1, SPPF, [1024, 5]] # 9 - [-1, 2, C2PSA, [1024]] # 10 # YOLO11n head head: - [-1, 1, nn.Upsample, [None, 2, \u0026#34;nearest\u0026#34;]] - [[-1, 6], 1, Concat, [1]] # cat backbone P4 - [-1, 2, C3k2, [512, False]] # 13 - [-1, 1, nn.Upsample, [None, 2, \u0026#34;nearest\u0026#34;]] - [[-1, 4], 1, Concat, [1]] # cat backbone P3 - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 13], 1, Concat, [1]] # cat head P4 - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 10], 1, Concat, [1]] # cat head P5 - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large) - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5) 其中骨干网络由11个模块组成,其中第1层是Conv,用于下采样。其输出通道为64,采用3x3卷积,步长stride为2。在上述行中-1表示接收上一层的输出,而1表示重复1次。紧接着又是1个Conv层,之后是重复2次的C3k2层,下一个还是Conv层。依次类推,直到最后一层C2PSA层。\n同理,对于网络结构中head部分亦是如此,首先最后1层输出会被输入到nn.Upsample中进行上采样,紧接着把上一层和backone索引为第6层的C3k2(标记为P4的层)使用Concat层拼接起来。\n完整的网络结构可以如下图所示:\n可以看到还是蛮简单的。\n下面是其调用方式:\nimport cv2 import torch from ultralytics import YOLO state_dict = model.state_dict() # 导出权重文件 torch.save(state_dict,\u0026#34;yolov8.pth\u0026#34;) img = cv2.imread(\u0026#34;twins-dog.jpg\u0026#34;) model = YOLO(\u0026#34;yolov8-seg.yaml\u0026#34;) model.load_state_dict(torch.load(\u0026#34;yolov8.pth\u0026#34;,weights_only=True)) result = model.predict(img) for res in result: boxes = res.boxes xyxy = boxes.xyxy for x in xyxy: x = x.cpu().numpy().astype(int).tolist() cv2.rectangle(img,tuple(x[:2]),tuple(x[2:]),(0,0,255),1) cv2.imshow(\u0026#34;Frame\u0026#34;,img) cv2.waitKey(0) cv2.destroyAllWindows() 除此之外,还可以使用load方法加载ckpt导出的模型,但是要保证其后缀为pt:\nckpt = model.ckpt torch.save(ckpt,\u0026#34;yolov8.ckpt\u0026#34;) model = model.load(\u0026#34;yolov8.pt\u0026#34;) 将ckpt修改为pt即可正常导入。\n","date":1757209758,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757238270,"objectID":"d1a28d5fc06b5199dcd4b0cff2f452c2","permalink":"https://zhuzhulang.github.io/blog/posts/yolov11-architecture-introduction/","publishdate":"2025-09-07T09:49:18+08:00","relpermalink":"/posts/yolov11-architecture-introduction/","section":"posts","summary":"对YOLO11网络结构进行简单的介绍","tags":["AI","CV"],"title":"YOLOv11网络结构简述","type":"posts"},{"authors":["码力全开"],"categories":["AI","CV"],"content":"在这里将简单介绍如何在YOLO中对物体进行跟踪,实际上在YOLO官方的库中提供了相关的track函数。而默认支持两种算法:\nBoT-SORT ByteTrack 关于相关的调用方法可以参考原文。\n这里主要介绍ONNX格式的物体跟踪,使用的是ByteTrack。源码参考bytetrack-opencv-onnxruntime。\n到百度网盘中下载相关的模型文件后,就可以开始进行调用了。难点在于如果使用python的话,在Windows下安装cython_bbox包会出现安装不上的问题。\n此时有两种方法来解决,其中第1种方法是将源码下载修改,这里修改是0.1.3版本,将setup.py中代码进行如下的修改:\nif os.name == \u0026#39;nt\u0026#39;: compile_args = {\u0026#39;gcc\u0026#39;: [\u0026#39;/Qstd=c99\u0026#39;]} else: compile_args = [\u0026#39;-Wno-cpp\u0026#39;] 或者采用cython_bbox中的代码。\n另外更为简单的方法是直接安装cython_bbox_windows包或者使用w64devkit进行编译安装。\n当然ByteTrack还是会出现物体跟丢的问题,同一个人还是分配给其不同的ID。比如视频sample.mp4中36号的靓女经过石像就变成了90。而对于静止的物体其ID还是保持不变的。\n","date":1757122532,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757123638,"objectID":"3eeb81929ba2e186c75e72322f8f0e5d","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-track-object-in-yolo/","publishdate":"2025-09-06T09:35:32+08:00","relpermalink":"/posts/how-to-track-object-in-yolo/","section":"posts","summary":"如何在YOLO中进行物体跟踪,特别是使用onnxruntime","tags":["AI","CV"],"title":"YOLO物体跟踪","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"MoE是大模型中的常见的术语,其是Mixture-Of-Experts的缩写,即专家混合。如果读者对机器学习有所了解的话,应该很容易理解其实MoE就是个换汤不换药的东西。\n在传统机器学习中有集成学习(Ensemble Learning)这个策略。说个大家都通俗易懂的方式,那就是俗语\u0026#34;三个臭皮匠,赛过诸葛亮\u0026#34;。毕竟个人的能力都是有限的,而多个人作为团队作战则可以弥补彼此的劣势,从而最大程度发挥优势。\n在机器学习之前,一般都会经历专家模型这个阶段。而MoE中选择是选择多个专家的方式,意思是听从多个专家的意见,然后自己总结归纳作出决策。而总结归纳这个过程,一般会采用均值或加权的方式。\nMoE作为一种由专家模型和门控模型组成的稀疏门控制的深度学习技术,由多个子模型(称之为专家)组成,每个子模型都是一个局部模型。其结构如下图所示:\n其中门控模型可以选择性让输入特征经过/激活某些子模型,从而减少训练的计算量。通过稀疏激活控制方式,通过Top-K机制筛选权重最高的专家来减少计算量。其通过激活少量专家,大幅降低推理和训练成本。\n可以看到AI中的一些概念还是源自生活的实践。\n","date":1755219471,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1755222520,"objectID":"606e87c978536357ba14b590af890f9a","permalink":"https://zhuzhulang.github.io/blog/posts/introduction-of-moe/","publishdate":"2025-08-15T08:57:51+08:00","relpermalink":"/posts/introduction-of-moe/","section":"posts","summary":"关于大模型中MoE的简单学习","tags":["AI"],"title":"MoE简述","type":"posts"},{"authors":["码力全开"],"categories":["AI实战"],"content":"对于点云特征点提取,主要有如下一些算法:\n基于曲率的特征点提取,对于曲率大的地方可能是特征点,比如关节、耳朵边缘 基于法向量变化的特征点提取,对于法向量变化大的地方就是特征点 ISS特征点,对噪声鲁棒,适合曲面特征 3D SIFT,基于尺度空间,适合不同尺度的特征 深度学习方法,如PointNet自动学习特征 对于法向量夹角算法,首先计算点云中每个点的法向量,然后计算邻域内法向量的夹角,根据夹角的大小判断是否为特征点。其中法向量计算需要用到PCA,而邻域搜索可以使用k近邻或半径邻域。\n其中的数据集来自斯坦福大学的bunny模型,利用其中的bun315.ply数据。\n下面是其关键步骤:\n点云加载与预处理,包括利用下采样减少计算量和统计滤波去噪提高精度 法向量的计算,利用Open3D的estimate_normals计算法向量,通过orient_normals_towards_camera_location统一法向量方向 特征点提取,构建KDTree进行高效的邻域搜索,并计算每个点与其邻域点的法向量夹角均值,最后根据设定的角度阈值筛选特征点,其中夹角均值越大,表明该点越可能是特征点 结果可视化,将原始点云显示为灰色,而特征点显示为红色,并绘制法向量夹角均值分布直方图,从而帮助确定合适的阈值 下面是其其实现代码:\nimport open3d as o3d import numpy as np import matplotlib.pyplot as plt def load_point_cloud(file_path): \u0026#34;\u0026#34;\u0026#34;加载点云数据\u0026#34;\u0026#34;\u0026#34; pcd = o3d.io.read_point_cloud(file_path) if pcd.is_empty(): raise ValueError(\u0026#34;无法加载点云文件，请检查路径是否正确\u0026#34;) print(f\u0026#34;成功加载点云，包含 {len(pcd.points)} 个点\u0026#34;) return pcd def preprocess_point_cloud(pcd, voxel_size=0.008): \u0026#34;\u0026#34;\u0026#34; 点云预处理：针对脊背调整参数 脊背细节丰富，采用较小的体素大小保留特征 \u0026#34;\u0026#34;\u0026#34; # 下采样：体素尺寸减小（默认0.008m），保留脊背细节 down_pcd = pcd.voxel_down_sample(voxel_size=voxel_size) # 统计滤波：放宽去噪阈值，避免过滤掉脊背边缘点 cl, ind = down_pcd.remove_statistical_outlier(nb_neighbors=15, std_ratio=2.5) inlier_cloud = down_pcd.select_by_index(ind) print(f\u0026#34;预处理后点云包含 {len(inlier_cloud.points)} 个点\u0026#34;) return inlier_cloud def extract_spine_roi(pcd, z_min_ratio=0.6): \u0026#34;\u0026#34;\u0026#34; 提取脊背ROI（感兴趣区域）：利用高度信息筛选背部区域 脊背位于身体较高位置，通过z轴坐标过滤 \u0026#34;\u0026#34;\u0026#34; points = np.asarray(pcd.points) # 计算z轴坐标范围，取上部区域作为ROI z_max = np.max(points[:, 2]) z_min = z_max * z_min_ratio # 可根据实际数据调整比例（0.5-0.7） roi_indices = np.where(points[:, 2] \u0026gt; z_min)[0] spine_roi = pcd.select_by_index(roi_indices) print(f\u0026#34;脊背ROI包含 {len(spine_roi.points)} 个点\u0026#34;) return spine_roi def compute_normals(pcd, radius=0.04, max_nn=20): \u0026#34;\u0026#34;\u0026#34; 计算法向量：针对脊背平缓曲面调整参数 减小搜索半径和邻域点数，提高法向量对局部曲率的敏感性 \u0026#34;\u0026#34;\u0026#34; pcd.estimate_normals( search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=radius, max_nn=max_nn) ) # 法向量方向统一指向身体外侧（脊背朝上，法向量向上） pcd.orient_normals_towards_camera_location(camera_location=np.array([0, 0, z_max*1.5])) return pcd def extract_feature_points_by_normal_angle(pcd, k=15, angle_thresh=20): \u0026#34;\u0026#34;\u0026#34; 提取特征点：针对脊背调整参数 脊背曲率变化较平缓，降低角度阈值以捕捉细微特征 \u0026#34;\u0026#34;\u0026#34; kdtree = o3d.geometry.KDTreeFlann(pcd) points = np.asarray(pcd.points) normals = np.asarray(pcd.normals) num_points = len(points) angle_means = [] for i in range(num_points): [_, idx, _] = kdtree.search_knn_vector_3d(points[i], k) neighbor_normals = normals[idx[1:], :] current_normal = normals[i, :] # 计算夹角（脊背特征点的邻域法向量夹角较小但有明显变化） dots = np.dot(neighbor_normals, current_normal) dots = np.clip(dots, -1.0, 1.0) angles = np.arccos(dots) angle_mean = np.mean(np.rad2deg(angles)) angle_means.append(angle_mean) # 脊背特征点的夹角均值通常比四肢小，降低阈值 angle_means = np.array(angle_means) feature_indices = np.where(angle_means \u0026gt; angle_thresh)[0] feature_pcd = pcd.select_by_index(feature_indices) print(f\u0026#34;从脊背ROI中提取到 {len(feature_pcd.points)} 个特征点\u0026#34;) return feature_pcd, angle_means def visualize_results(original_pcd, roi_pcd, feature_pcd): \u0026#34;\u0026#34;\u0026#34;可视化：区分原始点云、ROI和特征点\u0026#34;\u0026#34;\u0026#34; original_pcd.paint_uniform_color([0.8, 0.8, 0.8]) # 原始点云：浅灰 roi_pcd.paint_uniform_color([0.5, 0.5, 0.5]) # 脊背ROI：深灰 feature_pcd.paint_uniform_color([1.0, 0.0, 0.0]) # 特征点：红色 o3d.visualization.draw_geometries( [original_pcd, roi_pcd, feature_pcd], window_name=\u0026#34;脊背特征点提取结果\u0026#34;, width=1200, height=800, point_show_normal=False ) def plot_angle_distribution(angle_means, angle_thresh): \u0026#34;\u0026#34;\u0026#34;绘制脊背区域的夹角分布，辅助调整阈值\u0026#34;\u0026#34;\u0026#34; plt.figure(figsize=(10, 6)) plt.hist(angle_means, bins=30, alpha=0.7, color=\u0026#39;green\u0026#39;) plt.axvline(x=angle_thresh, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;, label=f\u0026#39;Threshold: {angle_thresh}°\u0026#39;) plt.xlabel(\u0026#39;Mean Angle\u0026#39;) plt.ylabel(\u0026#39;Points\u0026#39;) plt.title(\u0026#39;Distribution\u0026#39;) plt.legend() plt.grid(True, alpha=0.3) plt.show() def main(): # 针对脊背的参数设置（单位：米） file_path = \u0026#34;bun315.ply\u0026#34; # 点云文件路径 voxel_size = 0.008 # 体素大小：较小值保留脊背细节 z_min_ratio = 0.65 # 脊背ROI的z轴下限比例 normal_radius = 0.04 # 法向量计算半径：适应脊背宽度 k_neighbors = 15 # 邻域点数：减少平滑效应 angle_threshold = 22 # 夹角阈值：降低以捕捉平缓变化 try: # 1. 加载点云 pcd = load_point_cloud(file_path) global z_max # 全局变量，用于法向量方向调整 z_max = np.max(np.asarray(pcd.points)[:, 2]) # 2. 预处理（保留细节） processed_pcd = preprocess_point_cloud(pcd, voxel_size) # 3. 提取脊背ROI（关键步骤：减少无关区域干扰） spine_roi = extract_spine_roi(processed_pcd, z_min_ratio) # 4. 计算法向量（适应脊背曲面） spine_roi = compute_normals(spine_roi, normal_radius) # 5. 提取脊背特征点 feature_pcd, angle_means = extract_feature_points_by_normal_angle( spine_roi, k=k_neighbors, angle_thresh=angle_threshold ) # 6. 保存结果 o3d.io.write_point_cloud(\u0026#34;spine_feature_points.ply\u0026#34;, feature_pcd) print(\u0026#34;脊背特征点已保存为 pig_spine_feature_points.ply\u0026#34;) # 7. 可视化 visualize_results(processed_pcd, spine_roi, feature_pcd) # 8. 分析夹角分布 plot_angle_distribution(angle_means, angle_threshold) except Exception as e: print(f\u0026#34;处理过程中发生错误: {str(e)}\u0026#34;) 其输出如下:\n成功加载点云，包含 35336 个点 预处理后点云包含 587 个点 脊背ROI包含 198 个点 从脊背ROI中提取到 74 个特征点 其最终效果如下:\n从上图可以看出,我们成功将其兔子脊背特征点标注了出来。\n","date":1754464306,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754467564,"objectID":"6848800c75fcd79d31f0a87439fb9a83","permalink":"https://zhuzhulang.github.io/blog/posts/pointcloud-extract-feature-point/","publishdate":"2025-08-06T15:11:46+08:00","relpermalink":"/posts/pointcloud-extract-feature-point/","section":"posts","summary":"如何利益法向量夹角算法提取点云中的特征点","tags":["AI实战"],"title":"使用法向量夹角算法提取特征点","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"当要训练的模型数据比较大时,可以通过并行技术加速训练。在PyTorch的distributed包中提供了分布式数据并行的实现。\n首先先介绍下DataParallel(缩写为DP)与DistributedDataParallel(缩写为DDP)的区别。其中前者是单进程多线程的,其只能在单机上进行运行。相反后者是多进程并支持单/多机训练的。\n为了创建DDP模型,首先需要正确设置进程组。下面是一个简单的示例代码:\nimport os import sys import tempfile import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP def setup(rank, world_size): os.environ[\u0026#39;MASTER_ADDR\u0026#39;] = \u0026#39;localhost\u0026#39; os.environ[\u0026#39;MASTER_PORT\u0026#39;] = \u0026#39;12355\u0026#39; # initialize the process group dist.init_process_group(\u0026#34;gloo\u0026#34;, rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() 其中关于DistributedDataParallel的内容可以参考。\n需要注意的是,在Windows系统上torch.distributed包只支持Gloo、FileStore和TcpStore后端。通过init_process_group初始化进程组,因为分布式数据并行整个还是需要进行通信的,否则也不知道其完成进度。\n之后创建1个简单的模块,并使用DDP进行装饰并使用一些虚拟的输入数据。\nclass ToyModel(nn.Module): def __init__(self): super(ToyModel, self).__init__() self.net1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.net2 = nn.Linear(10, 5) def forward(self, x): return self.net2(self.relu(self.net1(x))) def demo_basic(rank, world_size): print(f\u0026#34;Running basic DDP example on rank {rank}.\u0026#34;) # 初始化GPU进程组 setup(rank, world_size) # create model and move it to GPU with id rank model = ToyModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) optimizer.zero_grad() outputs = ddp_model(torch.randn(20, 10)) labels = torch.randn(20, 5).to(rank) loss_fn(outputs, labels).backward() optimizer.step() # 对进程组进行清理 cleanup() print(f\u0026#34;Finished running basic DDP example on rank {rank}.\u0026#34;) def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) 而点对点通信主要通过阻塞的send和recv函数来实现,或者使用立即计数的isend和irecv函数。关于其集体的通信的方式主要有如下几种:\nScatter,平均划分 Gather,汇聚 Reduce All-Reduce Broadcast,广播 All-Gather Gather与Reduce都将数据汇总在一个节点上,但是Reduce会对这些数据进行相加得到1个值,而Gather是得到1个列表。\n与点对点通信相反,集体的方式允许组内所有进程进行通信。可以通过dist.new_group方法创建一个组。\n而数据的采样可以使用DistributedSampler。这样可以确保分布式中数据量保持不变。\n下面是在单机双GPU上训练的模型代码,比如在T4 x 2的服务器上:\nimport os import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import Dataset, DataLoader from torch.utils.data.distributed import DistributedSampler # 1. 定义简单模型 class SimpleModel(nn.Module): def __init__(self): super().__init__() self.net = nn.Sequential( nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 50), nn.ReLU(), nn.Linear(50, 2) ) def forward(self, x): return self.net(x) # 2. 定义虚拟数据集 class DummyDataset(Dataset): def __init__(self, size=10000): self.data = torch.randn(size, 10) self.targets = torch.randint(0, 2, (size,)) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.targets[idx] # 3. 训练函数 def train(rank, world_size): # 初始化进程组 os.environ[\u0026#39;MASTER_ADDR\u0026#39;] = \u0026#39;localhost\u0026#39; os.environ[\u0026#39;MASTER_PORT\u0026#39;] = \u0026#39;12355\u0026#39; dist.init_process_group(\u0026#34;nccl\u0026#34;, rank=rank, world_size=world_size) # 设置当前GPU设备 torch.cuda.set_device(rank) # 创建模型并移至GPU model = SimpleModel().to(rank) model = DDP(model, device_ids=[rank]) # 定义损失函数和优化器 criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # 准备数据加载器 dataset = DummyDataset() sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, batch_size=32, sampler=sampler) # 训练循环 for epoch in range(10): sampler.set_epoch(epoch) # 确保每个epoch有不同的shuffle model.train() for batch_idx, (data, target) in enumerate(dataloader): data, target = data.to(rank), target.to(rank) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() if batch_idx % 100 == 0: print(f\u0026#34;Rank {rank}, Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\u0026#34;) # 清理进程组 dist.destroy_process_group() if __name__ == \u0026#34;__main__\u0026#34;: world_size = 2 # 使用2个GPU torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True) 将上述代码保存在ddp_train.py中,之后在单机双GPU的Linux系统上进行运行:\npython ddp_train.py 之后就可以看到其输出了。\n最后,还可以借助其他一些框架,如DeepSpeed简化分布式数据的训练过程。\n参考文章:\nhttps://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html\nhttps://docs.pytorch.org/tutorials/intermediate/dist_tuto.html\nhttps://www.cnblogs.com/liyier/p/18136458\n","date":1754275550,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754358738,"objectID":"1e4b3cb14e74961899217af5d704b09d","permalink":"https://zhuzhulang.github.io/blog/posts/pytorch-distributed-data-parallel/","publishdate":"2025-08-04T10:45:50+08:00","relpermalink":"/posts/pytorch-distributed-data-parallel/","section":"posts","summary":"如何使用DDP实现分布式数据并行训练","tags":["AI"],"title":"PyTorch分布式数据并行入门","type":"posts"},{"authors":["码力全开"],"categories":["web"],"content":"这里将介绍如何使用vue-cli快速进行应用工程化。在此之前,需要进行一些额外的操作:\nnpm config set registry http://registry.npmmirror.com npm install -g @vue/cli@4.5.8 这里先配置其镜像地址为国内的淘宝镜像,之后使用全局方式安装vue-cli,版本为4.5.8。对于版本的选择可以使用如下的方式进行查看:\nnpm view @vue/cli versions 一切准备就绪后,就可以开始后续的操作了。首先创建一个应用:\n$ vue create hello-world 当然也可以使用vue ui以图形化界面的方式进行项目的创建。整个过程需要联网,否则是无法成功创建项目的。其中src/main.js中的代码如下:\nimport { createApp } from \u0026#39;vue\u0026#39; import App from \u0026#39;./App.vue\u0026#39; createApp(App).mount(\u0026#39;#app\u0026#39;) 接着使用如下的命令运行vue项目:\nnpm run serve 为了后续可以在vue中使用HTTP请求,我们可以选择安装axios库,在项目工程目录下运行如下命令:\n$ npm install --save axios vue-axios 安装完成可以在package.json文件中看到更新后的依赖信息:\n\u0026#34;dependencies\u0026#34;: { \u0026#34;axios\u0026#34;: \u0026#34;^1.11.0\u0026#34;, \u0026#34;core-js\u0026#34;: \u0026#34;^3.8.3\u0026#34;, \u0026#34;vue\u0026#34;: \u0026#34;^3.2.13\u0026#34;, \u0026#34;vue-axios\u0026#34;: \u0026#34;^3.5.2\u0026#34; }, 下面我们使用Element-Plus为界面进行美化,在此之前先进行安装相应的库:\n$ npm install --save element-plus 在main.js中添加如下的内容:\nimport ElementPlus from \u0026#39;element-plus\u0026#39; import \u0026#39;element-plus/dist/index.css\u0026#39; const app = createApp(App) app.use(ElementPlus) app.mount(\u0026#39;#app\u0026#39;) 我们为之前的例子添加1个按钮,当点击时将IP地址对应的经纬度显示出来,其对应代码如下:\n\u0026lt;template\u0026gt; \u0026lt;el-button type=\u0026#34;success\u0026#34; @click=\u0026#34;check_ip\u0026#34;\u0026gt;成功\u0026lt;/el-button\u0026gt; \u0026lt;el-table :data=\u0026#34;tableData\u0026#34;\u0026gt; \u0026lt;el-table-column prop=\u0026#34;regionName\u0026#34; label=\u0026#34;省份\u0026#34;\u0026gt;\u0026lt;/el-table-column\u0026gt; \u0026lt;el-table-column prop=\u0026#34;city\u0026#34; label=\u0026#34;城市\u0026#34;\u0026gt;\u0026lt;/el-table-column\u0026gt; \u0026lt;el-table-column prop=\u0026#34;lat\u0026#34; label=\u0026#34;纬度\u0026#34;\u0026gt;\u0026lt;/el-table-column\u0026gt; \u0026lt;el-table-column prop=\u0026#34;lon\u0026#34; label=\u0026#34;经度\u0026#34;\u0026gt;\u0026lt;/el-table-column\u0026gt; \u0026lt;/el-table\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; import axios from \u0026#34;axios\u0026#34; export default { name: \u0026#39;HelloWorld\u0026#39;, data(){ return { tableData: [] } }, props: { msg: String }, methods:{ check_ip(){ axios.get(\u0026#34;http://ip-api.com/json/114.114.114.114\u0026#34;).then(res=\u0026gt;{ this.tableData.push(res.data); }).catch(err=\u0026gt;{ console.error(err) }); } } } \u0026lt;/script\u0026gt; 一切准备就绪后就可以使用下面的命令构建工程了:\nnpm run build 紧接着我们来看vue的路由管理,首先安装vue-router模块。\nnpm install vue-router@4 -s 路由的作用是页面管理,我们将定义好的Vue组件绑定到指定的路由,然后通过路由指定在何时或何处渲染这个组件。在src/components目录下创建2个文件,其中MyDemo1.vue的内容如下:\n\u0026lt;template\u0026gt; \u0026lt;h1\u0026gt;示例页面1\u0026lt;/h1\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; export default {\t}; \u0026lt;/script\u0026gt; 而MyDemo2.vue的内容如下:\n\u0026lt;template\u0026gt; \u0026lt;h1\u0026gt;示例页面2\u0026lt;/h1\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; export default {\t}; \u0026lt;/script\u0026gt; 接着修改App.vue文件的内容:\n\u0026lt;template\u0026gt; \u0026lt;h1\u0026gt;Hello App!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt; \u0026lt;RouterLink to=\u0026#34;/demo1\u0026#34;\u0026gt;页面一\u0026lt;/RouterLink\u0026gt; \u0026lt;br/\u0026gt; \u0026lt;RouterLink to=\u0026#34;/demo2\u0026#34;\u0026gt;页面二\u0026lt;/RouterLink\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;RouterView /\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; export default { name: \u0026#39;App\u0026#39;, components: { } } \u0026lt;/script\u0026gt; 其中router-link组件是一个自定义的链接组件,其允许在不重新加载页面的情况下更改页面的URL。而router-view用来渲染与当前URL对应的组件。 再修改项目中的main.js文件,在其中进行路由的定义与注册。\nimport { createRouter, createWebHashHistory } from \u0026#34;vue-router\u0026#34; import Demo1 from \u0026#34;./components/MyDemo1.vue\u0026#34; import Demo2 from \u0026#34;./components/MyDemo2.vue\u0026#34; const app = createApp(App) const routes = [ { path: \u0026#34;/demo1\u0026#34;, component: Demo1 }, { path: \u0026#34;/demo2\u0026#34;, component: Demo2 } ] const router = createRouter({ history: createWebHashHistory(), routes }) const app = createApp(App) app.use(router) app.use(ElementPlus,VueAxios,axios) app.mount(\u0026#39;#app\u0026#39;) 最后我们来看下如何将上述多个页面整合在一起,首先是对HelloWorld.vue中的修改:\n\u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;hello\u0026#34;\u0026gt; \u0026lt;el-container\u0026gt; \u0026lt;el-aside width=\u0026#34;100px\u0026#34;\u0026gt; \u0026lt;el-menu :default-active=\u0026#34;$route.path\u0026#34; router=true\u0026gt; \u0026lt;el-menu-item index=\u0026#34;/\u0026#34;\u0026gt;首页\u0026lt;/el-menu-item\u0026gt; \u0026lt;el-menu-item index=\u0026#34;/demo1\u0026#34;\u0026gt;测试1\u0026lt;/el-menu-item\u0026gt; \u0026lt;el-menu-item index=\u0026#34;/demo2\u0026#34;\u0026gt;测试2\u0026lt;/el-menu-item\u0026gt; \u0026lt;el-menu-item index=\u0026#34;/user/zhangsan/123\u0026#34;\u0026gt;用户\u0026lt;/el-menu-item\u0026gt; \u0026lt;/el-menu\u0026gt; \u0026lt;/el-aside\u0026gt; \u0026lt;el-main\u0026gt; \u0026lt;el-header\u0026gt; \u0026lt;h1\u0026gt;{{ msg }}\u0026lt;/h1\u0026gt; \u0026lt;/el-header\u0026gt; \u0026lt;RouterView /\u0026gt; \u0026lt;/el-main\u0026gt; \u0026lt;/el-container\u0026gt; \u0026lt;el-footer\u0026gt; 版权所有,请勿转载 \u0026lt;/el-footer\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; 通过Element-Plus添加竖直菜单栏,并使用Vue-Router的路由进行跳转。\n参考文章:\nhttps://element-plus.sxtxhy.com/zh-CN/guide/installation.html#unpkg\n","date":1753517953,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753625818,"objectID":"170dcb0a13bc594a91bf2050fcb9b0cc","permalink":"https://zhuzhulang.github.io/blog/posts/rapid-engineering-application-of-vue/","publishdate":"2025-07-26T16:19:13+08:00","relpermalink":"/posts/rapid-engineering-application-of-vue/","section":"posts","summary":"如何使用Vue3快速进行单页面应用工程化","tags":["web"],"title":"Vue3快速应用工程化","type":"posts"},{"authors":["码力全开"],"categories":["web"],"content":"在这里,我们介绍如何快速入门Vue3,通过一些例子介绍其基本功能。要使用vue,主要有如下两种方法:\n使用包安装器,如npm 使用CDN直接进行加载 这里我们使用第2种方法,下面是一个简单的计数应用。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;title\u0026gt;计数器\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/vue@3.5.18\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{ count }}\u0026lt;/h1\u0026gt; \u0026lt;button v-on:click=\u0026#34;countNum\u0026#34;\u0026gt;点击\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;script\u0026gt; const app = { data(){ return { count:0 } }, methods:{ countNum(){ this.count = this.count + 1; } } } Vue.createApp(app).mount(\u0026#34;#app\u0026#34;); \u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt; 在上述代码中,我们加载vue的版本为3.5.18。之后是对应页面元素,在vue中变量的值使用花括号进行包裹,而通过v-on:click进行点击事件的绑定。\n之后我们创建一个应用变量app,其有1个data函数用于表示初始化的数据,而methods是其相关的方法。在countNum函数中我们每次将其count属性的值加1。最后通过Vue.createApp创建应用,并挂载在ID为app的元素上。\n可以说这个例子非常简单,当点击按钮其计数就加1。但是上述代码有一个问题就是元素在左侧显示,如果想将其居中显示,此时只能通过CSS进行设置。\n\u0026lt;style\u0026gt; #app { text-align: center; border: 1px solid red; color: blue; } \u0026lt;/style\u0026gt; 此时页面是1个红色边框将计数器和按钮包围了起来。此时如果想动态修改计数器字体的颜色,此时可以这样来操作,为其绑定属性:\n\u0026lt;style\u0026gt; #h1 { color: red; } \u0026lt;/style\u0026gt; \u0026lt;h1 v-bind:id=\u0026#34;id1\u0026#34;\u0026gt;{{ count }}\u0026lt;/h1\u0026gt; \u0026lt;script\u0026gt; const app = { data(){ return { count:0, id1:\u0026#34;h1\u0026#34; } }, ... } \u0026lt;/script\u0026gt; 在vue中,对于事件的绑定,实际上提供了另外一种简化方式,即是使用@符号代替v-on:,此时可以将上述代码进行如下的修改:\n\u0026lt;button @click=\u0026#34;countNum\u0026#34;\u0026gt;点击\u0026lt;/button\u0026gt; 而对于属性的绑定可以去掉v-bind,此时的代码如下:\n\u0026lt;h1 :id=\u0026#34;id1\u0026#34;\u0026gt;{{ count }}\u0026lt;/h1\u0026gt; 接下来我们来看下vue中条件渲染,对于之前的计数器,我们想让其值大于5之后换成蓝色及字体为48像素,此时进行如下的修改:\n\u0026lt;style\u0026gt; #h1-big { font-size: 48px; color: blue; } \u0026lt;/style\u0026gt; \u0026lt;h1 :id=\u0026#34;id1\u0026#34; v-if=\u0026#34;count \u0026lt; 5\u0026#34;\u0026gt;{{ count }}\u0026lt;/h1\u0026gt; \u0026lt;h1 :id=\u0026#34;id2\u0026#34; v-else-if=\u0026#34;count \u0026gt;= 5\u0026#34;\u0026gt;{{ count }}\u0026lt;/h1\u0026gt; \u0026lt;script\u0026gt; const app = { data(){ return { count:0, id1:\u0026#34;h1\u0026#34;, id2:\u0026#34;h1-big\u0026#34; } }, ... } \u0026lt;/script\u0026gt; 通过使用v-if与v-else-if进行条件渲染。接着我们还可以修改点击时每步的步数,让其随机添加1个1-10之间的值:\n\u0026lt;button @click=\u0026#34;countNum(Math.floor(Math.random()*10+1))\u0026#34;\u0026gt;点击\u0026lt;/button\u0026gt; ... methods:{ countNum(num){ this.count = this.count + num; } } 下一步我们将每次点击的数值进行显示,此时添加1个textarea及列表项:\n\u0026lt;ul\u0026gt; \u0026lt;li v-for=\u0026#34;item in num_arr\u0026#34;\u0026gt;{{ item }}\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;textarea v-model=\u0026#34;num_arr\u0026#34;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;script\u0026gt; const app = { data(){ return { count:0, id1:\u0026#34;h1\u0026#34;, id2:\u0026#34;h1-big\u0026#34;, num_arr:[] } }, methods:{ countNum(num){ this.num_arr.push(num); this.count = this.count + num; } } } \u0026lt;/script\u0026gt; 这样每次点击时就会添加列表项,而文本框中出现相应的数值。\n当然我们也可以使用element-plus对之前的按钮进行美化,此时需要对代码进行如下的修改:\n\u0026lt;script src=\u0026#34;https://unpkg.com/element-plus@2.10.4\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;https://unpkg.com/element-plus@2.10.4/dist/index.css\u0026#34;\u0026gt; \u0026lt;el-button @click=\u0026#34;countNum(Math.floor(Math.random()*10+1))\u0026#34;\u0026gt;点击\u0026lt;/el-button\u0026gt; \u0026lt;script\u0026gt; Vue.createApp(app).use(ElementPlus).mount(\u0026#34;#app\u0026#34;); \u0026lt;/script\u0026gt; 我们引入CSS及js脚本文件,将button修改为el-button元素,之后创建应用后使用ElementPlus。\n参考文章:\nhttps://cn.vuejs.org/guide/quick-start.html#using-vue-from-cdn\n","date":1753491476,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753523335,"objectID":"9bc81b0d68af8c32e295bc13cf13b2ba","permalink":"https://zhuzhulang.github.io/blog/posts/getting-started-with-vue3/","publishdate":"2025-07-26T08:57:56+08:00","relpermalink":"/posts/getting-started-with-vue3/","section":"posts","summary":"如何快速入门Vue3,并在项目中进行应用","tags":["web"],"title":"Vue3快速入门","type":"posts"},{"authors":["码力全开"],"categories":["AI实战"],"content":"这里主要介绍一下对于预训练大模型微调时内存占用情况的估算,对于一个预训练大模型,其训练时内存的占用需要考虑如下一些部分及相应的精度:\n模型参数 梯度 优化器状态 激活值 对于精度其由数据类型决定,其中FP32为4字节,FP16/BF16为2字节,INT8为1字节,INT4为0.5字节。\n对于模型参数内存的占用,可以使用如下公式进行表示:\n$$ M_{\\text{params}}=P\\times B $$\n其中P为模型参数量,而B为单参数字节数(精度)。以LLaMA-70B为例,使用BF16进行训练,其参数内存约等于$70\\times 10^{9}\\times 2\\approx 140\\text{GB}$。\n之后是梯度的内存占用,主要用于反向传播生成过程。其可以用如下公式进行表示:\n$$ M_{\\text{grads}} = P\\times B_{\\text{grad}} $$\n梯度通常与模型参数是相同的精度,即$B_{\\text{grad}}=B$,同上所述,对于70B模型使用BF16训练时,梯度内存约等于140GB。\n接下来是优化器状态的内存占用,这是训练开销的主要部分。之前的模型参数内存和梯度内存都是基础占用。对于不同的优化器,其占用情况有所不同。这里以Adam优化器为例,其占用可以使用如下公式进行表示$$ M_{\\text{opt}}=P\\times (2\\times 4+B) $$\n其中动量与二阶矩一般采用FP32,因此其内存占用为$2\\times 4\\times P$,之后需要保存模型参数副本,通常也是FP32。还是以70B模型使用Adam优化器为例,优化器状态内存约等于$70\\times 10^{9}\\times 12B\\approx 840\\text{GB}$。\n假设使用简单的SGD,由于其不需要存储模型参数和梯度,因此其计算公式为$$ M_{\\text{SGD}}=P\\times B+P\\times B=2P\\times B $$\n其中参数内存为$P\\times B$,梯度内存为$P\\times B$。\n若使用改进版SGDM(SGD with Momentum),由于引入动量机制(一阶矩),因此其计算公式为$$ M_{\\text{SGDM}}=P\\times B+P\\times B+P\\times 4=(2B+4)P $$\n其中动量内存一般采用FP32。\n而如果采用RMSProp优化器,由于需要存储梯度平方的指数移动平均(二阶矩),其计算公式为$$ M_{\\text{RMSProp}} = P\\times B+P\\times B+P\\times 4=(2B+4)P $$\n对于优化器,可以借助其更新步骤数学公式中$w_{t}$及$g_{t}$分别确定其参数与梯度。\n下面再考虑激活值内存,这部分是动态占用。其近似范围可以使用如下公式进行估计:\n$$ M_{\\text{activations}}\\approx (0.7\\sim 1.5)\\times M_{\\text{params}} $$\n由于该部分内存依赖于batch size、序列长度、模型结构,因此没有统一的公式。\n综上所述,对于一般的情况,其总内存占用可以使用如下公式进行估计:\n$$ M_{\\text{total}}\\approx M_{\\text{params}}+M_{\\text{grads}}+M_{\\text{opt}} $$\n主要包括参数、梯度和优化器这3部分。而对于完整版本,还需要考虑激活值及安全余量。此时可以使用如下公式:\n$$ M_{\\text{total}}\\approx (M_{\\text{params}}+M_{\\text{grads}}+M_{\\text{opt}}+M_{\\text{activations}})\\times 1.2 $$\n其中1.2为系统预留内存缓冲(安全余量)。\n","date":1753144257,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753148663,"objectID":"05c83b526b1e72e20a4582b1618e9521","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-compute-pretrained-model-parameter-and-memory-usage/","publishdate":"2025-07-22T08:30:57+08:00","relpermalink":"/posts/how-to-compute-pretrained-model-parameter-and-memory-usage/","section":"posts","summary":"如何对预训练大模型训练占用的内存进行估算","tags":["AI"],"title":"预训练大模型内存占用估计","type":"posts"},{"authors":["码力全开"],"categories":["AI"],"content":"关于Transformer网络结构,最早是发布在论文《Attention Is All You Need》。已经有不少人对该论文进行了详细的解读,而这里是从一个小白的角度进行介绍。\n实话说,第一次接触Transformer的网络结构,完全是看不懂的。毕竟没有基础,直到对CV有了一定的了解后回过头来再看Transformer一下子就通了。其网络结构图如下:\n其中左侧部分为Encoder,而右侧为Decoder结构。其中Inputs代表输入,经过嵌入后与位置编码拼接在一起传入到Encoder中。\n整个Encoder由4部分结构组成:\n多头注意力结构-Multi-Head Attention 前馈结构-Feed Forward 残差连接-residual connection LN结构-Layer Normalization 我们对之前的网络结构图进行标注后有:\n其中输入中有一条路径直接不经过Multi-Head Attention直接到达Add \u0026amp; Norm,而这条路径就是残差连接,其源自残差网络(Residual Network,ResNet)。而之后的Norm是LN层。\n在Encoder中Nx表示有N个这样的网络结构。继续沿着这条路径,可以发现Decoder中也有类似的网络结构。但是在Decoder中前面还有1个Masked Multi-Head Attention与Add \u0026amp; Norm层。此时看Transformer网络结构是不是就很简单了。\n另外在Transformer中很喜欢说的注意力机制,常常会使用Q,K,V这3个向量容易让人摸不着头脑。此时只要记住这样一个事情,注意力机制的主要目的是将查询与一组键值对进行比较,并计算出查询与每个键之间的相关性得分,然后使用这些得分对值进行加权平均。\n而打分函数主要有点积和加性两种操作,一般都是采用前者。此时就可以理解其公式了:\n$$ \\text{Score}(Q,K)=QK^{T} $$\n之后再经过归一化操作\n$$ \\text{Softmax}(\\text{Score})=\\frac{e^{\\text{score}}}{\\sum e^{\\text{score}}} $$\n将其与值相乘进行加权平均就得到了注意力矩阵了$$ \\text{Attention}(Q,K,V)=\\sum(\\text{Softmax}(\\text{Score})\\cdot V) $$\n其中归一化使用的是Softmax函数。是不是很简单,网上很多文章写得让人看着云里雾里的。\n整个过程可以说非常简单,首先计算出相关性矩阵,之后对这个相关性矩阵再乘以一个值的权重矩阵从而对这些相关性有一个重要性的区分。这就是所谓的注意力机制了,对于某些部分关注很高,而某些部分则选择性遗忘。\n","date":1752589034,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752618819,"objectID":"9e4a4ea1798e3322690d683e2f10703a","permalink":"https://zhuzhulang.github.io/blog/posts/a-new-perspective-of-transformer/","publishdate":"2025-07-15T22:17:14+08:00","relpermalink":"/posts/a-new-perspective-of-transformer/","section":"posts","summary":"从一种新的视角看Transformer模型","tags":["AI"],"title":"Transformer模型新解","type":"posts"},{"authors":["码力全开"],"categories":["AI实战","CV"],"content":"这里我们介绍如何通过YOLOv8-obb模型来检测旋转的物体,常用于停车场车辆、卫星图像、航拍船舶、工业零件等场景。\n首先安装YOLO,这里选择的版本是8.1.3。接着下载对应的模型:\nwget https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8m-obb.pt 之后是对应的代码:\nimport cv2 import math import numpy as np from ultralytics import YOLO img_path = \u0026#34;https://ultralytics.com/images/boats.jpg\u0026#34; img = cv2.imread(img_path) model = YOLO(\u0026#34;/path/to/yolov8m-obb.pt\u0026#34;) results = model(img_path) for result in results: xywhr = result.obb.xywhr cls_int = result.obb.cls.int() for elem,i in zip(xywhr,cls_int): if i == 1: x,y,w,h,r=elem angle_degrees = math.degrees(r) rect = ((x,y),(w,h),angle_degrees) box = cv2.boxPoints(rect) box = np.int0(box) cv2.drawContours(img, [box], 0, (0,0,255), 2) cv2.imshow(\u0026#34;Image\u0026#34;,img) cv2.waitKey(0) 我们将弧度转换为角度,之后使用boxPoints计算旋转后的点的坐标,通过drawContours绘制其边框而不是使用rectangle函数。\n其结果如下:\n可以看到大体上可以较好的将船标记出来,有些地方还有待加强。\n参考文章:\nhttps://docs.ultralytics.com/zh/models/yolov8/#performance-metrics\nhttps://docs.ultralytics.com/zh/tasks/obb/#val\n","date":1751548774,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751550128,"objectID":"676cdd0dccb773c3864253480fe30036","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-yolo-obb-for-rotate-object/","publishdate":"2025-07-03T21:19:34+08:00","relpermalink":"/posts/how-to-use-yolo-obb-for-rotate-object/","section":"posts","summary":"如何使用YOLO模型检测旋转的物体","tags":["AI"],"title":"使用YOLO-obb检测旋转物体","type":"posts"},{"authors":["码力全开"],"categories":["AI实战"],"content":"最近有家初创企业要求面试前机试,结果发了个ply(Polygen File Format)格式的文件,里面记录着生猪的点云图。其文件内容类似如下:\nply format binary_little_endian 1.0 element vertex 626250 property float x property float y property float z property uchar red property uchar green property uchar blue end_header 要读取其数据主要有两种方式,一种是使用plyfile,另一种是使用open3d。首先来看第1种方式读取其数据:\n\u0026gt;\u0026gt;\u0026gt; from plyfile import PlyData \u0026gt;\u0026gt;\u0026gt; ply_data = PlyData.read(\u0026#34;hog.ply\u0026#34;) \u0026gt;\u0026gt;\u0026gt; ply_data[\u0026#34;vertex\u0026#34;] PlyElement(\u0026#39;vertex\u0026#39;, (PlyProperty(\u0026#39;x\u0026#39;, \u0026#39;float\u0026#39;), PlyProperty(\u0026#39;y\u0026#39;, \u0026#39;float\u0026#39;), PlyProperty(\u0026#39;z\u0026#39;, \u0026#39;float\u0026#39;), PlyProperty(\u0026#39;red\u0026#39;, \u0026#39;uchar\u0026#39;), PlyProperty(\u0026#39;green\u0026#39;, \u0026#39;uchar\u0026#39;), PlyProperty(\u0026#39;blue\u0026#39;, \u0026#39;uchar\u0026#39;)), count=626250, comments=[]) \u0026gt;\u0026gt;\u0026gt; ply_data[\u0026#34;vertex\u0026#34;].data memmap([(186.54663, -111.72368, 778.7497, 219, 230, 234), (185.23463, -115.89012, 780.1351, 209, 220, 224), (183.42976, -117.20565, 780.244 , 201, 212, 216), ..., (161. , 371. , 1050. , 82, 84, 71), (162. , 371. , 1050. , 80, 82, 69), (164. , 371. , 1050. , 76, 75, 57)], dtype=[(\u0026#39;x\u0026#39;, \u0026#39;\u0026lt;f4\u0026#39;), (\u0026#39;y\u0026#39;, \u0026#39;\u0026lt;f4\u0026#39;), (\u0026#39;z\u0026#39;, \u0026#39;\u0026lt;f4\u0026#39;), (\u0026#39;red\u0026#39;, \u0026#39;u1\u0026#39;), (\u0026#39;green\u0026#39;, \u0026#39;u1\u0026#39;), (\u0026#39;blue\u0026#39;, \u0026#39;u1\u0026#39;)]) \u0026gt;\u0026gt;\u0026gt; ply_data[\u0026#34;vertex\u0026#34;].data[\u0026#34;x\u0026#34;] memmap([186.54663, 185.23463, 183.42976, ..., 161. , 162. , 164. ], dtype=float32) 要获取其坐标值,需要进行如下的处理:\n\u0026gt;\u0026gt;\u0026gt; import numpy as np \u0026gt;\u0026gt;\u0026gt; vertices = np.vstack([ply[\u0026#39;vertex\u0026#39;][\u0026#39;x\u0026#39;], ply[\u0026#39;vertex\u0026#39;][\u0026#39;y\u0026#39;], ply[\u0026#39;vertex\u0026#39;][\u0026#39;z\u0026#39;]]).T 通过使用numpy的vstack将3个向量拼接在一起,从而组成3x626250向量,再经过转置从而得到626250x3向量。\n而第2种方式的代码如下:\n\u0026gt;\u0026gt;\u0026gt; import open3d as o3d \u0026gt;\u0026gt;\u0026gt; pcd = o3d.io.read_point_cloud(\u0026#34;hog.ply\u0026#34;) \u0026gt;\u0026gt;\u0026gt; o3d.visualization.draw_geometries([pcd]) 上述代码将实现点云的可视化。若要获取其对应的点,则可以使用如下的代码:\n\u0026gt;\u0026gt;\u0026gt; points=np.asarray(pcd.points) 其结果如下图所示:\n最后来看下如何对立方体点云的8个顶点进行可视化:\nimport copy import numpy as np import open3d as o3d def create_cube_point_cloud(center=(0,0,0), side_length=1.0, points_per_edge=10): # 生成正方体顶点坐标 half_len = side_length / 2 x = np.linspace(center[0]-half_len, center[0]+half_len, points_per_edge) y = np.linspace(center[1]-half_len, center[1]+half_len, points_per_edge) z = np.linspace(center[2]-half_len, center[2]+half_len, points_per_edge) xx, yy, zz = np.meshgrid(x, y, z) points = np.vstack([xx.ravel(), yy.ravel(), zz.ravel()]).T # 创建点云对象 pcd = o3d.geometry.PointCloud() pcd.points = o3d.utility.Vector3dVector(points) return pcd def extract_cube_vertices(points): \u0026#34;\u0026#34;\u0026#34;提取立方体点云的8个顶点\u0026#34;\u0026#34;\u0026#34; min_coords = np.min(points, axis=0) max_coords = np.max(points, axis=0) # 组合所有极值点坐标 vertices = np.array([ [min_coords[0], min_coords[1], min_coords[2]], [min_coords[0], min_coords[1], max_coords[2]], [min_coords[0], max_coords[1], min_coords[2]], [min_coords[0], max_coords[1], max_coords[2]], [max_coords[0], min_coords[1], min_coords[2]], [max_coords[0], min_coords[1], max_coords[2]], [max_coords[0], max_coords[1], min_coords[2]], [max_coords[0], max_coords[1], max_coords[2]] ]) # 将Numpy转换为点云数据 vertex_cloud = o3d.geometry.PointCloud() vertex_cloud.points = o3d.utility.Vector3dVector(vertices) return vertex_cloud def visualize_keypoints(pcd, keypoints): \u0026#34;\u0026#34;\u0026#34;可视化点云和关键点\u0026#34;\u0026#34;\u0026#34; # 创建点云的副本并为关键点设置颜色 pcd_copy = copy.deepcopy(pcd) pcd_copy.paint_uniform_color([0.5, 0.5, 0.5]) # 原始点云设为灰色 # 创建关键点的点云并设置为红色 keypoints_pcd = o3d.geometry.PointCloud() keypoints_pcd.points = keypoints.points keypoints_pcd.paint_uniform_color([1.0, 0.0, 0.0]) # 关键点设为红色 # 可视化 o3d.visualization.draw_geometries([pcd_copy, keypoints_pcd]) def main(): # 生成立方体点云 pcd = create_cube_point_cloud() keypoints = extract_cube_vertices(pcd.points) # 可视化结果 visualize_keypoints(pcd, keypoints) 这里由于给定的是规则立方体点云,我们通过几何极值的方式对顶点进行提取,通过计算点云在X,Y,Z三个轴上最大值和最小值的组合。其实整个过程还是比较好理解的,我们通过np.min和np.max得到这1000个点中最小点和最大点的坐标。\n其结果如下图所示:\n","date":1751457218,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751511929,"objectID":"18378ddc9d46e4884e3f9c3aa8872be4","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-visualization-point-cloud/","publishdate":"2025-07-02T19:53:38+08:00","relpermalink":"/posts/how-to-visualization-point-cloud/","section":"posts","summary":"如何对点云图进行可视化","tags":["AI"],"title":"点云图可视化","type":"posts"},{"authors":["码力全开"],"categories":["CV","AI实战"],"content":"下面我们将通过DeepSort与YOLO的结合解决一般的目标追踪问题,虽然其结果可能会出现丢失或ID不唯一的问题,但是还是可以使用的。\n首先我们下载相应的源码:\ngit clone --recurse-submodules github.com/ok-lgtm/Yolov5_DeepSort_Pytorch 之后下载YOLOv5相应的模型及DeepSort的模型resnet50_msmt17,密码是2162。其他模型可以访问官方提供的MODEL_ZOO进行下载。\n一切准备就绪,就可以使用如下的命令进行运行:\n$ python track.py --source highway-between-trees-506-hd-ready.mp4 --yolo_model yolov5/yolov5m.pt --deep_sort_model resnet50_msmt17.pth --img 1280 --save-vid --show-vid 实际上其参数是原有YOLOv5基础上进行增加的,如果之前看过我YOLOv5文章的朋友应该很熟悉。\n这里我们对高速上的车辆进行检测与追踪。整个过程可以说非常简单。\n","date":1751360203,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751361527,"objectID":"8cad9fa7883aa7181ddc9acf7b307114","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-deepsort-track-object/","publishdate":"2025-07-01T16:56:43+08:00","relpermalink":"/posts/how-to-use-deepsort-track-object/","section":"posts","summary":"通过使用YOLOv5与DeepSort算法解决一般的目标追踪问题","tags":["AI"],"title":"YOLO + DeepSort 解决目标追踪问题","type":"posts"},{"authors":["码力全开"],"categories":["AI实战","CV"],"content":"之前对YOLO的一些基础知识以及数据集的内容进行了简单介绍。下面通过一个实际的项目来加深对YOLO的应用。\n在这里要实现一个检测人物是否带头盔的需求,特别是一些安全生产的项目中。对应的数据集选择是开源的harijawahar/Helmet_Detection。\n该数据集采用COCO格式进行标注,因此其中的坐标值分别为左上角x,y值及边界框的宽高。可以使用如下的代码将其数据集转换为YOLO的格式:\nimport json import shutil from pathlib import Path image_dict = {} image_arr = [] def mkdir_dir(path): p = Path(path) if not p.exists(): p.mkdir() output_directory = \u0026#34;helmet\u0026#34; mkdir_dir(output_directory) mkdir_dir(\u0026#34;helmet/train\u0026#34;) mkdir_dir(\u0026#34;helmet/train/images\u0026#34;) mkdir_dir(\u0026#34;helmet/train/labels\u0026#34;) mkdir_dir(\u0026#34;helmet/val\u0026#34;) mkdir_dir(\u0026#34;helmet/val/images\u0026#34;) mkdir_dir(\u0026#34;helmet/val/labels\u0026#34;) with open(\u0026#34;train/_annotations.coco.json\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) as f: data = f.read() json_data = json.loads(data) images = json_data.get(\u0026#34;images\u0026#34;, []) annotations = json_data.get(\u0026#34;annotations\u0026#34;, []) for d in images: _id = d.get(\u0026#34;id\u0026#34;) filename = d.get(\u0026#34;file_name\u0026#34;) height = d.get(\u0026#34;height\u0026#34;) width = d.get(\u0026#34;width\u0026#34;) image_arr.append((filename,width,height)) shutil.copy2(\u0026#34;train/{}\u0026#34;.format(filename),\u0026#34;helmet/train/images/{}.jpg\u0026#34;.format(_id)) for d in annotations: image_id = d.get(\u0026#34;image_id\u0026#34;) category_id = d.get(\u0026#34;category_id\u0026#34;) bbox = d.get(\u0026#34;bbox\u0026#34;) x1,y1,width2,height2 = bbox x2 = x1 + width2 y2 = y1 + height2 x1 = int(x1) x2 = int(x2) y1 = int(y1) y2 = int(y2) with open(\u0026#34;helmet/train/labels/{}.txt\u0026#34;.format(image_id),\u0026#34;a+\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(\u0026#34;{} {} {} {} {}\\n\u0026#34;.format(category_id,(x1+width2//2)/width,(y1+height2//2)/height,width2/width,height2/height)) 我们将对应的数据集下载解压到YOLOv5项目目录下,首先创建相应的训练及验证目录,每个目录下有2个子目录images和labels分别存储图片及对应的标签文件。\n根据数据集每个目录下的_annotations.coco.json的内容得到文件名称、分类类型、边界框的值。其中类型主要有3类,分别是不确定、带头盔、没带头盔。将左上角坐标加上边界框一般的宽高后归一化得到YOLO的格式。\n之后在YOLOv5源代码data目录下新增1个helmet.yaml的配置文件,其内容如下:\ntrain: ./helmet/helmet/train/ val: ./helmet/helmet/val/ nc: 3 names: [\u0026#34;undefined\u0026#34;,\u0026#34;helmet\u0026#34;,\u0026#34;no helmet\u0026#34;] 下面就可以开始进行模型训练后,其对应的目录如下:\npython train.py --data data/helmet.yaml --cfg models/yolov5m.yaml --weights weights/yolov5m.pt --epochs 100 --batch-size 8 这里模型使用的是YOLOv5m,训练轮数为100次,每个批次大小为8。经过几个小时的训练后得到最终的模型文件。\n之后就可以使用其检测代码了:\n$ python detect.py --weights /path/to/best.pt --source helmet/test 我们对测试集目录下的所有图片进行检测。其最终的效果如下:\n可以看到实际该数据集对图片进行强制拉伸为640x640的大小,从而导致图片会出现一些变形。更多情况还是推荐使用图像强化的方式会更好些。关于数据增强可以考虑使用albumentations。\n","date":1751205811,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751285290,"objectID":"e4c7cdf708d2d2fda9ecd2f6a9f34442","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-train-custom-dataset-in-yolov5/","publishdate":"2025-06-29T22:03:31+08:00","relpermalink":"/posts/how-to-train-custom-dataset-in-yolov5/","section":"posts","summary":"如何通过YOLOv5对自定义数据集进行训练,完成项目中一些实战的工作","tags":["AI"],"title":"YOLOv5训练自定义数据集","type":"posts"},{"authors":["码力全开"],"categories":["CV"],"content":"下面的内容实际上非常简单和基础,不涉及太高深的知识点。\n之前介绍了YOLOv5训练的代码,下面介绍下YOLO数据集的格式,方便在自己训练自定义数据集时游刃有余。YOLO数据集由图像文件夹及对应标签文件夹组成,其中标签文件每行描述1个目标,格式为:\n[类别索引] [x_center] [y_center] [width] [height] 其中后面4个值是归一化数值,取值在0-1之间。而x_center是边界框中心点x坐标,y_center是边界框中心点y坐标。而width是边界框宽度,height是边界框高度。\n要使用OpenCV将其绘制出来,可以使用rectangle函数,由于该函数需要传入左上角坐标及右下角坐标。因此,其计算代码类似如下:\nimage_filename = \u0026#34;images/train2017/000000000520.jpg\u0026#34; img = cv2.imread(image_filename) height,width = img.shape[:2] for label in labels: x,y,w,h = label[1:] width1 = int(width * x) width2 = int(width * w) height1 = int(height * y) height2 = int(height * h) x1 = width1 - width2 // 2 y1 = height1 - height2 // 2 x2 = width1 + width2 // 2 y2 = height1 + height2 // 2 cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),1) 我们需要分别计算出其边界框中心点坐标及边界框的大小。我们用边界框中心点坐标减去边界框一半的大小得到其左上角坐标,而加上边界框一般的大小得到右下角坐标。\n这里使用COCO128数据集中000000000520.jpg图片进行介绍,其结果如下:\n在YOLOv5的实现代码中,其中utils/general.py模块中的xywh2xyxy及xyxy2xywh函数分别用于将YOLO格式转换为标注的边界框和标注数据集转换为YOLO格式。\n","date":1751098912,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751100256,"objectID":"f34d366dd60a64edda5d3677832948dc","permalink":"https://zhuzhulang.github.io/blog/posts/yolo-dataset-format/","publishdate":"2025-06-28T16:21:52+08:00","relpermalink":"/posts/yolo-dataset-format/","section":"posts","summary":"对YOLO数据集格式进行简单的介绍,说明其计算过程","tags":["AI"],"title":"YOLO数据集格式简述","type":"posts"},{"authors":["码力全开"],"categories":["CV"],"content":"前言 之前介绍了YOLOv3推理过程整个代码的过程,详细内容可以参考。而YOLOv5的推理代码由于是同一个团队的实现,实际上并没有太大的变化,只是将其中的cfg配置文件修改为yaml格式,而权重文件是PyTorch格式的。\n首先下载其源码:\ngit clone -b v4.0 --depth 1 http://github.com/ultralytics/yolov5.git 这里采用的是其版本4.0的代码进行讲解。其代码目录结构如下:\n| detect.py | Dockerfile | hubconf.py | LICENSE | README.md | requirements.txt | test.py | train.py | tutorial.ipynb | +---.github | | dependabot.yml | | | +---ISSUE_TEMPLATE | | bug-report.md | | feature-request.md | | question.md | | | \\---workflows | ci-testing.yml | codeql-analysis.yml | greetings.yml | rebase.yml | stale.yml | +---data | | coco.yaml | | coco128.yaml | | hyp.finetune.yaml | | hyp.scratch.yaml | | voc.yaml | | | +---images | | bus.jpg | | zidane.jpg | | | \\---scripts | get_coco.sh | get_voc.sh | +---models | | common.py | | experimental.py | | export.py | | yolo.py | | yolov5l.yaml | | yolov5m.yaml | | yolov5s.yaml | | yolov5x.yaml | | __init__.py | | | \\---hub | anchors.yaml | yolov3-spp.yaml | yolov3-tiny.yaml | yolov3.yaml | yolov5-fpn.yaml | yolov5-p2.yaml | yolov5-p6.yaml | yolov5-p7.yaml | yolov5-panet.yaml | +---utils | | activations.py | | autoanchor.py | | datasets.py | | general.py | | google_utils.py | | loss.py | | metrics.py | | plots.py | | torch_utils.py | | __init__.py | | | \\---google_app_engine | additional_requirements.txt | app.yaml | Dockerfile | \\---weights download_weights.sh 其中YOLOv5中添加了torch.hub的加载方式,提供了shell脚本用于下载COCO及PASCAL VOC数据集。另外还提供了教程文件tutorial.ipynb用于让开发人员了解其调用方式。\n目标检测训练代码 其训练代码在train.py模块中,首先是相应命令行参数:\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--weights\u0026#39;, type=str, default=\u0026#39;yolov5s.pt\u0026#39;, help=\u0026#39;initial weights path\u0026#39;) parser.add_argument(\u0026#39;--cfg\u0026#39;, type=str, default=\u0026#39;\u0026#39;, help=\u0026#39;model.yaml path\u0026#39;) parser.add_argument(\u0026#39;--data\u0026#39;, type=str, default=\u0026#39;data/coco128.yaml\u0026#39;, help=\u0026#39;data.yaml path\u0026#39;) parser.add_argument(\u0026#39;--hyp\u0026#39;, type=str, default=\u0026#39;data/hyp.scratch.yaml\u0026#39;, help=\u0026#39;hyperparameters path\u0026#39;) parser.add_argument(\u0026#39;--epochs\u0026#39;, type=int, default=300) parser.add_argument(\u0026#39;--batch-size\u0026#39;, type=int, default=16, help=\u0026#39;total batch size for all GPUs\u0026#39;) parser.add_argument(\u0026#39;--img-size\u0026#39;, nargs=\u0026#39;+\u0026#39;, type=int, default=[640, 640], help=\u0026#39;[train, test] image sizes\u0026#39;) parser.add_argument(\u0026#39;--rect\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;rectangular training\u0026#39;) parser.add_argument(\u0026#39;--resume\u0026#39;, nargs=\u0026#39;?\u0026#39;, const=True, default=False, help=\u0026#39;resume most recent training\u0026#39;) parser.add_argument(\u0026#39;--nosave\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;only save final checkpoint\u0026#39;) parser.add_argument(\u0026#39;--notest\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;only test final epoch\u0026#39;) parser.add_argument(\u0026#39;--noautoanchor\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;disable autoanchor check\u0026#39;) parser.add_argument(\u0026#39;--evolve\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;evolve hyperparameters\u0026#39;) parser.add_argument(\u0026#39;--bucket\u0026#39;, type=str, default=\u0026#39;\u0026#39;, help=\u0026#39;gsutil bucket\u0026#39;) parser.add_argument(\u0026#39;--cache-images\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;cache images for faster training\u0026#39;) parser.add_argument(\u0026#39;--image-weights\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;use weighted image selection for training\u0026#39;) parser.add_argument(\u0026#39;--device\u0026#39;, default=\u0026#39;\u0026#39;, help=\u0026#39;cuda device, i.e. 0 or 0,1,2,3 or cpu\u0026#39;) parser.add_argument(\u0026#39;--multi-scale\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;vary img-size +/- 50%%\u0026#39;) parser.add_argument(\u0026#39;--single-cls\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;train multi-class data as single-class\u0026#39;) parser.add_argument(\u0026#39;--adam\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;use torch.optim.Adam() optimizer\u0026#39;) parser.add_argument(\u0026#39;--sync-bn\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;use SyncBatchNorm, only available in DDP mode\u0026#39;) parser.add_argument(\u0026#39;--local_rank\u0026#39;, type=int, default=-1, help=\u0026#39;DDP parameter, do not modify\u0026#39;) parser.add_argument(\u0026#39;--log-imgs\u0026#39;, type=int, default=16, help=\u0026#39;number of images for W\u0026amp;B logging, max 100\u0026#39;) parser.add_argument(\u0026#39;--log-artifacts\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;log artifacts, i.e. final trained model\u0026#39;) parser.add_argument(\u0026#39;--workers\u0026#39;, type=int, default=8, help=\u0026#39;maximum number of dataloader workers\u0026#39;) parser.add_argument(\u0026#39;--project\u0026#39;, default=\u0026#39;runs/train\u0026#39;, help=\u0026#39;save to project/name\u0026#39;) parser.add_argument(\u0026#39;--name\u0026#39;, default=\u0026#39;exp\u0026#39;, help=\u0026#39;save to project/name\u0026#39;) parser.add_argument(\u0026#39;--exist-ok\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;existing project/name ok, do not increment\u0026#39;) parser.add_argument(\u0026#39;--quad\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;quad dataloader\u0026#39;) opt = parser.parse_args() 其中参数说明如下:\nweights,模型权重文件路径 cfg,模型配置文件路径 data,训练的数据集,默认为data/coco128.yaml。该数据集只有6.6M,可以用于训练测试。而完整的数据集需要调用data/scripts目录下的shell脚本,其中COCO数据集大小为27GB,而VOC数据集为2.8GB hyp,超参数路径 epochs,训练的轮数,默认为300 batch-size,总的batch数量 其中数据集内目录结构如下:\nimages,图片数据 labels,标注数据 每张图片对应一个标注数据文本文件,其内容类似如下:\n45 0.479492 0.688771 0.955609 0.5955 45 0.736516 0.247188 0.498875 0.476417 50 0.637063 0.732938 0.494125 0.510583 45 0.339438 0.418896 0.678875 0.7815 其中45和50是相应的类型的ID,之后4位是x,y,w,h归一化后的值。而yaml文件中需要指定如下一些内容:\ntrain: ../coco128/images/train2017/ # 128 images val: ../coco128/images/train2017/ # 128 images # number of classes nc: 80 # class names names: [ \u0026#39;person\u0026#39;, ..., \u0026#39;hair drier\u0026#39;, \u0026#39;toothbrush\u0026#39; ] 分别是训练和验证集的目录,分类的数量及每个类的名称。\n之后是使用并行GPU的参数设置:\n# Set DDP variables opt.world_size = …","date":1751012988,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751026823,"objectID":"a774ae74f24aeb4c1e20c2beb52ec4d1","permalink":"https://zhuzhulang.github.io/blog/posts/in-depth-code-explaination-of-yolov5/","publishdate":"2025-06-27T16:29:48+08:00","relpermalink":"/posts/in-depth-code-explaination-of-yolov5/","section":"posts","summary":"手把手从YOLOv3代码入手,为你详细讲解其训练整个实现过程","tags":["AI"],"title":"YOLOv5目标检测代码精解","type":"posts"},{"authors":["码力全开"],"categories":["CV"],"content":"前言 通过对YOLOv3代码整个流程的讲解,从而更好的把握对YOLO的熟练度。这里使用的代码基于ultralytics/yolov3版本3.0的代码。\ngit clone -b v3.0 --depth 1 http://github.com/ultralytics/yolov3.git 整个项目目录如下图所示:\n| .gitignore | detect.py | LICENSE | models.py | README.md | requirements.txt | test.py | train.py | +---cfg | coco.data | yolov3-spp.cfg | yolov3-tiny.cfg | yolov3.cfg | +---data | | coco.names | | coco_paper.names | | get_coco_dataset.sh | | | \\---samples | zidane.jpg | +---utils | datasets.py | gcp.sh | parse_config.py | torch_utils.py | utils.py | \\---weights download_yolov3_weights.sh 其中cfg目录下存放是YOLOv3模型配置文件,可以在运行时指定对应配置构建相应的模型。而data目录是相应模型需要用到的数据,utils下是相应工具函数,weights下就是模型的权重文件。\n目标检测代码 我们从detect.py模块入手查看其整个调用的流程,官方给出的调用方式如下:\ndetect.py --cfg cfg/yolov3.cfg --weights weights/yolov3.pt 相应的代码如下:\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--cfg\u0026#39;, type=str, default=\u0026#39;cfg/yolov3.cfg\u0026#39;, help=\u0026#39;cfg file path\u0026#39;) parser.add_argument(\u0026#39;--weights\u0026#39;, type=str, default=\u0026#39;weights/yolov3.weights\u0026#39;, help=\u0026#39;path to weights file\u0026#39;) parser.add_argument(\u0026#39;--images\u0026#39;, type=str, default=\u0026#39;data/samples\u0026#39;, help=\u0026#39;path to images\u0026#39;) parser.add_argument(\u0026#39;--img-size\u0026#39;, type=int, default=32 * 13, help=\u0026#39;size of each image dimension\u0026#39;) parser.add_argument(\u0026#39;--conf-thres\u0026#39;, type=float, default=0.50, help=\u0026#39;object confidence threshold\u0026#39;) parser.add_argument(\u0026#39;--nms-thres\u0026#39;, type=float, default=0.45, help=\u0026#39;iou threshold for non-maximum suppression\u0026#39;) opt = parser.parse_args() print(opt) with torch.no_grad(): detect( opt.cfg, opt.weights, opt.images, img_size=opt.img_size, conf_thres=opt.conf_thres, nms_thres=opt.nms_thres ) 从命令行中接收相应的参数:\ncfg,配置文件config的路径 weights,模型权重文件的路径 images,推理的图片路径 img-size,每张图片维度,默认为32x13,即416像素 conf-thres,置信度阈值,默认为0.5 nms-thres,NMS阈值,默认为0.45 之后其调用detect函数:\ndevice = torch_utils.select_device() if os.path.exists(output): shutil.rmtree(output) # delete output folder os.makedirs(output) # make new output folder 首先选择运行的设备,其代码位于utils/torch_utils.py模块的select_device函数,返回值是cpu或cuda。接着判断输出目录是否存在,若存在则删除进行重建。\n# Initialize model model = Darknet(cfg, img_size) # Load weights if weights.endswith(\u0026#39;.pt\u0026#39;): # pytorch format if weights.endswith(\u0026#39;yolov3.pt\u0026#39;) and not os.path.exists(weights): if (platform == \u0026#39;darwin\u0026#39;) or (platform == \u0026#39;linux\u0026#39;): os.system(\u0026#39;wget https://storage.googleapis.com/ultralytics/yolov3.pt -O \u0026#39; + weights) model.load_state_dict(torch.load(weights, map_location=\u0026#39;cpu\u0026#39;)[\u0026#39;model\u0026#39;]) else: # darknet format _ = load_darknet_weights(model, weights) 之后开始初始化模型,并加载权重。其中模型类位于models.py下的Darknet类。\nclass Darknet(nn.Module): \u0026#34;\u0026#34;\u0026#34;YOLOv3 object detection model\u0026#34;\u0026#34;\u0026#34; def __init__(self, cfg_path, img_size=416): super(Darknet, self).__init__() self.module_defs = parse_model_cfg(cfg_path) self.module_defs[0][\u0026#39;cfg\u0026#39;] = cfg_path self.module_defs[0][\u0026#39;height\u0026#39;] = img_size self.hyperparams, self.module_list = create_modules(self.module_defs) self.img_size = img_size self.loss_names = [\u0026#39;loss\u0026#39;, \u0026#39;xy\u0026#39;, \u0026#39;wh\u0026#39;, \u0026#39;conf\u0026#39;, \u0026#39;cls\u0026#39;, \u0026#39;nT\u0026#39;] self.losses = [] 此时将解析传入cfg参数中的配置文件的内容,通过调用utils/parse_config.py模块中的parse_model_cfg函数。在该类中调用同模块下create_modules函数根据配置构建相应的网络结构。\n初始化网络模型结构后,下一步就是要加载模型的权重了。此时判断文件的名称是否为.pt后缀,若该文件不存在且为yolov3.pt则尝试下载对应的网络权重文件模型。由于YOLOv3默认的权重文件后缀为.weights,则原作者网站已经无法下载该文件了。如果手头上有相应的文件则直接调用load_darknet_weights函数加载Darknet格式的权重。\n紧接着将模型设置为评估模式并开始加载图片。\nmodel.to(device).eval() # Set Dataloader if webcam: save_images = False dataloader = LoadWebcam(img_size=img_size) else: dataloader = LoadImages(images, img_size=img_size) 对于网络摄像头,其调用utils/datasets.py模块下LoadWebcam函数,否则调用LoadImages函数。可以在调用detect函数时指定其webcam参数为True来使用网络摄像头。\n下一步就是加载数据目录data下的类别和颜色:\n# Get classes and colors classes = load_classes(parse_data_cfg(\u0026#39;cfg/coco.data\u0026#39;)[\u0026#39;names\u0026#39;]) colors = [[random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)] for _ in range(len(classes))] 其中YOLOv3中coco数据集共有80个类别,而颜色则为随机分配。下面就要开始遍历图片数据对其进行预测了:\nfor i, (path, img, im0) in enumerate(dataloader): t = time.time() if webcam: print(\u0026#39;webcam frame %g: \u0026#39; % (i + 1), end=\u0026#39;\u0026#39;) else: print(\u0026#39;image %g/%g %s: \u0026#39; % (i + 1, len(dataloader), path), end=\u0026#39;\u0026#39;) save_path = str(Path(output) / Path(path).name) # Get detections img = torch.from_numpy(img).unsqueeze(0).to(device) if ONNX_EXPORT: torch.onnx.export(model, img, \u0026#39;weights/model.onnx\u0026#39;, verbose=True) return 这里获取图片路径后将其与输出目录拼接在一起,并通过torch加载图片数据,让其为4维张量。如果设置了ONNX导出选项,则直接不进行后续操作,直接将模型导出了。\npred = model(img) pred = pred[pred[:, :, 4] \u0026gt; conf_thres] # remove boxes \u0026lt; threshold 将图片张量传入模型中,并筛选大于置信度阈值的数据。\nif len(pred) \u0026gt; 0: # Run NMS on predictions detections = non_max_suppression(pred.unsqueeze(0), conf_thres, nms_thres)[0] # Rescale boxes from 416 to true image size scale_coords(img_size, detections[:, :4], im0.shape).round() # Print results to screen unique_classes = detections[:, -1].cpu().unique() for c in unique_classes: n = (detections[:, -1].cpu() == c).sum() print(\u0026#39;%g %ss\u0026#39; % (n, classes[int(c)]), end=\u0026#39;, \u0026#39;) # Draw bounding boxes and labels of detections for x1, y1, x2, y2, conf, cls_conf, cls in detections: if save_txt: # Write to file with open(save_path + \u0026#39;.txt\u0026#39;, \u0026#39;a\u0026#39;) as file: file.write(\u0026#39;%g %g %g %g %g %g\\n\u0026#39; % (x1, y1, x2, y2, …","date":1750991171,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751013032,"objectID":"68ebc189fc3aaa65ad93656bd5863c97","permalink":"https://zhuzhulang.github.io/blog/posts/in-depth-code-explaination-of-yolov3/","publishdate":"2025-06-27T10:26:11+08:00","relpermalink":"/posts/in-depth-code-explaination-of-yolov3/","section":"posts","summary":"手把手从YOLOv3代码入手,为你详细讲解其推理整个实现过程","tags":["AI"],"title":"YOLOv3目标检测代码精解","type":"posts"},{"authors":["码力全开"],"categories":["CV"],"content":"前言 以下是YOLO系列各个版本的发布时间及对应论文概述。\n网络架构 发布时间 论文标题 YOLOv1 2015.6 You Only Look Once: Unified, Real-Time Object Detection YOLOv2 2016.12 YOLO9000: Better, Faster, Stronger YOLOv3 2018.4 YOLOv3: An Incremental Improvement YOLOv4 2020.4 YOLOv4: Optimal Speed and Accuracy of Object Detection 我们可以将YOLO系统按照时间顺序划分为3个阶段:\n开创纪元（2015-2016） YOLO v1（2015）：提出\u0026#34;回归式检测\u0026#34;新范式，将检测任务转化为单次网格预测，实现155 FPS的实时性能 YOLO v2（2016）：引入锚框聚类、多尺度训练等创新，mAP提升10个百分点，支持9000类物体检测 技术爆发（2017-2020） YOLO v3（2018）：采用Darknet-53骨干网络，引入FPN结构，在保持速度优势下精度媲美两阶段算法 YOLO v4（2020）：集成CSPNet、SAM模块等前沿技术，首次在COCO数据集实现43.5% AP@50:95 工业革命（2021至今） YOLO v5：开源社区驱动的工程化改进，支持TensorRT加速 YOLOv6/v7：美团/旷视等企业的工业级优化，推理速度突破200FPS YOLOv8（2023）：加入可编程梯度信息（PGI）技术，实现精度-速度-易用性三重突破 这里只介绍前3个版本的内容。\nYOLOv1 YOLO的网络结构的灵感来自图片分类的GoogLeNet。网络有24个卷积层,紧接着是2个全连接层。其网络架构图如下:\n模型输入的图像尺寸为448x448x3,而最终输出为7x7x30的张量。通过公式SxSx(Bx5+C)计算得到,其中S=7(网格)，B=2(边框数)，C=20(类别数,采用PASCAL VOC 2007与2012数据集)。\n其思想是通过将输入图片划分为固定大小SxS的格子(grid cells),每个网格单元负责对落在该单元内的物体进行预测,并一次性预测所有格子所含目标的边界框、定位置信度以及所有类别概率向量。简而言之,当一个物体的边界框中心点落在某个网格单元内,该单元负责预测该物体。每个网格单元预测B个边界框,每个框包含坐标和置信度,但最终采用与真实框(Ground Truth)IOU最高的预测框作为有效输出。\n而置信度的定义为\n$$ \\text{Pr}(\\text{Object})\\ast\\text{IOU}_{\\text{pred}}^{\\text{truth}} $$\n如果网格单元内没有物体则置信度分数为0。每个边界框由5个预测值组成,分别为x,y,w,h和置信度。每个网格单元也预测C个条件类概率$\\text{Pr}(\\text{Class}_{i}\\mid\\text{Object})$。该概率表示网格单元包含物体的条件概率。\nYOLOv2 在YOLOv2中完全移除了之前的全连接层,并引入BN层(Batch Normalization)加速收敛,另外使用锚框(Anchor)来预测边界框。与v1不同的是,v2版本输入的图片尺寸为416x416x3,主要是为了能在特征图中得到奇数的位置。对于输出13x13的特征图,其32倍上采样后正好是416。\n另外其使用k均值聚类自动来划分先验框,其中k=5时效果较好。\n而在网络结构上,使用Darknet-19替换了之前的GoogLeNet网络,该网络具有19个卷积层和5个池化层。其网络结构如下图所示:\n并引入了Passthrough层,通过将浅层26x26特征图与深层13x13特征图进行拼接,保留细节信息从而提升小目标检测能力。而在训练时采用多尺度方式,每10个batch随机切换输入尺寸(从320x320至608x608),增强模型对不同尺度目标的适应性。\nYOLOv3 YOLOv3最大的改进是网络结构,使其更适合小目标检测。另外特征做的更细致,通过融入多持续特征图信息来预测不同规格物体。另外,先验框更丰富了,一共有3种比例,每种3个规格,故有9种。对softmax进行改进使其可以预测多标签任务。 其网络结构如下图所示,通过跳跃连接解决深层网络梯度消失问题:\n其引入FPN模块,通过三尺度进行特征融合。新增52x52高分辨率特征图用于小目标检测,结合26x26(中目标)、13x13(大目标)形成金字塔结构。深层特征上采样后与浅层特征拼接,融合语义与细节信息。\n损失函数(Loss function) 损失函数包括:\nclassification loss,分类损失 localization loss,定位损失(预测边界框与Ground Truth之间的误差) confidence loss,置信度损失(框的目标性) 总的损失函数为classification loss+localization loss+confidence loss。\n","date":1750813630,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750991239,"objectID":"5143eb3575e13ff582d2d062c5283753","permalink":"https://zhuzhulang.github.io/blog/posts/unfolding-the-evolution-of-yolo-series-in-object-detection/","publishdate":"2025-06-25T09:07:10+08:00","relpermalink":"/posts/unfolding-the-evolution-of-yolo-series-in-object-detection/","section":"posts","summary":"对YOLO整个系列进行简单的介绍","tags":["AI"],"title":"从\"一眼万年\"到\"万物可识\"：YOLO系列目标检测技术的进化之路","type":"posts"},{"authors":["码力全开"],"categories":["CV"],"content":"简述 在正式介绍UNet与DeepLab网络架构之前,先对其整个过程进行简单的梳理。\n网络架构 发布时间 初稿标题 U-Net 2015.5 U-Net: Convolutional Networks for Biomedical Image Segmentation UNet++ 2019.12 UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation UNet+++ 2020.4 UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation DeepLab v1 2014.12 Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs DeepLab v2 2016.6 DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs DeepLab v3 2017.6 Rethinking Atrous Convolution for Semantic Image Segmentation DeepLab v3+ 2018.2 Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 需要注意的是,U-Net、UNet++、UNet+++分别由3个不同团队提出的,是基于前者网络结构上的改进。相比而言,DeepLab系列是Google核心团队主导的统一迭代演进模型。\n另外UNet网络系列是纯正的Encoder-Decoder结构,而DeepLab v1-v3仅有Encoder,直到DeepLab v3+时才引入Decoder。\nUNet网络框架 U-Net U-Net最初通过卷积网络解决医学中图像分割问题,由于该网络结构非常简单而得到广泛的应用。其通过基础卷积层+对称编解码+跳跃连接的设计,解决了医学图像分割中三个核心问题:\n小样本训练: 依靠数据增强弥补训练数据不足的问题 器官/病变边界模糊: 利用跳跃连接skip-connection融合多尺度特征,保留细节信息 计算资源受限: 无冗余设计,整个网络没有使用全连接层、批量归一化(BN)或Dropout等复杂技术,从而减少了参数量。 其网络架构如下图所示:\n从上图可以看出,使用的仅是3x3卷积与ReLU的基础块,而下采样通过2x2最大池化来实现,上采样通过2x2转置卷积或插值法进行实现。最后输出层使用1x1卷积将通道数映射为类别数。\nUNet++ U-Net虽然网络结构非常简单,但是存在如下一些问题:\n直接使用skip-connection将编码器的浅层特征与解码器深层特征进行拼接,而两者语义信息存在显著差异,从而导致直接融合效果受限 网络需要预先设定固定深度,但不同任务(如不同尺寸的病变分割)所需的最优深度不同,从而限制了模型的适应性 而UNet++通过如下改进设计解决上述问题:\n嵌套密集跳跃连接:通过在编码器与解码器之间引入多级子网络,形成稠密连接路径。而每一层解码器接收所有同尺度及更浅层的编码器特征,从而实现渐进式特征融合,减少语义差异并增强尺度信息传递 自适应网络剪枝:可在推理阶段移除部分子网络分支,从而平衡精度与速度的要求 其网络结构如下图所示:\n在上图中,每个节点代表一个卷积块,向下的箭头代表下采样,而向上的箭头代表上采样,点状箭头代表跳跃连接(skip connections)。 其借鉴了DenseNet密集连接的思想,在Encoder和Decoder之间跳跃路径上,跳跃路径上的每一个节点都接收前面所有更浅层节点的特征图,并将其拼接起来作为输入。\n而在推理时可以根据业务需求进行剪枝:\nUNet+++ 相比UNet++仅嵌套融合同层级特征,导致跨尺度信息交互不足的问题。在UNet 3+提出了全尺度跳跃连接(Full-Scale Skip Connections)的概念,通过Decoder每层直接融合Encoder所有层级(包括浅、中、深层)的特征图,通过跳跃连接实现跨分辨率特征融合,从而增强小目标检测能力。\n其网络结构如下所示:\nDeepLab网络框架 相比U-Net系列,DeepLab系列引入空洞卷积,在不牺牲分辨率情况下扩大了感受野。本质上DeepLab是FCN(Fully Convolutional Network)架构改进的变体。\nDeepLab v1 传统CNN网络中存在这样的问题,最后的全连接层会限制输入的尺寸并。而将全连接层FC移除后,将其全部替换为卷积层,从而使得网络可接受任意尺寸输入并可输出同尺寸热力图。\n下面是其网络结构:\n可以看到输入图片经过深度卷积神经网络后得到对应分数图,再经过双线性插值进行上采样,最后通过全连接CRF层改进其最终输出。\nDeepLab v2 由于DeepLab v1存在如下多尺度分割问题:\n物体尺寸差异大,由于单一空洞卷积感受野固定,难以覆盖不同尺度目标 深层特征偏向大物体,丢失小目标。 因此在DeepLab v2中引入ASPP(Atrous Spatial Pyramid Pooling)模块,通过并行使用不同空洞率(如rates=6,12,18,24)的卷积层捕捉多尺度上下文信息。如下图所示:\n另外DeepLab v2中将主干网络由VGG16升级为ResNet101,提升高层语义表达能力并支持更高效训练。\nDeepLab v3 由于DeepLab v2继承之前版本的CRF模块从而造成计算瓶颈,而在DeepLab v3中移除了该模块从而实现纯卷积架构。\n另外由于ASPP不同空洞率分支感受野重叠严重问题,在v3中引入1x1卷积和全局平均池化分支,通过补充局部与全局上下文信息,缓解空洞卷积边界栅格效应。\nDeepLab v3+ DeepLab v3+为了解决v3中直接8倍或16倍采样导致边界模糊导致的物体轮廓断裂、小目标分割精度低的问题,新增了轻量级Decoder模块。\n另外将主干网络从ResNet升级为Xception。\n可以看到整个DeepLab结构变动并不是很大。\n代码实现 最后说完理论上网络结构的变化,下面提供相应代码的实现:\nDeepLab官方基于TensorFlow的实现deeplab DeepLab v3的PyTorch实现DeepLabV3Plus-Pytorch或直接使用TorchVision中的模型 U-Net的PyTorch实现unet UNet++的实现pytorch-nested-unet及segmentation-models-pytorch UNet++的PyTorch实现UNet-3-Plus-Pytorch ","date":1750388319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750991248,"objectID":"ca83d0d3cf499878e9db3e05f96bcc8e","permalink":"https://zhuzhulang.github.io/blog/posts/from-unet-to-deeplab/","publishdate":"2025-06-20T10:58:39+08:00","relpermalink":"/posts/from-unet-to-deeplab/","section":"posts","summary":"如何通过UNet和DeepLab网络架构的演进逐步解决语义分割中出现的问题","tags":["AI"],"title":"从UNet到DeepLab:语义分割网络的架构演进与技术突破","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"国内的软件环境,实际上PostgreSQL用的企业并不是,但是PostgreSQL提供了一些扩展可以让关系型数据库转换为向量数据库。\n借助PostgreSQL的向量化扩展,其支持:\n提取并搜索最近邻 支持单精度、半精度、二进制和稀疏向量 任何语言的PostgreSQL客户端 支持多种距离度量方式,如L2距离、内积等6种方法 在PostgreSQL中提供了2种扩展:\npg_vector pg_embedding 这2种扩展都提供了对HNSW索引的支持,但是前者还提供了IVFFlat的支持。关于这部分内容实际上是数据挖掘中的内容,这里就不赘述了。更多内容可以参考之前的文章HNSW算法简述。需要注意的是,后者现在处于维护状态,换句话就是作者现在不维护了。\n如果说向量数据库有什么用,实际上跟Dify并没有太大的联系。主要还是RAG的热度让其被大家所熟知,而该领域也只是个细分的领域,可以说是坑还是蛮多的。对于中小企业就不要想了,因为其成本根本不是企业能承受的。因此更多只是停留在demo阶段。\n实际上除了RAG外,向量化还可以用于其他领域,比如听歌识曲这样的app。使用向量数据库可以很轻松解决查询效率慢的问题。\n关于pg_vector扩展的使用可以参考原来项目。需要注意的是,其对PostgreSQL的版本要求是\u0026gt;=13.0。如果你使用的是低版本,就不要考虑了。其安装方法主要有2种:\n通过下载源码并编译 使用docker镜像 而docker镜像可以使用类似如下的方法:\ndocker pull pgvector/pgvector:pg[version] 其中version替换为对应的数值,比如13、17,分别表示PostgreSQL版本13和17的镜像。\n而源码安装的方式如下,首先是Linux和MacOS环境:\ncd /tmp git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git cd pgvector make make install # may need sudo 可以将v0.8.0替换为任何版本,如v0.6.0。 而在Windows上则需要进行额外的配置,需要确保已经安装了Visual Studio 2022:\nset \u0026#34;PGROOT=\\path\\to\\PostgreSQL\\17\u0026#34; cd %TEMP% git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git cd pgvector nmake /F Makefile.win nmake /F Makefile.win install 因此一般测试使用建议通过Docker的方式进行安装。\n安装完成后,就可以创建对应的扩展:\nCREATE EXTENSION vector; 而在创建表时添加对应的向量列,例如:\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3)); 而插入向量则以如下的方式进行添加:\nINSERT INTO items (embedding) VALUES (\u0026#39;[1,2,3]\u0026#39;), (\u0026#39;[4,5,6]\u0026#39;); 之后可以通过L2距离查询其最近邻:\nSELECT * FROM items ORDER BY embedding \u0026lt;-\u0026gt; \u0026#39;[3,1,2]\u0026#39; LIMIT 5; 其支持的距离函数如下:\n\u0026lt;-\u0026gt; - L2距离 \u0026lt;#\u0026gt; - (negative)内积或点积 \u0026lt;=\u0026gt; - 余弦距离 \u0026lt;+\u0026gt; - L1距离 \u0026lt;~\u0026gt; - Hamming距离 (binary vectors) \u0026lt;%\u0026gt; - Jaccard距离 (binary vectors) 如果你觉得对这些概念生疏,建议学习一下数据挖掘或数据科学中的知识。整体来说,难度不是大。\n最后该数据库可以支撑几百万数据量的业务场景,对于更大数据量需要使用其他的向量数据库。因为向量相似度计算是很耗资源的,否则会出现响应极度缓慢的问题。\n参考文章:\nhttps://www.timescale.com/blog/postgresql-as-a-vector-database-using-pgvector\n","date":1750147455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750212943,"objectID":"fbf2b714cd418f0997dfdb089999159e","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-postgresql-as-vector-database/","publishdate":"2025-06-17T16:04:15+08:00","relpermalink":"/posts/how-to-use-postgresql-as-vector-database/","section":"posts","summary":"如何将PostgreSQL摇身一变为向量数据库,并在项目中进行使用","tags":["AI"],"title":"PostgreSQL + 向量搜索:解锁关系型数据库的AI潜能","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"之前介绍了如何在Dify中生成图表,而本篇文章的主要目的是新增图表类型,例如新增1个雷达图的类型。\n可以看到在之前Dify二次开发-生成图表中,该插件支持3种图表类型,功能比较基础。现在在其基础上进行迭代和扩展。\n首先其离线包实际上是使用zip格式进行压缩的文件:\n$ zipinfo langgenius-echarts_0.0.1.difypkg Archive: langgenius-echarts_0.0.1.difypkg Zip file size: 8763 bytes, number of entries: 16 -rw---- 2.0 fat 145 bl defN 80-000-00 00:00 .env.example -rw---- 2.0 fat 0 bl defN 80-000-00 00:00 README.md -rw---- 2.0 fat 2719 bl defN 80-000-00 00:00 _assets/icon.svg -rw---- 2.0 fat 148 bl defN 80-000-00 00:00 main.py ... -rw---- 2.0 fat 36 bl defN 80-000-00 00:00 .verification.dify.json -rw---- 2.0 fat 36 bl defN 80-000-00 00:00 .verification.dify.json 16 files, 13711 bytes uncompressed, 6073 bytes compressed: 55.7% 可以修改其目录结构实现功能的迭代。具体代码实现这里不再赘述,之前介绍过相关的逻辑。\n关于其打包的逻辑可以参考。并在.env中配置对应参数:\nFORCE_VERIFYING_SIGNATURE=false 这样Dify就不会校验其签名并可以成功安装了。可以使用如下方式查看容器的环境变量:\nsudo docker inspect --format=\u0026#39;{{range .Config.Env}}{{println .}}{{end}}\u0026#39; [容器ID] 在Windows上其打包工具可以访问dify-plugin-daemon进行下载。整体来说难度并不是很大。\n其最终效果如下:\n参考文章:\nhttps://docs.dify.ai/zh-hans/plugins/quick-start/install-plugins\nhttps://docs.dify.ai/zh-hans/plugins/quick-start/develop-plugins/initialize-development-tools\n","date":1750029624,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750038412,"objectID":"df21836246b48ff268c91968381daec1","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-add-new-chart-in-plugins/","publishdate":"2025-06-16T07:20:24+08:00","relpermalink":"/posts/how-to-add-new-chart-in-plugins/","section":"posts","summary":"如何根据实际需求新增图表","tags":["AI"],"title":"Dify二次开发-新增图表类型","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"这篇文章实际上是在之前文章Dify二次开发-使用MCP协议查询数据库数据基础上进行的技术迭代。 通过MCP协议获取到数据后,我们需要将数据转换为对应图表的格式从而让其在Dify中显示出来。这里选择是Echarts的插件,该插件接收3个参数:\ntitle: 标题 data: 数据 x_axis: x轴 但是该插件支持3种图表类型:\n线性图表 柱状图 饼图 可以看到功能比较基础,如果有额外的需求还需要进行定制化开发,难度并不是很大。后续有时间再介绍下如何定制进行开发。 最终其整个流程图如下:\n我们可以输入如下的内容让LLM生成对应的图表:\n获取2023年每个月漏洞的数量,返回内容是月份及数量,格式为JSON数据,如\u0026#39;[{\u0026#34;month\u0026#34;:\u0026#34;1月\u0026#34;,\u0026#34;num\u0026#34;:100},...]\u0026#39; 当开始时需要输入最终图表的标题,之后经过MCP服务器获取对应的数据,紧接着对数据进行转换,将转换后的输出输入到线形图中,最终进行输出。可以看到流程还是很清楚的,下面是其最终效果:\n不得不说MCP还是蛮方便的,稍加修改即可作为真实项目了。\n","date":1749971353,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750056253,"objectID":"7651ac98999017a6ec346f8f93110f2a","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-dify-generate-graph/","publishdate":"2025-06-15T15:09:13+08:00","relpermalink":"/posts/how-to-use-dify-generate-graph/","section":"posts","summary":"如何在Dify中根据数据库中的数据生成图表","tags":["AI"],"title":"Dify二次开发-生成图表","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"这应该是Dify二次开发最后一篇文章了,主要是觉得没什么可以写的了。而且Dify调试起来很不方便,不想花费太多心思在这上面。而且要通过该技能找到工作还是蛮难的,现在的企业都想找熟手的,问题哪里可以快速培养出这么多的人来。\n说是月薪15K,但很多企业连10K都给不到,实际上Dify开发只要1-3年Python开发经验即可。一边是大模型无所不能的需求,一边是薪资对不起就业的环境。最近才遇到一个企业的领导,觉得我Dify不是很深入。这平台出来3年都不好,你要资深不得一出来就开始玩。既要你懂还要经验丰富,薪资就15-30K。后面那个数字基本不用看了,结果还是一时兴起想找个人做这个项目,但又不确定能让你干多久,还要求会Java。敢问会Java能进行Dify二次开发?更有意思的是说他也是搞技术的,哈哈。\n这里Dify版本必须大于1.0.0才能使用其插件功能,这里使用的版本是1.0.1。首先是对应插件的安装:\n接着是MCP服务器代码的编写,这里使用的是FastMCP库,可以直接使用pip进行安装。但是需要确保Python的版本大于3.10。其对应的代码如下:\nimport psycopg2 from fastmcp import FastMCP mcp = FastMCP(\u0026#34;postgresql-mcp\u0026#34;,port=9000) conn_params = { \u0026#34;dbname\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, # 默认本地 \u0026#34;port\u0026#34;: \u0026#34;5432\u0026#34; # PostgreSQL默认端口 } @mcp.tool() def execute_sql(query:str) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34; 执行SQL查询语句 参数: query (str): 要执行的SQL语句，支持多条语句以分号分隔 返回: list: 包含查询结果的TextContent列表 - 对于SELECT查询：返回CSV格式的结果，包含列名和数据 - 对于SHOW TABLES：返回数据库中的所有表名 - 对于其他查询：返回执行状态和影响行数 - 多条语句的结果以\u0026#34;---\u0026#34;分隔 \u0026#34;\u0026#34;\u0026#34; try: with psycopg2.connect(**conn_params) as conn: with conn.cursor() as cursor: statements = [stmt.strip() for stmt in query.split(\u0026#34;;\u0026#34;) if stmt.strip()] results = [] for statement in statements: try: print(\u0026#34;SQL:{}\u0026#34;.format(statement)) cursor.execute(statement) # 检查语句是否返回了结果集 (SELECT, SHOW, EXPLAIN, etc.) if cursor.description: columns = [desc[0] for desc in cursor.description] rows = cursor.fetchall() # 将每一行的数据转换为字符串，特殊处理None值 formatted_rows = [] for row in rows: formatted_row = [ \u0026#34;NULL\u0026#34; if value is None else str(value) for value in row ] formatted_rows.append(\u0026#34;,\u0026#34;.join(formatted_row)) # 将列名和数据合并为CSV格式 results.append( \u0026#34;\\n\u0026#34;.join([\u0026#34;,\u0026#34;.join(columns)] + formatted_rows) ) # 如果语句没有返回结果集 (INSERT, UPDATE, DELETE, etc.) else: conn.commit() # 只有在非查询语句时才提交 results.append(f\u0026#34;查询执行成功。影响行数: {cursor.rowcount}\u0026#34;) except Error as stmt_error: # 单条语句执行出错时，记录错误并继续执行 results.append( f\u0026#34;执行语句 \u0026#39;{statement}\u0026#39; 出错: {str(stmt_error)}\u0026#34; ) # 可以在这里选择是否继续执行后续语句，目前是继续 return [\u0026#34;\\n---\\n\u0026#34;.join(results)] except Exception as e: print(f\u0026#34;Error for \u0026#39;{query}\u0026#39;: {e}\u0026#34;) return [f\u0026#34;Error for: {str(e)}\u0026#34;] if __name__ == \u0026#39;__main__\u0026#39;: mcp.run(transport=\u0026#34;sse\u0026#34;) 之后是对MCP SSE插件进行授权,其内容类似如下:\n{ \u0026#34;postgresql-mcp\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://127.0.0.1:9000/sse\u0026#34;, \u0026#34;headers\u0026#34;: {}, \u0026#34;timeout\u0026#34;: 500, \u0026#34;sse_read_timeout\u0026#34;: 50 }} 可以根据实际的IP进行修改。需要注意的是其2个插件版本要求必须是0.06才能正常调用。 最后就可以创建应用了,选择Chatflow即可。按照要求填写相关的内容。其中MCP服务器地址为:\n{ \u0026#34;postgresql-mcp\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://127.0.0.1:9000/sse\u0026#34;, \u0026#34;headers\u0026#34;: {}, \u0026#34;timeout\u0026#34;: 150, \u0026#34;sse_read_timeout\u0026#34;: 50 }} 这里使用MCP对PostgreSQL中的数据进行查询,只涉及单表数据的查询,因此效果会很好。 而指令中内容如下:\n使用中文回复。 当用户提问中涉及漏洞实体时,需要使用postgresql-mcp进行数据查询和操作,表结构说明如下: ## 漏洞信息表(vulnerability_information) | 字段名 | 类型 | 描述 | |--------------|------------------------|---------| | id | integer | ID | | bug_name | character varying(200) | 漏洞名称 | | cnnvd_id | character varying(20) | CNNVD号 | | publish_date | date | 发布时间 | | bug_desc | text | 漏洞描述 | | cve_id | character varying(20) | CVE号 | | severity | character varying(2) | 风险级别 | 其最终效果如下:\n可以看到输入内容后,对应的结果就开始不断输出了。而在MCP服务器这边可以看到如下的输出:\nSQL:SELECT * FROM vulnerability_information WHERE publish_date \u0026gt;= \u0026#39;2025-01-01\u0026#39; AND publish_date \u0026lt; \u0026#39;2026-01-01\u0026#39; LIMIT 10 可以清楚看到大模型很好的理解其需求,并输出了正确的SQL语句并执行。而对于多表复杂的条件,其效果则会大打折扣。因此实际商用还面临很多挑战急需解决。\n参考文章:\nhttps://www.cnblogs.com/xiao987334176/p/18827261\nhttps://www.cnblogs.com/xiao987334176/p/18822444\n","date":1749817650,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1749826631,"objectID":"53866d24a6a630ad11a61511c8995893","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-mcp-protocol-query-data/","publishdate":"2025-06-13T20:27:30+08:00","relpermalink":"/posts/how-to-use-mcp-protocol-query-data/","section":"posts","summary":"如何通过MCP协议对数据库中数据进行查询","tags":["AI"],"title":"Dify二次开发-使用MCP协议查询数据库数据","type":"posts"},{"authors":[],"categories":["杂谈"],"content":"实际上arXiv平台上论文只是初稿,并没有经过同行的评审,因此其可信度需要结合实验复现、论文引用量及作者团队背景综合判断其可信度。不过这种深度一般需要研究生学历才会接触到,但实际上本科生稍加学习也是可以应对的。\n一般来说,这些论文单词量并不会很大,CET-4基本可以应付。重要的是信心,不要胆怯,而且论文中很多想法实际上你也是可以想到的。在阅读论文时,要带着问题,即本次你想从这篇论文中学习到什么。比如想看下它说的是什么,或者想看下它的网络结构是怎样设计的。每次阅读一部分,逐次蚕食吸收,自然有所成就。 一般而言,每篇论文开头都会介绍相关的背景,比如之前有什么类似的技术,而这些技术得到了怎样的效果或有什么缺点。之后就开始进入自己内容的介绍,介绍的想法、网络结构,有什么优势,解决怎样的问题。最后自然是使用的数据集、训练参数、训练结果的展现以及细节的叙述。就比如这段内容:\nTTS is a typical one-to-many mapping problem, since multiple possible speech sequences can correspond to a text sequence due to different variations in speech audio, such as pitch, duration,sound volume and prosody. In autoregressive TTS, the decoder can condition on the text sequence and the previous mel-spectrograms to predict next mel-spectrograms, where the previous mel-spectrograms can provide some variation information and thus alleviate this problem to a certain degree. While in non-autoregressive TTS, the only input information is text which is not enough to fully predict the variance in speech. In this case, the model is prone to overfit to the variations of the target speech in the training set, resulting in poor generalization ability. 自己的翻译是:\nTTS是典型的一对多映射问题,因为一个文本序列由于不同变化(音高、时长、音量和韵律)可以对应多种可能语音序列。在自回归TTS中,解码器可以以文本序列作为条件,通过之前的梅尔频谱预测下一个梅尔频谱,其中之前的梅尔频谱可以提供一些变体信息从而在一定程序上缓解这一问题。而在非自回归TTS中,输入的信息只有文本,它是不足以完全预测语音中的变化。在这种情况下,模型容易拟合训练集目标语音的变化,结构泛化较差。 而DeepSeek的翻译如下:\n文本到语音（TTS）是一个典型的一对多映射问题。由于语音音频中存在音高、时长、音量和韵律等多种可变因素，同一文本序列可能对应多个不同的语音序列。在自回归TTS模型中，解码器可以通过文本序列和前一时间步的梅尔频谱图来预测后续频谱，其中历史梅尔频谱提供了部分变化信息，从而在一定程度上缓解了这一问题。而在非自回归TTS模型中，仅有文本输入信息不足以完整预测语音的所有动态变化。这种情况下，模型容易过度拟合训练数据中的语音变化特征，导致泛化能力显著下降。 阅读论文没必要做到100%都翻译正确,掌握大概的意思即可。毕竟语言之间的鸿沟,在深度神经网络盛行的今天,已经区别不是那么大了。如果可以话,可以借助一些翻译工具。\n因此有一些插件可以同步翻译,如沉浸式翻译及有道翻译。\n这里以FastSpeech作为例子进行说明,其paper分别为:\nFastSpeech: Fast, Robust and Controllable Text to Speech FastSpeech 2: Fast and High-Quality End-to-End Text to Speech 首先从其编号1905和2006可以看出这两篇文章分别发布在19年5月和20年6月。从其标题可以看出,FastSpeech是一种快速、鲁棒和可控的文本到语音合成。而FastSpeech2是快速高质量端到端文本到语音合成。\n其中FastSpeech有5个版本,而FastSpeech2有8个版本,一般我们选择最新版本进行阅读即可。在这里主要是为了获取其网络结构,然后编写代码进行实现。\n首先对比其网络结构,下面是FastSpeech的结构:\n之后是FastSpeech2的结构:\n在FastSpeech中可以看到音素(Phoneme)经过音素嵌入层,再经过位置编码进入N层的前馈变换器块。之后经过长度调节器(Length Regulator)后,与位置编码一起再经过N层的前馈变换器块(FFT Block)后,经过线性层输出梅尔频谱图。\n而FastSpeech2中的变化是将第1个FFT Block修改为编码器(Encoder),经过方差适配器(Variance Adaptor),叠加位置编码后输出到梅尔频谱解码器(Mel-spectrogram Decoder)和声波解码器(Waveform Decoder)中。\n对于代码的复现,可以参考Paperswithcode,我们可以搜索相关的paper,查看其是否有一些代码、数据集及结果的测评。\n从上图可以看到,FastSpeech有多个实现,根据框架不同选择对应的实现。而数据集只有LJSpeech。\n在FastSpeech2的实现ming024/FastSpeech2中有如下的代码:\nclass FastSpeech2(nn.Module): \u0026#34;\u0026#34;\u0026#34; FastSpeech2 \u0026#34;\u0026#34;\u0026#34; def __init__(self, preprocess_config, model_config): super(FastSpeech2, self).__init__() self.model_config = model_config self.encoder = Encoder(model_config) self.variance_adaptor = VarianceAdaptor(preprocess_config, model_config) self.decoder = Decoder(model_config) self.mel_linear = nn.Linear( model_config[\u0026#34;transformer\u0026#34;][\u0026#34;decoder_hidden\u0026#34;], preprocess_config[\u0026#34;preprocessing\u0026#34;][\u0026#34;mel\u0026#34;][\u0026#34;n_mel_channels\u0026#34;], ) self.postnet = PostNet() 正好对应之前的网络结构。\n总体而言,对于论文的阅读,需要对基础有一定的了解,能够举一反三。论文中可能会提出一些很新颖的名词,而实际上就是你熟悉的某些方法。不要被他吓到了。\n","date":1748232545,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748245882,"objectID":"eb9768461150a1c40e26c58556d5d619","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-read-arxiv-papers/","publishdate":"2025-05-26T12:09:05+08:00","relpermalink":"/posts/how-to-read-arxiv-papers/","section":"posts","summary":"介绍一些阅读arXiv中paper的方法","tags":["other"],"title":"如何阅读arXiv的初稿","type":"posts"},{"authors":null,"categories":["AI实战"],"content":"前言 之前介绍了如何新增模型提供商,下面继续之前的内容,介绍如何在Dify中新增内置工具。按照之前的做法,本次新增一个Doga Speech的小工具,用于将输入的文本转换为语音输出。\n开始操作 首先在dify/api/core/tools/provider/builtin目录下新增1个doga的包:\nmkdir dify/api/core/tools/provider/builtin/doga 其中配置doga.yaml中内容如下:\nidentity: author: demo name: doga label: en_US: Doga Speech zh_Hans: Doga Speech description: en_US: a tool for speech synthesis zh_Hans: 语音合成小工具 icon: doga_speech.png tags: - utilities 如果之前看过新增模型提供商文章的小伙伴,应该对这段代码感觉很熟悉。分别是说明工具的作者、显示的标签、文本说明、图标及类型。 在doga.py模块中实现模型提供商相关认证代码,由于不需要认证,因此其代码如下:\nfrom typing import Any from core.tools.provider.builtin_tool_provider import BuiltinToolProviderController class DogaProvider(BuiltinToolProviderController): def _validate_credentials(self, credentials: dict[str, Any]): pass 之后创建1个tools的子包用于表示该工具有哪些功能。由于只有简单的TTS功能,因此目录下doga_speech.yaml的内容如下:\nidentity: name: doga_speech author: doga label: en_US: Doga Speech zh_Hans: Doga语音 description: human: en_US: A tool for speech synthesis zh_Hans: 用于语音合成的工具 llm: A tool for speech synthesis parameters: - name: input_text type: string required: true label: en_US: input text zh_Hans: 文本输入 human_description: en_US: input text for speech synthesis zh_Hans: 用于语音合成的输入文本 form: form 其中要求用户输入1个文本字段,接收到该字段后在doga_speech.py模块中进行相应的处理并返回对应的内容。\nfrom core.tools.tool.builtin_tool import BuiltinTool from core.tools.entities.tool_entities import ToolInvokeMessage from typing import Any, Dict, List, Union class DogaSpeechTool(BuiltinTool): def _invoke(self, tool_Parameters: Dict[str, Any]): input_text = tool_Parameters[\u0026#34;input_text\u0026#34;] if input_text: return self.create_text_message(\u0026#34;Audio generated successfully\u0026#34;) return self.create_text_message(\u0026#34;Audio generated failed\u0026#34;), 编写完上述代码后,在Dify界面中可以搜索到如下的工具:\n之后创建1个Agent,其对应的工具添加如下:\n更多内容可以查看官方文档,本任务相对来说比较基础和简单。\n参考文章:\nhttps://docs.dify.ai/zh-hans/guides/tools/quick-tool-integration\n","date":1748098527,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748221550,"objectID":"f246157b0f8db2699b55b23da6e59b53","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-add-custom-tool/","publishdate":"2025-05-24T22:55:27+08:00","relpermalink":"/posts/how-to-add-custom-tool/","section":"posts","summary":"在Dify上新增内置工具","tags":["AI"],"title":"Dify二次开发-新增工具","type":"posts"},{"authors":null,"categories":null,"content":"1. 致谢 感谢金主爸爸们的投喂！本咸鱼写博客纯属用爱发电，没想到居然真能换到茶叶蛋钱，这下更有动力摸鱼更新了（老板：？）\n2. 打赏名单 以下是打赏名单列表:\n打赏时间 打赏者 打赏方式 打赏金额 2025.3.21 **文 支付宝 ￥0.1 2025.5.20 *民 微信 ￥1 ","date":1748046214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748049546,"objectID":"13e23b0fba5009f20ac372d2c13bc1da","permalink":"https://zhuzhulang.github.io/blog/reward/","publishdate":"2025-05-24T08:23:34+08:00","relpermalink":"/reward/","section":"","summary":"1. 致谢 感谢金主爸爸们的投喂！本咸鱼写博客纯属用爱发电，没想到居然真能换到茶叶蛋钱，这下更有动力摸鱼更新了（老板：？）\n2. 打赏名单 以下是打赏名单列表:\n打赏时间 打赏者 打赏方式 打赏金额 2025.3.21 **文 支付宝 ￥0.1 2025.5.20 *民 微信 ￥1 ","tags":null,"title":"感谢网友打赏","type":"page"},{"authors":[],"categories":["AI实战"],"content":"没想到介绍Dify二次开发第4篇文章竟然是重置登录邮箱的,这个实在太简单了,可以说毫无难度,只是官方文档里面没有写。至于第3篇还在修改中,预计很快就能上线了。 在官方文档中介绍了如何重置管理员密码,详情可以参考,使用的方式是:\ndocker exec -it docker-api-1 flask reset-password\r如果想重置管理员密码呢?此时有两种方式,一种是直接进入数据库修改密码,其中的数据库是dify:\n但是这种方式很容易不小心就把数据库给搞坏了。 下面推荐一种更为靠谱的方式,就是使用官方提供的命令:\ndocker exec -it docker_api_1 flask reset-email\r然后就会在终端中出现对应的信息:\n可以看到最终更新邮箱成功了。 这个功能看似没用,实际上在二次开发中特别是权限管理时还有有些用处的。比如添加新的账户的场景,这里就不赘述了,操作难度不大。\n","date":1747898214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748221619,"objectID":"f51e174c8852546015273449badae4d1","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-reset-email-in-dify/","publishdate":"2025-05-22T15:16:54+08:00","relpermalink":"/posts/how-to-reset-email-in-dify/","section":"posts","summary":"如何重置Dify的管理员邮箱","tags":["AI"],"title":"Dify二次开发-重置管理员邮箱","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"前言 之前介绍了如何搭建Dify的环境,可以说是非常的简单,没什么难度。下面开始正式进行Dify的二次开发,预计要分多篇文章来叙述了。 整体来说,二次Dify开发难度并不大,可能会有一些坑,但是对于经验丰富的我来说都不成问题。不知道谁说Dify是高级Python工程师应有的水平,我只能呵呵了。 废话不多说,这里先实现个小小的目标,自定义模型服务商吧。首先提前申明下下面的代码是基于0.15.3的。 在官方文档中有如下这么一个提示:\n相关内容可以参考。可以看到,如果要使用插件功能,只能升级到1.0.0版本。而且Dify这个项目更新很快,换句话说就是很不稳定。\n新增模型提供商 下面开始正式的操作。先创建1个doga的包,主要是纪念如下的人物:\n这里没什么恶意,仅仅是学习而已。整个包的目录结构如下:\ndoga ├── _assets │ ├── icon_l_en.png │ └── icon_s_en.png ├── doga.py ├── doga.yaml ├── __init__.py └── llm ├── doga-1.0.yaml ├── __init__.py └── llm.py 其中_assets目录用于存储logo,而dogma.yaml是配置文件。 在doga.yaml中先定义如下的内容:\nprovider: doga\rlabel:\ren_US: Doga\rzh_Hans: 卡波苏\rdescription:\rzh_Hans: 卡波苏模型\ren_US: doga model\ricon_small:\ren_US: icon_s_en.png\rzh_Hans: icon_s_en.png\ricon_large:\ren_US: icon_l_en.png\rzh_Hans: icon_l_en.png\rbackground: \u0026#34;#93c5fd\u0026#34;\rsupported_model_types:\r- llm\rconfigurate_methods:\r- predefined-model\rprovider_credential_schema:\rcredential_form_schemas:\r- variable: doga_api_key\rlabel:\ren_US: API Key\rtype: secret-input\rrequired: true\rplaceholder:\rzh_Hans: 请在此输入您的API Key\ren_US: Please enter your API Key\r- variable: doga_endpoint_url\rlabel:\rzh_Hans: 自定义API endpoint地址\ren_US: Custom API endpoint URL\rtype: text-input\rrequired: false\rplaceholder:\rzh_Hans: Base URL, e.g. https://api.example.com/v1\ren_US: Base URL, e.g. https://api.example.com/v1 相关的说明可以查看参考文章中的链接。由于该模态只支持LLM对话,因此只有llm子包,其中的llm.py中需要实现一个继承自LargeLanguageModel的自定义类,该类需要实现如下一些方法:\n_invoke,模型运行调用 get_num_tokens,预计算输入 tokens validate_credentials,模型凭据校验 _invoke_error_mapping,调用异常错误映射表 相关的代码这里就赘述了,完整的代码可以参考。\n最终效果 最后是更新后的效果,成功出现了自己定义的模型提供商:\n配置出现如下的页面:\n输入对应的API key后,我们创建一个空白的应用进行测试:\n选择新增的模型提供商,再输入相应的内容可以看到如下的结果:\n可以说,整个过程还是很简单的,只需要按照说明进行操作即可。\n参考文章:\nhttps://docs.dify.ai/zh-hans/guides/model-configuration/new-provider\nhttps://docs.dify.ai/zh-hans/guides/model-configuration/predefined-model\n","date":1747725414,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748221500,"objectID":"0869d5f0e50d852d604cf45c22a08f95","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-add-custom-providers/","publishdate":"2025-05-20T15:16:54+08:00","relpermalink":"/posts/how-to-add-custom-providers/","section":"posts","summary":"如何在Dify上新增模型提供商","tags":["AI"],"title":"Dify二次开发-新增模型提供商","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"\n搭建前准备工序 其实Dify是个传统的东西,只是搭上了AI的快车。下面简单说下其搭建过程。 在开始之前,先看下自己服务器的配置是否满足如下的条件:\n硬件方面必须是双核4G内存,这很容易满足。磁盘大约需要占用15G,而软件方面Docker版本必须大于19.03且Docker Compose需要大于1.28。因为Docker版本18.x存在线程创建限制。 不妨这么进行查看:\n~$ docker --version Docker version 19.03.15, build 99e3ed8919 ~$ docker-compose --version docker-compose version 1.29.0, build 07737305 相比几十M的版本2,更喜欢只有10来M的版本1,这里直接使用如下方式下载Docker-Compose:\nwget http://github.com/docker/compose/releases/download/1.29.0/docker-compose-linux-x86_64 chmod +x docker-compose-linux-x86_64 mv docker-compose-linux-x86_64 /usr/bin/docker-compose 开始搭建环境 一切准备就绪,开始拉取代码:\n# 假设当前最新版本为 0.15.3 git clone https://github.com/langgenius/dify.git --branch 0.15.3 进入源代码的docker目录并开始拉取并打包:\ncd dify/docker cp .env.example .env sudo docker-compose up -d 一堆操作猛如虎,难就难在国内Docker-Hub访问不了。按照下面方式修改镜像源:\nsudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;:[ \u0026#34;https://docker.1ms.run\u0026#34;, \u0026#34;https://docker.xuanyuan.me\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.m.daocloud.io\u0026#34; ] } EOF sudo systemctl daemon-reload \u0026amp;\u0026amp; sudo systemctl restart docker 重新拉取,经过1个多小时后,如果网速不怎么好的情况下,最终安装完成的截图:\n别忘了初始化管理员账户即可使用了:\n# 本地环境 http://localhost/install # 服务器环境 http://your_server_ip/install 如下图所示:\n访问Dify的URL即可使用:\n# 本地环境 http://localhost # 服务器环境 http://your_server_ip 探索页面如下:\n效果如下:\n常见问题 如果使用docker-compose成功拉取镜像但是出不来管理员账户页面且出现502问题,大概率是容器没有创建线程的权限,修改docker-compose.yaml中api部分的内容:\napi: image: langgenius/dify-api:0.15.3 privileged: true cap_add: - SYS_ADMIN ulimits: nproc: 65535 security_opt: - seccomp:unconfined 可以看到docker容器的日志是否出现如下的异常:\nOpenBLAS blas_thread_init: pthread_create failed for thread 7 of 32: Operation not permitted OpenBLAS blas_thread_init: RLIMIT_NPROC -1 current, -1 max 参考文章:\nhttps://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose\n","date":1747466214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748221610,"objectID":"194a43e3487b02682c72ef6c6b74ab4c","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-build-dify-environment/","publishdate":"2025-05-17T15:16:54+08:00","relpermalink":"/posts/how-to-build-dify-environment/","section":"posts","summary":"如何搭建Dify环境","tags":["AI"],"title":"Dify二次开发-环境搭建","type":"posts"},{"authors":["码力全开"],"categories":["TTS"],"content":"这里介绍下如何通过FastSpeech2项目进行中文的语音合成,由于该仓库把相关demo数据也提交了,因此仓库比较大。此时可以使用sparse-checkout拉取相关的代码,而忽略一些无关紧要的目录。\n要确保git的版本大于2.25,可以使用git version进行查看。\n接着就开始代码的拉取:\ngit clone --filter=blob:none --depth=1 --no-checkout https://github.com/ming024/FastSpeech2 cd FastSpeech2 git sparse-checkout init --cone 接着修改.git/info/sparse-checkout中的内容为:\n/* !/demo/ /preprocessed_data/*/*.json 最后进行git checkout即可。\n之后就是对应模型的下载了,可以访问FastSpeech2进行对应模型的下载,密码是9615。其中AISHELL-3是中文多人,LJSpeech是英文单人,LibriTTS是英文多人。\n将下载的模型放置在目录output/ckpt对应目录下,也不知道为何要搞这么复杂。只能手动创建对应的目录:\nmkdir -p output/ckpt/{AISHELL3,LJSpeech,LibriTTS} mkdir -p output/result/{AISHELL3,LJSpeech,LibriTTS} 需要注意的是,放置在目录中的文件必须是pth.tar后缀结尾,且需要去掉之前的语料库名称只保留数字即可,比如LJSpeech_900000.zip需要变为900000.pth.tar。\n一切准备就绪,就可以开始推理了。这里使用GPU进行推理:\n$ python synthesize.py --text \u0026#34;哈哈,我是个传说的人物\u0026#34; --speaker_id 0 --restore_step 600000 --mode single -p config/AISHE LL3/preprocess.yaml -m config/AISHELL3/model.yaml -t config/AISHELL3/train.yaml 最终的效果如下:\n可以听见机械音还是有点重的。\n当然也可以参考之前写的另一篇使用原神中人物的语音合成,相对来说效果会更好一些。详情请点击。\n参考文章:\nhttps://youwu.today/blog/git-sparse-checkout-for-partial-repository-clone/\n","date":1745888507,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748222758,"objectID":"474aa3d3b2b58bcd3cc7a5d34d88fb00","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-fastspeech2-for-speech-synthesis/","publishdate":"2025-04-29T09:01:47+08:00","relpermalink":"/posts/how-to-use-fastspeech2-for-speech-synthesis/","section":"posts","summary":"如何使用FastSpeech2的模型实现中文语音合成","tags":["AI"],"title":"使用FastSpeech2进行语音合成","type":"posts"},{"authors":[],"categories":["TTS"],"content":"VITS是什么? VITS(Variational Inference with adversarial learning for end-to-end Text-to-Speech)是一种基于深度学习的端到端语音合成技术。相较于传统TTS系统，它无需复杂的声学模型和声码器，直接通过文本生成自然流畅的语音。\n其核心在于：\n变分自编码器(VAE)结构 对抗训练机制 蒙特卡洛flow-based时长预测 其内部由3个模型组成,分别是GAN、VAE和Flow。\n安装必备工具 下面先安装相应的工具:\n# 基础环境 conda create -n vits python=3.8 conda activate vits pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0 -f https://download.pytorch.org/whl/cu113/torch_stable.html # 核心依赖 pip install numpy scipy matplotlib pandas pip install tensorboard librosa unidecode inflect 获取代码仓库 这里没有选择官方的仓库,而是选择如下微调的仓库代码,因为其支持中文模型:\ngit clone https://github.com/Plachtaa/VITS-fast-fine-tuning.git pip install -r requirements.txt 下载原神角色模型 访问【VITS-genshin】下载相关的模型,密码是8243。将整个目录下载下来放在源码目录下。\n合成你的第一段语音 在项目仓库目录下新建1个脚本,其中的代码如下\nimport os import torch import utils import commons import logging import soundfile as sf from text import text_to_sequence from models import SynthesizerTrn from torch import no_grad, LongTensor logger = logging.getLogger(\u0026#34;jieba\u0026#34;) logger.setLevel(logging.WARNING) limitation = False hps_ms = utils.get_hparams_from_file(\u0026#39;./configs/uma_trilingual.json\u0026#39;) device = torch.device(\u0026#34;cpu\u0026#34;) net_g_ms = SynthesizerTrn( len(hps_ms.symbols), hps_ms.data.filter_length // 2 + 1, hps_ms.train.segment_size // hps_ms.data.hop_length, n_speakers=hps_ms.data.n_speakers, **hps_ms.model) _ = net_g_ms.eval().to(device) model, optimizer, learning_rate, epochs = utils.load_checkpoint(\u0026#34;./pretrained_models/G_trilingual.pth\u0026#34;, net_g_ms, None) def get_text(text, hps): text_norm = text_to_sequence(text, hps.symbols, hps.data.text_cleaners) if hps.data.add_blank: text_norm = commons.intersperse(text_norm, 0) text_norm = LongTensor(text_norm) return text_norm def vits(text, language, speaker_id, noise_scale, noise_scale_w, length_scale): if not len(text): return \u0026#34;输入文本不能为空！\u0026#34;, None, None text = text.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;).replace(\u0026#39;\\r\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;) length = len(text) if length \u0026gt; 100: return \u0026#34;输入文字过长!\u0026#34;, None, None if language == 0: text = \u0026#34;[ZH]{}[ZH]\u0026#34;.format(text) elif language == 1: text = \u0026#34;[JA]{}[JA]\u0026#34;.format(text) stn_tst = get_text(text, hps_ms) with no_grad(): x_tst = stn_tst.unsqueeze(0).to(device) x_tst_lengths = LongTensor([stn_tst.size(0)]).to(device) speaker_id = LongTensor([speaker_id]).to(device) audio = net_g_ms.infer(x_tst, x_tst_lengths, sid=speaker_id, noise_scale=noise_scale, noise_scale_w=noise_scale_w, length_scale=length_scale)[0][0, 0].data.cpu().float().numpy() return 22050, audio if __name__ == \u0026#34;__main__\u0026#34;: speakers = { \u0026#34;特别周\u0026#34;: 0, \u0026#34;无声铃鹿\u0026#34;: 1, \u0026#34;东海帝王\u0026#34;: 2, \u0026#34;丸善斯基\u0026#34;: 3, \u0026#34;富士奇迹\u0026#34;: 4, \u0026#34;小栗帽\u0026#34;: 5, \u0026#34;黄金船\u0026#34;: 6, \u0026#34;伏特加\u0026#34;: 7, \u0026#34;大和赤骥\u0026#34;: 8, \u0026#34;大树快车\u0026#34;: 9, \u0026#34;草上飞\u0026#34;: 10, \u0026#34;菱亚马逊\u0026#34;: 11, \u0026#34;目白麦昆\u0026#34;: 12, \u0026#34;神鹰\u0026#34;: 13, \u0026#34;好歌剧\u0026#34;: 14, \u0026#34;成田白仁\u0026#34;: 15, \u0026#34;鲁道夫象征\u0026#34;: 16, \u0026#34;气槽\u0026#34;: 17, \u0026#34;爱丽数码\u0026#34;: 18, \u0026#34;青云天空\u0026#34;: 19, \u0026#34;玉藻十字\u0026#34;: 20, \u0026#34;美妙姿势\u0026#34;: 21, \u0026#34;琵琶晨光\u0026#34;: 22, \u0026#34;重炮\u0026#34;: 23, \u0026#34;曼城茶座\u0026#34;: 24, \u0026#34;美普波旁\u0026#34;: 25, \u0026#34;目白雷恩\u0026#34;: 26, \u0026#34;雪之美人\u0026#34;: 28, \u0026#34;米浴\u0026#34;: 29, \u0026#34;艾尼斯风神\u0026#34;: 30, \u0026#34;爱丽速子\u0026#34;: 31, \u0026#34;爱慕织姬\u0026#34;: 32, \u0026#34;稻荷一\u0026#34;: 33, \u0026#34;胜利奖券\u0026#34;: 34, \u0026#34;空中神宫\u0026#34;: 35, \u0026#34;荣进闪耀\u0026#34;: 36, \u0026#34;真机伶\u0026#34;: 37, \u0026#34;川上公主\u0026#34;: 38, \u0026#34;黄金城市\u0026#34;: 39, \u0026#34;樱花进王\u0026#34;: 40, \u0026#34;采珠\u0026#34;: 41, \u0026#34;新光风\u0026#34;: 42, \u0026#34;东商变革\u0026#34;: 43, \u0026#34;超级小溪\u0026#34;: 44, \u0026#34;醒目飞鹰\u0026#34;: 45, \u0026#34;荒漠英雄\u0026#34;: 46, \u0026#34;东瀛佐敦\u0026#34;: 47, \u0026#34;中山庆典\u0026#34;: 48, \u0026#34;成田大进\u0026#34;: 49, \u0026#34;西野花\u0026#34;: 50, \u0026#34;春乌拉拉\u0026#34;: 51, \u0026#34;青竹回忆\u0026#34;: 52, \u0026#34;待兼福来\u0026#34;: 55, \u0026#34;名将怒涛\u0026#34;: 57, \u0026#34;目白多伯\u0026#34;: 58, \u0026#34;优秀素质\u0026#34;: 59, \u0026#34;帝王光环\u0026#34;: 60, \u0026#34;待兼诗歌剧\u0026#34;: 61, \u0026#34;生野狄杜斯\u0026#34;: 62, \u0026#34;目白善信\u0026#34;: 63, \u0026#34;大拓太阳神\u0026#34;: 64, \u0026#34;双涡轮\u0026#34;: 65, \u0026#34;里见光钻\u0026#34;: 66, \u0026#34;北部玄驹\u0026#34;: 67, \u0026#34;樱花千代王\u0026#34;: 68, \u0026#34;天狼星象征\u0026#34;: 69, \u0026#34;目白阿尔丹\u0026#34;: 70, \u0026#34;八重无敌\u0026#34;: 71, \u0026#34;鹤丸刚志\u0026#34;: 72, \u0026#34;目白光明\u0026#34;: 73, \u0026#34;樱花桂冠\u0026#34;: 74, \u0026#34;成田路\u0026#34;: 75, \u0026#34;也文摄辉\u0026#34;: 76, \u0026#34;真弓快车\u0026#34;: 80, \u0026#34;骏川手纲\u0026#34;: 81, \u0026#34;小林历奇\u0026#34;: 83, \u0026#34;奇锐骏\u0026#34;: 85, \u0026#34;秋川理事长\u0026#34;: 86, \u0026#34;綾地\u0026#34;: 87, \u0026#34;因幡\u0026#34;: 88, \u0026#34;椎葉\u0026#34;: 89, \u0026#34;仮屋\u0026#34;: 90, \u0026#34;戸隠\u0026#34;: 91, \u0026#34;九条裟罗\u0026#34;: 92, \u0026#34;芭芭拉\u0026#34;: 93, \u0026#34;派蒙\u0026#34;: 94, \u0026#34;荒泷一斗\u0026#34;: 96, \u0026#34;早柚\u0026#34;: 97, \u0026#34;香菱\u0026#34;: 98, \u0026#34;神里绫华\u0026#34;: 99, \u0026#34;重云\u0026#34;: 100, \u0026#34;流浪者\u0026#34;: 102, \u0026#34;优菈\u0026#34;: 103, \u0026#34;凝光\u0026#34;: 105, \u0026#34;钟离\u0026#34;: 106, \u0026#34;雷电将军\u0026#34;: 107, \u0026#34;枫原万叶\u0026#34;: 108, \u0026#34;赛诺\u0026#34;: 109, \u0026#34;诺艾尔\u0026#34;: 112, \u0026#34;八重神子\u0026#34;: 113, \u0026#34;凯亚\u0026#34;: 114, \u0026#34;魈\u0026#34;: 115, \u0026#34;托马\u0026#34;: 116, \u0026#34;可莉\u0026#34;: 117, \u0026#34;迪卢克\u0026#34;: 120, \u0026#34;夜兰\u0026#34;: 121, \u0026#34;鹿野院平藏\u0026#34;: 123, \u0026#34;辛焱\u0026#34;: 124, \u0026#34;丽莎\u0026#34;: 125, \u0026#34;云堇\u0026#34;: 126, \u0026#34;坎蒂丝\u0026#34;: 127, \u0026#34;罗莎莉亚\u0026#34;: 128, \u0026#34;北斗\u0026#34;: 129, \u0026#34;珊瑚宫心海\u0026#34;: 132, \u0026#34;烟绯\u0026#34;: 133, \u0026#34;久岐忍\u0026#34;: 136, \u0026#34;宵宫\u0026#34;: 139, \u0026#34;安柏\u0026#34;: 143, \u0026#34;迪奥娜\u0026#34;: 144, \u0026#34;班尼特\u0026#34;: 146, \u0026#34;雷泽\u0026#34;: 147, \u0026#34;阿贝多\u0026#34;: 151, \u0026#34;温迪\u0026#34;: 152, \u0026#34;空\u0026#34;: 153, \u0026#34;神里绫人\u0026#34;: 154, \u0026#34;琴\u0026#34;: 155, \u0026#34;艾尔海森\u0026#34;: 156, \u0026#34;莫娜\u0026#34;: 157, \u0026#34;妮露\u0026#34;: 159, \u0026#34;胡桃\u0026#34;: 160, \u0026#34;甘雨\u0026#34;: 161, \u0026#34;纳西妲\u0026#34;: 162, \u0026#34;刻晴\u0026#34;: 165, \u0026#34;荧\u0026#34;: 169, \u0026#34;埃洛伊\u0026#34;: 179, \u0026#34;柯莱\u0026#34;: 182, \u0026#34;多莉\u0026#34;: 184, \u0026#34;提纳里\u0026#34;: 186, \u0026#34;砂糖\u0026#34;: 188, \u0026#34;行秋\u0026#34;: 190, \u0026#34;奥兹\u0026#34;: 193, \u0026#34;五郎\u0026#34;: 198, \u0026#34;达达利亚\u0026#34;: 202, \u0026#34;七七\u0026#34;: 207, \u0026#34;申鹤\u0026#34;: 217, \u0026#34;莱依拉\u0026#34;: 228, \u0026#34;菲谢尔\u0026#34;: 230 } speed = 1 output_dir = \u0026#34;output\u0026#34; if not os.path.exists(output_dir): os.makedirs(output_dir) for k in speakers: id = speakers[k] print(\u0026#34;Speaker:\u0026#34;,k) sr, audio = vits(\u0026#39;你好,我是玛丽\u0026#39;, 0, torch.tensor([id]), 0.1, 0.668, 1.0/speed) filename = os.path.join(output_dir, \u0026#34;{}.wav\u0026#34;.format(k)) sf.write(filename, audio, samplerate=sr) 运行即可看到如下生成的效果:\n最后来听下效果:\n八重神子 东海帝王 相对来说,其合成的语音还是比较真实的。但是限于原神角色的问题,如果想更为正式的场景,还是需要自己微调。\n","date":1745629993,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748222582,"objectID":"3deb21ba96b36a9940fb1b0d96a110ec","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-vits-for-speech-synthesis/","publishdate":"2025-04-26T09:13:13+08:00","relpermalink":"/posts/how-to-use-vits-for-speech-synthesis/","section":"posts","summary":"如何使用VITS模型进行中文语音合成","tags":["AI"],"title":"使用VITS进行语音合成","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"前言 如果非要说什么的话,那就是用OCR来识别文件内容其实是把问题复杂化的表现,但是应用场景还是有那么一些,比如古籍的扫描后文字的识别,毕竟人工成本还是比较高的。 先来看个自拍的图片:\n可以很清楚看到其中的图片,下面开始编写相关的代码:\nfrom paddleocr import PPStructure\rtable_engine = PPStructure(show_log=True)\rimg_path=\u0026#34;20250422094717.jpg\u0026#34;\rresult = table_engine(img_path)\rfor line in result:\rif line[\u0026#34;type\u0026#34;] == \u0026#34;Table\u0026#34;:\rhtml = line[\u0026#34;res\u0026#34;][\u0026#34;html\u0026#34;] 其中html变量就是识别出来的HTML代码,其效果如下:\n我们为其添加一个像素的边框后可以看到其内容识别的并不全。预计是没有进行预处理的,导致其版面识别就有问题,自然影响后续内容的识别。\n识别截图 接着来看一张从PDF中的截图,这张图片相对来说比较干净,因为只有黑白两种颜色,是很适合OCR进行处理的。\n其效果如下:\n可以看到其成功将表格识别出来了,另外最后一行中10.76%的值漏掉了点号,因此还需要后处理进行校正。\n","date":1745288214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748007479,"objectID":"ee48e8ddccb20b7e35282bfddb7ad5d5","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-paddleocr-recognition-table/","publishdate":"2025-04-22T10:16:54+08:00","relpermalink":"/posts/how-to-use-paddleocr-recognition-table/","section":"posts","summary":"将介绍如何通过PaddleOCR实现表格识别的需求","tags":["AI"],"title":"使用PaddleOCR进行表格识别","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"最近突发奇想实现一个API功能,就是输入一个歌曲,可以自动下载其相应的歌词。话不多说,干就是了。\n虽然可以借助163MusicLyrics这样的开源工具,但是为了更好的嵌入到程序中,于是只好从头造轮子。\n假设找到了1个网站,它提供了相应的歌词,但是没有提供下载。此时我们可以借助AI来帮助把歌词提取出来。这里以陈奕迅的《孤勇者》作为例子,其界面如下:\n下面我们在AI中输入如下一段话:\n从以下网址中提取出歌曲的歌词,格式为LRC: https://www.gecimao.com/geci/316835.html\r其结果如下:\n可以看到AI工具真的一次不差的帮助我们把其歌词提取出来了,而且还是LRC格式的。此时不得不说AI真的强大。这只是AIGC中的一个小小的应用。基于大模型的AI还是可以理解人类的意思进行相关操作的。\n","date":1741745814,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748008879,"objectID":"0b6deba5b8c6061598b99821cb37214a","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-ai-extract-lyrics/","publishdate":"2025-03-12T10:16:54+08:00","relpermalink":"/posts/how-to-use-ai-extract-lyrics/","section":"posts","summary":"如何使用AI智能地从网页中提取歌词","tags":["AI"],"title":"使用AI提取歌词","type":"posts"}]