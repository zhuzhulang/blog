[{"authors":[],"categories":["AI实战"],"content":"国内的软件环境,实际上PostgreSQL用的企业并不是,但是PostgreSQL提供了一些扩展可以让关系型数据库转换为向量数据库。\n借助PostgreSQL的向量化扩展,其支持:\n提取并搜索最近邻 支持单精度、半精度、二进制和稀疏向量 任何语言的PostgreSQL客户端 支持多种距离度量方式,如L2距离、内积等6种方法 在PostgreSQL中提供了2种扩展:\npg_vector pg_embedding 这2种扩展都提供了对HNSW索引的支持,但是前者还提供了IVFFlat的支持。关于这部分内容实际上是数据挖掘中的内容,这里就不赘述了。更多内容可以参考之前的文章HNSW算法简述。需要注意的是,后者现在处于维护状态,换句话就是作者现在不维护了。\n如果说向量数据库有什么用,实际上跟Dify并没有太大的联系。主要还是RAG的热度让其被大家所熟知,而该领域也只是个细分的领域,可以说是坑还是蛮多的。对于中小企业就不要想了,因为其成本根本不是企业能承受的。因此更多只是停留在demo阶段。\n实际上除了RAG外,向量化还可以用于其他领域,比如听歌识曲这样的app。使用向量数据库可以很轻松解决查询效率慢的问题。\n关于pg_vector扩展的使用可以参考原来项目。需要注意的是,其对PostgreSQL的版本要求是\u0026gt;=13.0。如果你使用的是低版本,就不要考虑了。其安装方法主要有2种:\n通过下载源码并编译 使用docker镜像 而docker镜像可以使用类似如下的方法:\ndocker pull pgvector/pgvector:pg[version] 其中version替换为对应的数值,比如13、17,分别表示PostgreSQL版本13和17的镜像。\n而源码安装的方式如下,首先是Linux和MacOS环境:\ncd /tmp git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git cd pgvector make make install # may need sudo 可以将v0.8.0替换为任何版本,如v0.6.0。 而在Windows上则需要进行额外的配置,需要确保已经安装了Visual Studio 2022:\nset \u0026#34;PGROOT=\\path\\to\\PostgreSQL\\17\u0026#34; cd %TEMP% git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git cd pgvector nmake /F Makefile.win nmake /F Makefile.win install 因此一般测试使用建议通过Docker的方式进行安装。\n安装完成后,就可以创建对应的扩展:\nCREATE EXTENSION vector; 而在创建表时添加对应的向量列,例如:\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3)); 而插入向量则以如下的方式进行添加:\nINSERT INTO items (embedding) VALUES (\u0026#39;[1,2,3]\u0026#39;), (\u0026#39;[4,5,6]\u0026#39;); 之后可以通过L2距离查询其最近邻:\nSELECT * FROM items ORDER BY embedding \u0026lt;-\u0026gt; \u0026#39;[3,1,2]\u0026#39; LIMIT 5; 其支持的距离函数如下:\n\u0026lt;-\u0026gt; - L2距离 \u0026lt;#\u0026gt; - (negative)内积或点积 \u0026lt;=\u0026gt; - 余弦距离 \u0026lt;+\u0026gt; - L1距离 \u0026lt;~\u0026gt; - Hamming距离 (binary vectors) \u0026lt;%\u0026gt; - Jaccard距离 (binary vectors) 如果你觉得对这些概念生疏,建议学习一下数据挖掘或数据科学中的知识。整体来说,难度不是大。\n最后该数据库可以支撑几百万数据量的业务场景,对于更大数据量需要使用其他的向量数据库。因为向量相似度计算是很耗资源的,否则会出现响应极度缓慢的问题。\n参考文章:\nhttps://www.timescale.com/blog/postgresql-as-a-vector-database-using-pgvector\n","date":1750147455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750149653,"objectID":"fbf2b714cd418f0997dfdb089999159e","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-postgresql-as-vector-database/","publishdate":"2025-06-17T16:04:15+08:00","relpermalink":"/posts/how-to-use-postgresql-as-vector-database/","section":"posts","summary":"国内的软件环境,实际上PostgreSQL用的企业并不是,但是PostgreSQL提供了一些扩展可以让关系型数据库转换为向量数据库。\n借助PostgreSQL的向量化扩展,其支持:\n提取并搜索最近邻 支持单精度、半精度、二进制和稀疏向量 任何语言的PostgreSQL客户端 支持多种距离度量方式,如L2距离、内积等6种方法 在PostgreSQL中提供了2种扩展:\npg_vector pg_embedding 这2种扩展都提供了对HNSW索引的支持,但是前者还提供了IVFFlat的支持。关于这部分内容实际上是数据挖掘中的内容,这里就不赘述了。更多内容可以参考之前的文章HNSW算法简述。需要注意的是,后者现在处于维护状态,换句话就是作者现在不维护了。\n如果说向量数据库有什么用,实际上跟Dify并没有太大的联系。主要还是RAG的热度让其被大家所熟知,而该领域也只是个细分的领域,可以说是坑还是蛮多的。对于中小企业就不要想了,因为其成本根本不是企业能承受的。因此更多只是停留在demo阶段。\n实际上除了RAG外,向量化还可以用于其他领域,比如听歌识曲这样的app。使用向量数据库可以很轻松解决查询效率慢的问题。\n关于pg_vector扩展的使用可以参考原来项目。需要注意的是,其对PostgreSQL的版本要求是\u003e=13.0。如果你使用的是低版本,就不要考虑了。其安装方法主要有2种:\n通过下载源码并编译 使用docker镜像 而docker镜像可以使用类似如下的方法:\ndocker pull pgvector/pgvector:pg[version] 其中version替换为对应的数值,比如13、17,分别表示PostgreSQL版本13和17的镜像。\n而源码安装的方式如下,首先是Linux和MacOS环境:\ncd /tmp git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git cd pgvector make make install # may need sudo 可以将v0.8.0替换为任何版本,如v0.6.0。 而在Windows上则需要进行额外的配置,需要确保已经安装了Visual Studio 2022:\nset \"PGROOT=\\path\\to\\PostgreSQL\\17\" cd %TEMP% git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git cd pgvector nmake /F Makefile.win nmake /F Makefile.win install 因此一般测试使用建议通过Docker的方式进行安装。\n安装完成后,就可以创建对应的扩展:\nCREATE EXTENSION vector; 而在创建表时添加对应的向量列,例如:\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3)); 而插入向量则以如下的方式进行添加:\n","tags":["AI"],"title":"PostgreSQL + 向量搜索:解锁关系型数据库的AI潜能","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"之前介绍了如何在Dify中生成图表,而本篇文章的主要目的是新增图表类型,例如新增1个雷达图的类型。\n可以看到在之前Dify二次开发-生成图表中,该插件支持3种图表类型,功能比较基础。现在在其基础上进行迭代和扩展。\n首先其离线包实际上是使用zip格式进行压缩的文件:\n$ zipinfo langgenius-echarts_0.0.1.difypkg Archive: langgenius-echarts_0.0.1.difypkg Zip file size: 8763 bytes, number of entries: 16 -rw---- 2.0 fat 145 bl defN 80-000-00 00:00 .env.example -rw---- 2.0 fat 0 bl defN 80-000-00 00:00 README.md -rw---- 2.0 fat 2719 bl defN 80-000-00 00:00 _assets/icon.svg -rw---- 2.0 fat 148 bl defN 80-000-00 00:00 main.py ... -rw---- 2.0 fat 36 bl defN 80-000-00 00:00 .verification.dify.json -rw---- 2.0 fat 36 bl defN 80-000-00 00:00 .verification.dify.json 16 files, 13711 bytes uncompressed, 6073 bytes compressed: 55.7% 可以修改其目录结构实现功能的迭代。具体代码实现这里不再赘述,之前介绍过相关的逻辑。\n关于其打包的逻辑可以参考。并在.env中配置对应参数:\nFORCE_VERIFYING_SIGNATURE=false 这样Dify就不会校验其签名并可以成功安装了。可以使用如下方式查看容器的环境变量:\nsudo docker inspect --format=\u0026#39;{{range .Config.Env}}{{println .}}{{end}}\u0026#39; [容器ID] 在Windows上其打包工具可以访问dify-plugin-daemon进行下载。整体来说难度并不是很大。\n其最终效果如下:\n参考文章:\nhttps://docs.dify.ai/zh-hans/plugins/quick-start/install-plugins\nhttps://docs.dify.ai/zh-hans/plugins/quick-start/develop-plugins/initialize-development-tools\n","date":1750029624,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750038412,"objectID":"df21836246b48ff268c91968381daec1","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-add-new-chart-in-plugins/","publishdate":"2025-06-16T07:20:24+08:00","relpermalink":"/posts/how-to-add-new-chart-in-plugins/","section":"posts","summary":"如何根据实际需求新增图表","tags":["AI"],"title":"Dify二次开发-新增图表类型","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"这篇文章实际上是在之前文章Dify二次开发-使用MCP协议查询数据库数据基础上进行的技术迭代。 通过MCP协议获取到数据后,我们需要将数据转换为对应图表的格式从而让其在Dify中显示出来。这里选择是Echarts的插件,该插件接收3个参数:\ntitle: 标题 data: 数据 x_axis: x轴 但是该插件支持3种图表类型:\n线性图表 柱状图 饼图 可以看到功能比较基础,如果有额外的需求还需要进行定制化开发,难度并不是很大。后续有时间再介绍下如何定制进行开发。 最终其整个流程图如下:\n我们可以输入如下的内容让LLM生成对应的图表:\n获取2023年每个月漏洞的数量,返回内容是月份及数量,格式为JSON数据,如\u0026#39;[{\u0026#34;month\u0026#34;:\u0026#34;1月\u0026#34;,\u0026#34;num\u0026#34;:100},...]\u0026#39; 当开始时需要输入最终图表的标题,之后经过MCP服务器获取对应的数据,紧接着对数据进行转换,将转换后的输出输入到线形图中,最终进行输出。可以看到流程还是很清楚的,下面是其最终效果:\n不得不说MCP还是蛮方便的,稍加修改即可作为真实项目了。\n","date":1749971353,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750056253,"objectID":"7651ac98999017a6ec346f8f93110f2a","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-dify-generate-graph/","publishdate":"2025-06-15T15:09:13+08:00","relpermalink":"/posts/how-to-use-dify-generate-graph/","section":"posts","summary":"如何在Dify中根据数据库中的数据生成图表","tags":["AI"],"title":"Dify二次开发-生成图表","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"这应该是Dify二次开发最后一篇文章了,主要是觉得没什么可以写的了。而且Dify调试起来很不方便,不想花费太多心思在这上面。而且要通过该技能找到工作还是蛮难的,现在的企业都想找熟手的,问题哪里可以快速培养出这么多的人来。\n说是月薪15K,但很多企业连10K都给不到,实际上Dify开发只要1-3年Python开发经验即可。一边是大模型无所不能的需求,一边是薪资对不起就业的环境。最近才遇到一个企业的领导,觉得我Dify不是很深入。这平台出来3年都不好,你要资深不得一出来就开始玩。既要你懂还要经验丰富,薪资就15-30K。后面那个数字基本不用看了,结果还是一时兴起想找个人做这个项目,但又不确定能让你干多久,还要求会Java。敢问会Java能进行Dify二次开发?更有意思的是说他也是搞技术的,哈哈。\n这里Dify版本必须大于1.0.0才能使用其插件功能,这里使用的版本是1.0.1。首先是对应插件的安装:\n接着是MCP服务器代码的编写,这里使用的是FastMCP库,可以直接使用pip进行安装。但是需要确保Python的版本大于3.10。其对应的代码如下:\nimport psycopg2 from fastmcp import FastMCP mcp = FastMCP(\u0026#34;postgresql-mcp\u0026#34;,port=9000) conn_params = { \u0026#34;dbname\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, # 默认本地 \u0026#34;port\u0026#34;: \u0026#34;5432\u0026#34; # PostgreSQL默认端口 } @mcp.tool() def execute_sql(query:str) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34; 执行SQL查询语句 参数: query (str): 要执行的SQL语句，支持多条语句以分号分隔 返回: list: 包含查询结果的TextContent列表 - 对于SELECT查询：返回CSV格式的结果，包含列名和数据 - 对于SHOW TABLES：返回数据库中的所有表名 - 对于其他查询：返回执行状态和影响行数 - 多条语句的结果以\u0026#34;---\u0026#34;分隔 \u0026#34;\u0026#34;\u0026#34; try: with psycopg2.connect(**conn_params) as conn: with conn.cursor() as cursor: statements = [stmt.strip() for stmt in query.split(\u0026#34;;\u0026#34;) if stmt.strip()] results = [] for statement in statements: try: print(\u0026#34;SQL:{}\u0026#34;.format(statement)) cursor.execute(statement) # 检查语句是否返回了结果集 (SELECT, SHOW, EXPLAIN, etc.) if cursor.description: columns = [desc[0] for desc in cursor.description] rows = cursor.fetchall() # 将每一行的数据转换为字符串，特殊处理None值 formatted_rows = [] for row in rows: formatted_row = [ \u0026#34;NULL\u0026#34; if value is None else str(value) for value in row ] formatted_rows.append(\u0026#34;,\u0026#34;.join(formatted_row)) # 将列名和数据合并为CSV格式 results.append( \u0026#34;\\n\u0026#34;.join([\u0026#34;,\u0026#34;.join(columns)] + formatted_rows) ) # 如果语句没有返回结果集 (INSERT, UPDATE, DELETE, etc.) else: conn.commit() # 只有在非查询语句时才提交 results.append(f\u0026#34;查询执行成功。影响行数: {cursor.rowcount}\u0026#34;) except Error as stmt_error: # 单条语句执行出错时，记录错误并继续执行 results.append( f\u0026#34;执行语句 \u0026#39;{statement}\u0026#39; 出错: {str(stmt_error)}\u0026#34; ) # 可以在这里选择是否继续执行后续语句，目前是继续 return [\u0026#34;\\n---\\n\u0026#34;.join(results)] except Exception as e: print(f\u0026#34;Error for \u0026#39;{query}\u0026#39;: {e}\u0026#34;) return [f\u0026#34;Error for: {str(e)}\u0026#34;] if __name__ == \u0026#39;__main__\u0026#39;: mcp.run(transport=\u0026#34;sse\u0026#34;) 之后是对MCP SSE插件进行授权,其内容类似如下:\n{ \u0026#34;postgresql-mcp\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://127.0.0.1:9000/sse\u0026#34;, \u0026#34;headers\u0026#34;: {}, \u0026#34;timeout\u0026#34;: 500, \u0026#34;sse_read_timeout\u0026#34;: 50 }} 可以根据实际的IP进行修改。需要注意的是其2个插件版本要求必须是0.06才能正常调用。 最后就可以创建应用了,选择Chatflow即可。按照要求填写相关的内容。其中MCP服务器地址为:\n{ \u0026#34;postgresql-mcp\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://127.0.0.1:9000/sse\u0026#34;, \u0026#34;headers\u0026#34;: {}, \u0026#34;timeout\u0026#34;: 150, \u0026#34;sse_read_timeout\u0026#34;: 50 }} 这里使用MCP对PostgreSQL中的数据进行查询,只涉及单表数据的查询,因此效果会很好。 而指令中内容如下:\n使用中文回复。 当用户提问中涉及漏洞实体时,需要使用postgresql-mcp进行数据查询和操作,表结构说明如下: ## 漏洞信息表(vulnerability_information) | 字段名 | 类型 | 描述 | |--------------|------------------------|---------| | id | integer | ID | | bug_name | character varying(200) | 漏洞名称 | | cnnvd_id | character varying(20) | CNNVD号 | | publish_date | date | 发布时间 | | bug_desc | text | 漏洞描述 | | cve_id | character varying(20) | CVE号 | | severity | character varying(2) | 风险级别 | 其最终效果如下:\n可以看到输入内容后,对应的结果就开始不断输出了。而在MCP服务器这边可以看到如下的输出:\nSQL:SELECT * FROM vulnerability_information WHERE publish_date \u0026gt;= \u0026#39;2025-01-01\u0026#39; AND publish_date \u0026lt; \u0026#39;2026-01-01\u0026#39; LIMIT 10 可以清楚看到大模型很好的理解其需求,并输出了正确的SQL语句并执行。而对于多表复杂的条件,其效果则会大打折扣。因此实际商用还面临很多挑战急需解决。\n参考文章:\nhttps://www.cnblogs.com/xiao987334176/p/18827261\nhttps://www.cnblogs.com/xiao987334176/p/18822444\n","date":1749817650,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1749826631,"objectID":"53866d24a6a630ad11a61511c8995893","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-mcp-protocol-query-data/","publishdate":"2025-06-13T20:27:30+08:00","relpermalink":"/posts/how-to-use-mcp-protocol-query-data/","section":"posts","summary":"如何通过MCP协议对数据库中数据进行查询","tags":["AI"],"title":"Dify二次开发-使用MCP协议查询数据库数据","type":"posts"},{"authors":[],"categories":["杂谈"],"content":"实际上arXiv平台上论文只是初稿,并没有经过同行的评审,因此其可信度需要结合实验复现、论文引用量及作者团队背景综合判断其可信度。不过这种深度一般需要研究生学历才会接触到,但实际上本科生稍加学习也是可以应对的。\n一般来说,这些论文单词量并不会很大,CET-4基本可以应付。重要的是信心,不要胆怯,而且论文中很多想法实际上你也是可以想到的。在阅读论文时,要带着问题,即本次你想从这篇论文中学习到什么。比如想看下它说的是什么,或者想看下它的网络结构是怎样设计的。每次阅读一部分,逐次蚕食吸收,自然有所成就。 一般而言,每篇论文开头都会介绍相关的背景,比如之前有什么类似的技术,而这些技术得到了怎样的效果或有什么缺点。之后就开始进入自己内容的介绍,介绍的想法、网络结构,有什么优势,解决怎样的问题。最后自然是使用的数据集、训练参数、训练结果的展现以及细节的叙述。就比如这段内容:\nTTS is a typical one-to-many mapping problem, since multiple possible speech sequences can correspond to a text sequence due to different variations in speech audio, such as pitch, duration,sound volume and prosody. In autoregressive TTS, the decoder can condition on the text sequence and the previous mel-spectrograms to predict next mel-spectrograms, where the previous mel-spectrograms can provide some variation information and thus alleviate this problem to a certain degree. While in non-autoregressive TTS, the only input information is text which is not enough to fully predict the variance in speech. In this case, the model is prone to overfit to the variations of the target speech in the training set, resulting in poor generalization ability. 自己的翻译是:\nTTS是典型的一对多映射问题,因为一个文本序列由于不同变化(音高、时长、音量和韵律)可以对应多种可能语音序列。在自回归TTS中,解码器可以以文本序列作为条件,通过之前的梅尔频谱预测下一个梅尔频谱,其中之前的梅尔频谱可以提供一些变体信息从而在一定程序上缓解这一问题。而在非自回归TTS中,输入的信息只有文本,它是不足以完全预测语音中的变化。在这种情况下,模型容易拟合训练集目标语音的变化,结构泛化较差。 而DeepSeek的翻译如下:\n文本到语音（TTS）是一个典型的一对多映射问题。由于语音音频中存在音高、时长、音量和韵律等多种可变因素，同一文本序列可能对应多个不同的语音序列。在自回归TTS模型中，解码器可以通过文本序列和前一时间步的梅尔频谱图来预测后续频谱，其中历史梅尔频谱提供了部分变化信息，从而在一定程度上缓解了这一问题。而在非自回归TTS模型中，仅有文本输入信息不足以完整预测语音的所有动态变化。这种情况下，模型容易过度拟合训练数据中的语音变化特征，导致泛化能力显著下降。 阅读论文没必要做到100%都翻译正确,掌握大概的意思即可。毕竟语言之间的鸿沟,在深度神经网络盛行的今天,已经区别不是那么大了。如果可以话,可以借助一些翻译工具。\n因此有一些插件可以同步翻译,如沉浸式翻译及有道翻译。\n这里以FastSpeech作为例子进行说明,其paper分别为:\nFastSpeech: Fast, Robust and Controllable Text to Speech FastSpeech 2: Fast and High-Quality End-to-End Text to Speech 首先从其编号1905和2006可以看出这两篇文章分别发布在19年5月和20年6月。从其标题可以看出,FastSpeech是一种快速、鲁棒和可控的文本到语音合成。而FastSpeech2是快速高质量端到端文本到语音合成。\n其中FastSpeech有5个版本,而FastSpeech2有8个版本,一般我们选择最新版本进行阅读即可。在这里主要是为了获取其网络结构,然后编写代码进行实现。\n首先对比其网络结构,下面是FastSpeech的结构:\n之后是FastSpeech2的结构:\n在FastSpeech中可以看到音素(Phoneme)经过音素嵌入层,再经过位置编码进入N层的前馈变换器块。之后经过长度调节器(Length Regulator)后,与位置编码一起再经过N层的前馈变换器块(FFT Block)后,经过线性层输出梅尔频谱图。\n而FastSpeech2中的变化是将第1个FFT Block修改为编码器(Encoder),经过方差适配器(Variance Adaptor),叠加位置编码后输出到梅尔频谱解码器(Mel-spectrogram Decoder)和声波解码器(Waveform Decoder)中。\n对于代码的复现,可以参考Paperswithcode,我们可以搜索相关的paper,查看其是否有一些代码、数据集及结果的测评。\n从上图可以看到,FastSpeech有多个实现,根据框架不同选择对应的实现。而数据集只有LJSpeech。\n在FastSpeech2的实现ming024/FastSpeech2中有如下的代码:\nclass FastSpeech2(nn.Module): \u0026#34;\u0026#34;\u0026#34; FastSpeech2 \u0026#34;\u0026#34;\u0026#34; def __init__(self, preprocess_config, model_config): super(FastSpeech2, self).__init__() self.model_config = model_config self.encoder = Encoder(model_config) self.variance_adaptor = VarianceAdaptor(preprocess_config, model_config) self.decoder = Decoder(model_config) self.mel_linear = nn.Linear( model_config[\u0026#34;transformer\u0026#34;][\u0026#34;decoder_hidden\u0026#34;], preprocess_config[\u0026#34;preprocessing\u0026#34;][\u0026#34;mel\u0026#34;][\u0026#34;n_mel_channels\u0026#34;], ) self.postnet = PostNet() 正好对应之前的网络结构。\n总体而言,对于论文的阅读,需要对基础有一定的了解,能够举一反三。论文中可能会提出一些很新颖的名词,而实际上就是你熟悉的某些方法。不要被他吓到了。\n","date":1748232545,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748245882,"objectID":"eb9768461150a1c40e26c58556d5d619","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-read-arxiv-papers/","publishdate":"2025-05-26T12:09:05+08:00","relpermalink":"/posts/how-to-read-arxiv-papers/","section":"posts","summary":"介绍一些阅读arXiv中paper的方法","tags":["other"],"title":"如何阅读arXiv的初稿","type":"posts"},{"authors":null,"categories":["AI实战"],"content":"前言 之前介绍了如何新增模型提供商,下面继续之前的内容,介绍如何在Dify中新增内置工具。按照之前的做法,本次新增一个Doga Speech的小工具,用于将输入的文本转换为语音输出。\n开始操作 首先在dify/api/core/tools/provider/builtin目录下新增1个doga的包:\nmkdir dify/api/core/tools/provider/builtin/doga 其中配置doga.yaml中内容如下:\nidentity: author: demo name: doga label: en_US: Doga Speech zh_Hans: Doga Speech description: en_US: a tool for speech synthesis zh_Hans: 语音合成小工具 icon: doga_speech.png tags: - utilities 如果之前看过新增模型提供商文章的小伙伴,应该对这段代码感觉很熟悉。分别是说明工具的作者、显示的标签、文本说明、图标及类型。 在doga.py模块中实现模型提供商相关认证代码,由于不需要认证,因此其代码如下:\nfrom typing import Any from core.tools.provider.builtin_tool_provider import BuiltinToolProviderController class DogaProvider(BuiltinToolProviderController): def _validate_credentials(self, credentials: dict[str, Any]): pass 之后创建1个tools的子包用于表示该工具有哪些功能。由于只有简单的TTS功能,因此目录下doga_speech.yaml的内容如下:\nidentity: name: doga_speech author: doga label: en_US: Doga Speech zh_Hans: Doga语音 description: human: en_US: A tool for speech synthesis zh_Hans: 用于语音合成的工具 llm: A tool for speech synthesis parameters: - name: input_text type: string required: true label: en_US: input text zh_Hans: 文本输入 human_description: en_US: input text for speech synthesis zh_Hans: 用于语音合成的输入文本 form: form 其中要求用户输入1个文本字段,接收到该字段后在doga_speech.py模块中进行相应的处理并返回对应的内容。\nfrom core.tools.tool.builtin_tool import BuiltinTool from core.tools.entities.tool_entities import ToolInvokeMessage from typing import Any, Dict, List, Union class DogaSpeechTool(BuiltinTool): def _invoke(self, tool_Parameters: Dict[str, Any]): input_text = tool_Parameters[\u0026#34;input_text\u0026#34;] if input_text: return self.create_text_message(\u0026#34;Audio generated successfully\u0026#34;) return self.create_text_message(\u0026#34;Audio generated failed\u0026#34;), 编写完上述代码后,在Dify界面中可以搜索到如下的工具:\n之后创建1个Agent,其对应的工具添加如下:\n更多内容可以查看官方文档,本任务相对来说比较基础和简单。\n参考文章:\nhttps://docs.dify.ai/zh-hans/guides/tools/quick-tool-integration\n","date":1748098527,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748221550,"objectID":"f246157b0f8db2699b55b23da6e59b53","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-add-custom-tool/","publishdate":"2025-05-24T22:55:27+08:00","relpermalink":"/posts/how-to-add-custom-tool/","section":"posts","summary":"在Dify上新增内置工具","tags":["AI"],"title":"Dify二次开发-新增工具","type":"posts"},{"authors":null,"categories":null,"content":"1. 致谢 感谢金主爸爸们的投喂！本咸鱼写博客纯属用爱发电，没想到居然真能换到茶叶蛋钱，这下更有动力摸鱼更新了（老板：？）\n2. 打赏名单 以下是打赏名单列表:\n打赏时间 打赏者 打赏方式 打赏金额 2025.3.21 **文 支付宝 ￥0.1 2025.5.20 *民 微信 ￥1 ","date":1748046214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748049546,"objectID":"13e23b0fba5009f20ac372d2c13bc1da","permalink":"https://zhuzhulang.github.io/blog/reward/","publishdate":"2025-05-24T08:23:34+08:00","relpermalink":"/reward/","section":"","summary":"1. 致谢 感谢金主爸爸们的投喂！本咸鱼写博客纯属用爱发电，没想到居然真能换到茶叶蛋钱，这下更有动力摸鱼更新了（老板：？）\n2. 打赏名单 以下是打赏名单列表:\n打赏时间 打赏者 打赏方式 打赏金额 2025.3.21 **文 支付宝 ￥0.1 2025.5.20 *民 微信 ￥1 ","tags":null,"title":"感谢网友打赏","type":"page"},{"authors":[],"categories":["AI实战"],"content":"没想到介绍Dify二次开发第4篇文章竟然是重置登录邮箱的,这个实在太简单了,可以说毫无难度,只是官方文档里面没有写。至于第3篇还在修改中,预计很快就能上线了。 在官方文档中介绍了如何重置管理员密码,详情可以参考,使用的方式是:\ndocker exec -it docker-api-1 flask reset-password\r如果想重置管理员密码呢?此时有两种方式,一种是直接进入数据库修改密码,其中的数据库是dify:\n但是这种方式很容易不小心就把数据库给搞坏了。 下面推荐一种更为靠谱的方式,就是使用官方提供的命令:\ndocker exec -it docker_api_1 flask reset-email\r然后就会在终端中出现对应的信息:\n可以看到最终更新邮箱成功了。 这个功能看似没用,实际上在二次开发中特别是权限管理时还有有些用处的。比如添加新的账户的场景,这里就不赘述了,操作难度不大。\n","date":1747898214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748221619,"objectID":"f51e174c8852546015273449badae4d1","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-reset-email-in-dify/","publishdate":"2025-05-22T15:16:54+08:00","relpermalink":"/posts/how-to-reset-email-in-dify/","section":"posts","summary":"如何重置Dify的管理员邮箱","tags":["AI"],"title":"Dify二次开发-重置管理员邮箱","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"前言 之前介绍了如何搭建Dify的环境,可以说是非常的简单,没什么难度。下面开始正式进行Dify的二次开发,预计要分多篇文章来叙述了。 整体来说,二次Dify开发难度并不大,可能会有一些坑,但是对于经验丰富的我来说都不成问题。不知道谁说Dify是高级Python工程师应有的水平,我只能呵呵了。 废话不多说,这里先实现个小小的目标,自定义模型服务商吧。首先提前申明下下面的代码是基于0.15.3的。 在官方文档中有如下这么一个提示:\n相关内容可以参考。可以看到,如果要使用插件功能,只能升级到1.0.0版本。而且Dify这个项目更新很快,换句话说就是很不稳定。\n新增模型提供商 下面开始正式的操作。先创建1个doga的包,主要是纪念如下的人物:\n这里没什么恶意,仅仅是学习而已。整个包的目录结构如下:\ndoga ├── _assets │ ├── icon_l_en.png │ └── icon_s_en.png ├── doga.py ├── doga.yaml ├── __init__.py └── llm ├── doga-1.0.yaml ├── __init__.py └── llm.py 其中_assets目录用于存储logo,而dogma.yaml是配置文件。 在doga.yaml中先定义如下的内容:\nprovider: doga\rlabel:\ren_US: Doga\rzh_Hans: 卡波苏\rdescription:\rzh_Hans: 卡波苏模型\ren_US: doga model\ricon_small:\ren_US: icon_s_en.png\rzh_Hans: icon_s_en.png\ricon_large:\ren_US: icon_l_en.png\rzh_Hans: icon_l_en.png\rbackground: \u0026#34;#93c5fd\u0026#34;\rsupported_model_types:\r- llm\rconfigurate_methods:\r- predefined-model\rprovider_credential_schema:\rcredential_form_schemas:\r- variable: doga_api_key\rlabel:\ren_US: API Key\rtype: secret-input\rrequired: true\rplaceholder:\rzh_Hans: 请在此输入您的API Key\ren_US: Please enter your API Key\r- variable: doga_endpoint_url\rlabel:\rzh_Hans: 自定义API endpoint地址\ren_US: Custom API endpoint URL\rtype: text-input\rrequired: false\rplaceholder:\rzh_Hans: Base URL, e.g. https://api.example.com/v1\ren_US: Base URL, e.g. https://api.example.com/v1 相关的说明可以查看参考文章中的链接。由于该模态只支持LLM对话,因此只有llm子包,其中的llm.py中需要实现一个继承自LargeLanguageModel的自定义类,该类需要实现如下一些方法:\n_invoke,模型运行调用 get_num_tokens,预计算输入 tokens validate_credentials,模型凭据校验 _invoke_error_mapping,调用异常错误映射表 相关的代码这里就赘述了,完整的代码可以参考。\n最终效果 最后是更新后的效果,成功出现了自己定义的模型提供商:\n配置出现如下的页面:\n输入对应的API key后,我们创建一个空白的应用进行测试:\n选择新增的模型提供商,再输入相应的内容可以看到如下的结果:\n可以说,整个过程还是很简单的,只需要按照说明进行操作即可。\n参考文章:\nhttps://docs.dify.ai/zh-hans/guides/model-configuration/new-provider\nhttps://docs.dify.ai/zh-hans/guides/model-configuration/predefined-model\n","date":1747725414,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748221500,"objectID":"0869d5f0e50d852d604cf45c22a08f95","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-add-custom-providers/","publishdate":"2025-05-20T15:16:54+08:00","relpermalink":"/posts/how-to-add-custom-providers/","section":"posts","summary":"如何在Dify上新增模型提供商","tags":["AI"],"title":"Dify二次开发-新增模型提供商","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"\n搭建前准备工序 其实Dify是个传统的东西,只是搭上了AI的快车。下面简单说下其搭建过程。 在开始之前,先看下自己服务器的配置是否满足如下的条件:\n硬件方面必须是双核4G内存,这很容易满足。磁盘大约需要占用15G,而软件方面Docker版本必须大于19.03且Docker Compose需要大于1.28。因为Docker版本18.x存在线程创建限制。 不妨这么进行查看:\n~$ docker --version Docker version 19.03.15, build 99e3ed8919 ~$ docker-compose --version docker-compose version 1.29.0, build 07737305 相比几十M的版本2,更喜欢只有10来M的版本1,这里直接使用如下方式下载Docker-Compose:\nwget http://github.com/docker/compose/releases/download/1.29.0/docker-compose-linux-x86_64 chmod +x docker-compose-linux-x86_64 mv docker-compose-linux-x86_64 /usr/bin/docker-compose 开始搭建环境 一切准备就绪,开始拉取代码:\n# 假设当前最新版本为 0.15.3 git clone https://github.com/langgenius/dify.git --branch 0.15.3 进入源代码的docker目录并开始拉取并打包:\ncd dify/docker cp .env.example .env sudo docker-compose up -d 一堆操作猛如虎,难就难在国内Docker-Hub访问不了。按照下面方式修改镜像源:\nsudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;:[ \u0026#34;https://docker.1ms.run\u0026#34;, \u0026#34;https://docker.xuanyuan.me\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.m.daocloud.io\u0026#34; ] } EOF sudo systemctl daemon-reload \u0026amp;\u0026amp; sudo systemctl restart docker 重新拉取,经过1个多小时后,如果网速不怎么好的情况下,最终安装完成的截图:\n别忘了初始化管理员账户即可使用了:\n# 本地环境 http://localhost/install # 服务器环境 http://your_server_ip/install 如下图所示:\n访问Dify的URL即可使用:\n# 本地环境 http://localhost # 服务器环境 http://your_server_ip 探索页面如下:\n效果如下:\n常见问题 如果使用docker-compose成功拉取镜像但是出不来管理员账户页面且出现502问题,大概率是容器没有创建线程的权限,修改docker-compose.yaml中api部分的内容:\napi: image: langgenius/dify-api:0.15.3 privileged: true cap_add: - SYS_ADMIN ulimits: nproc: 65535 security_opt: - seccomp:unconfined 可以看到docker容器的日志是否出现如下的异常:\nOpenBLAS blas_thread_init: pthread_create failed for thread 7 of 32: Operation not permitted OpenBLAS blas_thread_init: RLIMIT_NPROC -1 current, -1 max 参考文章:\nhttps://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose\n","date":1747466214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748221610,"objectID":"194a43e3487b02682c72ef6c6b74ab4c","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-build-dify-environment/","publishdate":"2025-05-17T15:16:54+08:00","relpermalink":"/posts/how-to-build-dify-environment/","section":"posts","summary":"如何搭建Dify环境","tags":["AI"],"title":"Dify二次开发-环境搭建","type":"posts"},{"authors":["码力全开"],"categories":["TTS"],"content":"这里介绍下如何通过FastSpeech2项目进行中文的语音合成,由于该仓库把相关demo数据也提交了,因此仓库比较大。此时可以使用sparse-checkout拉取相关的代码,而忽略一些无关紧要的目录。\n要确保git的版本大于2.25,可以使用git version进行查看。\n接着就开始代码的拉取:\ngit clone --filter=blob:none --depth=1 --no-checkout https://github.com/ming024/FastSpeech2 cd FastSpeech2 git sparse-checkout init --cone 接着修改.git/info/sparse-checkout中的内容为:\n/* !/demo/ /preprocessed_data/*/*.json 最后进行git checkout即可。\n之后就是对应模型的下载了,可以访问FastSpeech2进行对应模型的下载,密码是9615。其中AISHELL-3是中文多人,LJSpeech是英文单人,LibriTTS是英文多人。\n将下载的模型放置在目录output/ckpt对应目录下,也不知道为何要搞这么复杂。只能手动创建对应的目录:\nmkdir -p output/ckpt/{AISHELL3,LJSpeech,LibriTTS} mkdir -p output/result/{AISHELL3,LJSpeech,LibriTTS} 需要注意的是,放置在目录中的文件必须是pth.tar后缀结尾,且需要去掉之前的语料库名称只保留数字即可,比如LJSpeech_900000.zip需要变为900000.pth.tar。\n一切准备就绪,就可以开始推理了。这里使用GPU进行推理:\n$ python synthesize.py --text \u0026#34;哈哈,我是个传说的人物\u0026#34; --speaker_id 0 --restore_step 600000 --mode single -p config/AISHE LL3/preprocess.yaml -m config/AISHELL3/model.yaml -t config/AISHELL3/train.yaml 最终的效果如下:\n可以听见机械音还是有点重的。\n当然也可以参考之前写的另一篇使用原神中人物的语音合成,相对来说效果会更好一些。详情请点击。\n参考文章:\nhttps://youwu.today/blog/git-sparse-checkout-for-partial-repository-clone/\n","date":1745888507,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748222758,"objectID":"474aa3d3b2b58bcd3cc7a5d34d88fb00","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-fastspeech2-for-speech-synthesis/","publishdate":"2025-04-29T09:01:47+08:00","relpermalink":"/posts/how-to-use-fastspeech2-for-speech-synthesis/","section":"posts","summary":"如何使用FastSpeech2的模型实现中文语音合成","tags":["AI"],"title":"使用FastSpeech2进行语音合成","type":"posts"},{"authors":[],"categories":["TTS"],"content":"VITS是什么? VITS(Variational Inference with adversarial learning for end-to-end Text-to-Speech)是一种基于深度学习的端到端语音合成技术。相较于传统TTS系统，它无需复杂的声学模型和声码器，直接通过文本生成自然流畅的语音。\n其核心在于：\n变分自编码器(VAE)结构 对抗训练机制 蒙特卡洛flow-based时长预测 其内部由3个模型组成,分别是GAN、VAE和Flow。\n安装必备工具 下面先安装相应的工具:\n# 基础环境 conda create -n vits python=3.8 conda activate vits pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0 -f https://download.pytorch.org/whl/cu113/torch_stable.html # 核心依赖 pip install numpy scipy matplotlib pandas pip install tensorboard librosa unidecode inflect 获取代码仓库 这里没有选择官方的仓库,而是选择如下微调的仓库代码,因为其支持中文模型:\ngit clone https://github.com/Plachtaa/VITS-fast-fine-tuning.git pip install -r requirements.txt 下载原神角色模型 访问【VITS-genshin】下载相关的模型,密码是8243。将整个目录下载下来放在源码目录下。\n合成你的第一段语音 在项目仓库目录下新建1个脚本,其中的代码如下\nimport os import torch import utils import commons import logging import soundfile as sf from text import text_to_sequence from models import SynthesizerTrn from torch import no_grad, LongTensor logger = logging.getLogger(\u0026#34;jieba\u0026#34;) logger.setLevel(logging.WARNING) limitation = False hps_ms = utils.get_hparams_from_file(\u0026#39;./configs/uma_trilingual.json\u0026#39;) device = torch.device(\u0026#34;cpu\u0026#34;) net_g_ms = SynthesizerTrn( len(hps_ms.symbols), hps_ms.data.filter_length // 2 + 1, hps_ms.train.segment_size // hps_ms.data.hop_length, n_speakers=hps_ms.data.n_speakers, **hps_ms.model) _ = net_g_ms.eval().to(device) model, optimizer, learning_rate, epochs = utils.load_checkpoint(\u0026#34;./pretrained_models/G_trilingual.pth\u0026#34;, net_g_ms, None) def get_text(text, hps): text_norm = text_to_sequence(text, hps.symbols, hps.data.text_cleaners) if hps.data.add_blank: text_norm = commons.intersperse(text_norm, 0) text_norm = LongTensor(text_norm) return text_norm def vits(text, language, speaker_id, noise_scale, noise_scale_w, length_scale): if not len(text): return \u0026#34;输入文本不能为空！\u0026#34;, None, None text = text.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;).replace(\u0026#39;\\r\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;) length = len(text) if length \u0026gt; 100: return \u0026#34;输入文字过长!\u0026#34;, None, None if language == 0: text = \u0026#34;[ZH]{}[ZH]\u0026#34;.format(text) elif language == 1: text = \u0026#34;[JA]{}[JA]\u0026#34;.format(text) stn_tst = get_text(text, hps_ms) with no_grad(): x_tst = stn_tst.unsqueeze(0).to(device) x_tst_lengths = LongTensor([stn_tst.size(0)]).to(device) speaker_id = LongTensor([speaker_id]).to(device) audio = net_g_ms.infer(x_tst, x_tst_lengths, sid=speaker_id, noise_scale=noise_scale, noise_scale_w=noise_scale_w, length_scale=length_scale)[0][0, 0].data.cpu().float().numpy() return 22050, audio if __name__ == \u0026#34;__main__\u0026#34;: speakers = { \u0026#34;特别周\u0026#34;: 0, \u0026#34;无声铃鹿\u0026#34;: 1, \u0026#34;东海帝王\u0026#34;: 2, \u0026#34;丸善斯基\u0026#34;: 3, \u0026#34;富士奇迹\u0026#34;: 4, \u0026#34;小栗帽\u0026#34;: 5, \u0026#34;黄金船\u0026#34;: 6, \u0026#34;伏特加\u0026#34;: 7, \u0026#34;大和赤骥\u0026#34;: 8, \u0026#34;大树快车\u0026#34;: 9, \u0026#34;草上飞\u0026#34;: 10, \u0026#34;菱亚马逊\u0026#34;: 11, \u0026#34;目白麦昆\u0026#34;: 12, \u0026#34;神鹰\u0026#34;: 13, \u0026#34;好歌剧\u0026#34;: 14, \u0026#34;成田白仁\u0026#34;: 15, \u0026#34;鲁道夫象征\u0026#34;: 16, \u0026#34;气槽\u0026#34;: 17, \u0026#34;爱丽数码\u0026#34;: 18, \u0026#34;青云天空\u0026#34;: 19, \u0026#34;玉藻十字\u0026#34;: 20, \u0026#34;美妙姿势\u0026#34;: 21, \u0026#34;琵琶晨光\u0026#34;: 22, \u0026#34;重炮\u0026#34;: 23, \u0026#34;曼城茶座\u0026#34;: 24, \u0026#34;美普波旁\u0026#34;: 25, \u0026#34;目白雷恩\u0026#34;: 26, \u0026#34;雪之美人\u0026#34;: 28, \u0026#34;米浴\u0026#34;: 29, \u0026#34;艾尼斯风神\u0026#34;: 30, \u0026#34;爱丽速子\u0026#34;: 31, \u0026#34;爱慕织姬\u0026#34;: 32, \u0026#34;稻荷一\u0026#34;: 33, \u0026#34;胜利奖券\u0026#34;: 34, \u0026#34;空中神宫\u0026#34;: 35, \u0026#34;荣进闪耀\u0026#34;: 36, \u0026#34;真机伶\u0026#34;: 37, \u0026#34;川上公主\u0026#34;: 38, \u0026#34;黄金城市\u0026#34;: 39, \u0026#34;樱花进王\u0026#34;: 40, \u0026#34;采珠\u0026#34;: 41, \u0026#34;新光风\u0026#34;: 42, \u0026#34;东商变革\u0026#34;: 43, \u0026#34;超级小溪\u0026#34;: 44, \u0026#34;醒目飞鹰\u0026#34;: 45, \u0026#34;荒漠英雄\u0026#34;: 46, \u0026#34;东瀛佐敦\u0026#34;: 47, \u0026#34;中山庆典\u0026#34;: 48, \u0026#34;成田大进\u0026#34;: 49, \u0026#34;西野花\u0026#34;: 50, \u0026#34;春乌拉拉\u0026#34;: 51, \u0026#34;青竹回忆\u0026#34;: 52, \u0026#34;待兼福来\u0026#34;: 55, \u0026#34;名将怒涛\u0026#34;: 57, \u0026#34;目白多伯\u0026#34;: 58, \u0026#34;优秀素质\u0026#34;: 59, \u0026#34;帝王光环\u0026#34;: 60, \u0026#34;待兼诗歌剧\u0026#34;: 61, \u0026#34;生野狄杜斯\u0026#34;: 62, \u0026#34;目白善信\u0026#34;: 63, \u0026#34;大拓太阳神\u0026#34;: 64, \u0026#34;双涡轮\u0026#34;: 65, \u0026#34;里见光钻\u0026#34;: 66, \u0026#34;北部玄驹\u0026#34;: 67, \u0026#34;樱花千代王\u0026#34;: 68, \u0026#34;天狼星象征\u0026#34;: 69, \u0026#34;目白阿尔丹\u0026#34;: 70, \u0026#34;八重无敌\u0026#34;: 71, \u0026#34;鹤丸刚志\u0026#34;: 72, \u0026#34;目白光明\u0026#34;: 73, \u0026#34;樱花桂冠\u0026#34;: 74, \u0026#34;成田路\u0026#34;: 75, \u0026#34;也文摄辉\u0026#34;: 76, \u0026#34;真弓快车\u0026#34;: 80, \u0026#34;骏川手纲\u0026#34;: 81, \u0026#34;小林历奇\u0026#34;: 83, \u0026#34;奇锐骏\u0026#34;: 85, \u0026#34;秋川理事长\u0026#34;: 86, \u0026#34;綾地\u0026#34;: 87, \u0026#34;因幡\u0026#34;: 88, \u0026#34;椎葉\u0026#34;: 89, \u0026#34;仮屋\u0026#34;: 90, \u0026#34;戸隠\u0026#34;: 91, \u0026#34;九条裟罗\u0026#34;: 92, \u0026#34;芭芭拉\u0026#34;: 93, \u0026#34;派蒙\u0026#34;: 94, \u0026#34;荒泷一斗\u0026#34;: 96, \u0026#34;早柚\u0026#34;: 97, \u0026#34;香菱\u0026#34;: 98, \u0026#34;神里绫华\u0026#34;: 99, \u0026#34;重云\u0026#34;: 100, \u0026#34;流浪者\u0026#34;: 102, \u0026#34;优菈\u0026#34;: 103, \u0026#34;凝光\u0026#34;: 105, \u0026#34;钟离\u0026#34;: 106, \u0026#34;雷电将军\u0026#34;: 107, \u0026#34;枫原万叶\u0026#34;: 108, \u0026#34;赛诺\u0026#34;: 109, \u0026#34;诺艾尔\u0026#34;: 112, \u0026#34;八重神子\u0026#34;: 113, \u0026#34;凯亚\u0026#34;: 114, \u0026#34;魈\u0026#34;: 115, \u0026#34;托马\u0026#34;: 116, \u0026#34;可莉\u0026#34;: 117, \u0026#34;迪卢克\u0026#34;: 120, \u0026#34;夜兰\u0026#34;: 121, \u0026#34;鹿野院平藏\u0026#34;: 123, \u0026#34;辛焱\u0026#34;: 124, \u0026#34;丽莎\u0026#34;: 125, \u0026#34;云堇\u0026#34;: 126, \u0026#34;坎蒂丝\u0026#34;: 127, \u0026#34;罗莎莉亚\u0026#34;: 128, \u0026#34;北斗\u0026#34;: 129, \u0026#34;珊瑚宫心海\u0026#34;: 132, \u0026#34;烟绯\u0026#34;: 133, \u0026#34;久岐忍\u0026#34;: 136, \u0026#34;宵宫\u0026#34;: 139, \u0026#34;安柏\u0026#34;: 143, \u0026#34;迪奥娜\u0026#34;: 144, \u0026#34;班尼特\u0026#34;: 146, \u0026#34;雷泽\u0026#34;: 147, \u0026#34;阿贝多\u0026#34;: 151, \u0026#34;温迪\u0026#34;: 152, \u0026#34;空\u0026#34;: 153, \u0026#34;神里绫人\u0026#34;: 154, \u0026#34;琴\u0026#34;: 155, \u0026#34;艾尔海森\u0026#34;: 156, \u0026#34;莫娜\u0026#34;: 157, \u0026#34;妮露\u0026#34;: 159, \u0026#34;胡桃\u0026#34;: 160, \u0026#34;甘雨\u0026#34;: 161, \u0026#34;纳西妲\u0026#34;: 162, \u0026#34;刻晴\u0026#34;: 165, \u0026#34;荧\u0026#34;: 169, \u0026#34;埃洛伊\u0026#34;: 179, \u0026#34;柯莱\u0026#34;: 182, \u0026#34;多莉\u0026#34;: 184, \u0026#34;提纳里\u0026#34;: 186, \u0026#34;砂糖\u0026#34;: 188, \u0026#34;行秋\u0026#34;: 190, \u0026#34;奥兹\u0026#34;: 193, \u0026#34;五郎\u0026#34;: 198, \u0026#34;达达利亚\u0026#34;: 202, \u0026#34;七七\u0026#34;: 207, \u0026#34;申鹤\u0026#34;: 217, \u0026#34;莱依拉\u0026#34;: 228, \u0026#34;菲谢尔\u0026#34;: 230 } speed = 1 output_dir = \u0026#34;output\u0026#34; if not os.path.exists(output_dir): os.makedirs(output_dir) for k in speakers: id = speakers[k] print(\u0026#34;Speaker:\u0026#34;,k) sr, audio = vits(\u0026#39;你好,我是玛丽\u0026#39;, 0, torch.tensor([id]), 0.1, 0.668, 1.0/speed) filename = os.path.join(output_dir, \u0026#34;{}.wav\u0026#34;.format(k)) sf.write(filename, audio, samplerate=sr) 运行即可看到如下生成的效果:\n最后来听下效果:\n八重神子 东海帝王 相对来说,其合成的语音还是比较真实的。但是限于原神角色的问题,如果想更为正式的场景,还是需要自己微调。\n","date":1745629993,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748222582,"objectID":"3deb21ba96b36a9940fb1b0d96a110ec","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-vits-for-speech-synthesis/","publishdate":"2025-04-26T09:13:13+08:00","relpermalink":"/posts/how-to-use-vits-for-speech-synthesis/","section":"posts","summary":"如何使用VITS模型进行中文语音合成","tags":["AI"],"title":"使用VITS进行语音合成","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"前言 如果非要说什么的话,那就是用OCR来识别文件内容其实是把问题复杂化的表现,但是应用场景还是有那么一些,比如古籍的扫描后文字的识别,毕竟人工成本还是比较高的。 先来看个自拍的图片:\n可以很清楚看到其中的图片,下面开始编写相关的代码:\nfrom paddleocr import PPStructure\rtable_engine = PPStructure(show_log=True)\rimg_path=\u0026#34;20250422094717.jpg\u0026#34;\rresult = table_engine(img_path)\rfor line in result:\rif line[\u0026#34;type\u0026#34;] == \u0026#34;Table\u0026#34;:\rhtml = line[\u0026#34;res\u0026#34;][\u0026#34;html\u0026#34;] 其中html变量就是识别出来的HTML代码,其效果如下:\n我们为其添加一个像素的边框后可以看到其内容识别的并不全。预计是没有进行预处理的,导致其版面识别就有问题,自然影响后续内容的识别。\n识别截图 接着来看一张从PDF中的截图,这张图片相对来说比较干净,因为只有黑白两种颜色,是很适合OCR进行处理的。\n其效果如下:\n可以看到其成功将表格识别出来了,另外最后一行中10.76%的值漏掉了点号,因此还需要后处理进行校正。\n","date":1745288214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748007479,"objectID":"ee48e8ddccb20b7e35282bfddb7ad5d5","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-paddleocr-recognition-table/","publishdate":"2025-04-22T10:16:54+08:00","relpermalink":"/posts/how-to-use-paddleocr-recognition-table/","section":"posts","summary":"将介绍如何通过PaddleOCR实现表格识别的需求","tags":["AI"],"title":"使用PaddleOCR进行表格识别","type":"posts"},{"authors":[],"categories":["AI实战"],"content":"最近突发奇想实现一个API功能,就是输入一个歌曲,可以自动下载其相应的歌词。话不多说,干就是了。\n虽然可以借助163MusicLyrics这样的开源工具,但是为了更好的嵌入到程序中,于是只好从头造轮子。\n假设找到了1个网站,它提供了相应的歌词,但是没有提供下载。此时我们可以借助AI来帮助把歌词提取出来。这里以陈奕迅的《孤勇者》作为例子,其界面如下:\n下面我们在AI中输入如下一段话:\n从以下网址中提取出歌曲的歌词,格式为LRC: https://www.gecimao.com/geci/316835.html\r其结果如下:\n可以看到AI工具真的一次不差的帮助我们把其歌词提取出来了,而且还是LRC格式的。此时不得不说AI真的强大。这只是AIGC中的一个小小的应用。基于大模型的AI还是可以理解人类的意思进行相关操作的。\n","date":1741745814,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748008879,"objectID":"0b6deba5b8c6061598b99821cb37214a","permalink":"https://zhuzhulang.github.io/blog/posts/how-to-use-ai-extract-lyrics/","publishdate":"2025-03-12T10:16:54+08:00","relpermalink":"/posts/how-to-use-ai-extract-lyrics/","section":"posts","summary":"如何使用AI智能地从网页中提取歌词","tags":["AI"],"title":"使用AI提取歌词","type":"posts"}]