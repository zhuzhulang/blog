<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script type=application/javascript src=https://zhuzhulang.github.io/blog/js/theme-mode.js></script><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/frameworks.min.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/github.min.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/github-style.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/light.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/dark.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/syntax.css><title>从UNet到DeepLab:语义分割网络的架构演进与技术突破 - 码力全开</title>
<link rel=icon type=image/x-icon href=https://zhuzhulang.github.io/blog/images/github-mark.png><meta name=theme-color content="#1e2327"><meta name=description content="如何通过UNet和DeepLab网络架构的演进逐步解决语义分割中出现的问题"><meta name=keywords content='blog,ai,in action'><meta name=robots content="noodp"><link rel=canonical href=https://zhuzhulang.github.io/blog/posts/from-unet-to-deeplab/><meta name=twitter:card content="summary"><meta name=twitter:title content="从UNet到DeepLab:语义分割网络的架构演进与技术突破 - 码力全开"><meta name=twitter:description content="如何通过UNet和DeepLab网络架构的演进逐步解决语义分割中出现的问题"><meta name=twitter:site content="https://zhuzhulang.github.io/blog/"><meta name=twitter:creator content><meta name=twitter:image content="https://zhuzhulang.github.io/blog/"><meta property="og:type" content="article"><meta property="og:title" content="从UNet到DeepLab:语义分割网络的架构演进与技术突破 - 码力全开"><meta property="og:description" content="如何通过UNet和DeepLab网络架构的演进逐步解决语义分割中出现的问题"><meta property="og:url" content="https://zhuzhulang.github.io/blog/posts/from-unet-to-deeplab/"><meta property="og:site_name" content="从UNet到DeepLab:语义分割网络的架构演进与技术突破"><meta property="og:image" content="https://zhuzhulang.github.io/blog/"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2025-06-20 10:58:39 +0800 CST"></head><script type=text/javascript src="//api.wukongtongji.com/c?_=793054730599555072" async></script><body><div style=position:relative><header class="Header js-details-container Details px-3 px-md-4 px-lg-5 flex-wrap flex-md-nowrap open Details--on"><div class="Header-item mobile-none" style=margin-top:-4px;margin-bottom:-4px><a class=Header-link href=https://zhuzhulang.github.io/blog/><img class=octicon height=32 width=32 src=https://zhuzhulang.github.io/blog/images/github-mark-white.png></a></div><div class="Header-item d-md-none"><button class="Header-link btn-link js-details-target" type=button onclick='document.querySelector("#header-search").style.display=document.querySelector("#header-search").style.display=="none"?"block":"none"'><svg height="24" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="24"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button></div><div style=display:none id=header-search class="Header-item Header-item--full flex-column flex-md-row width-full flex-order-2 flex-md-order-none mr-0 mr-md-3 mt-3 mt-md-0 Details-content--hidden-not-important d-md-flex"><div class="Header-search header-search flex-auto js-site-search position-relative flex-self-stretch flex-md-self-auto mb-3 mb-md-0 mr-0 mr-md-3 scoped-search site-scoped-search js-jump-to"><div class=position-relative><form target=_blank id=search-form accept-charset=UTF-8 method=get autocomplete=off><label class="Header-search-label form-control input-sm header-search-wrapper p-0 js-chromeless-input-container header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center"><input type=text class="Header-search-input form-control input-sm header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable" name=q placeholder=Search autocomplete=off></label></form></div></div></div><div class="Header-item Header-item--full flex-justify-center d-md-none position-relative"><a class=Header-link href=https://zhuzhulang.github.io/blog/><img class="octicon octicon-mark-github v-align-middle" height=32 width=32 src=https://zhuzhulang.github.io/blog/images/github-mark-white.png></a></div><div class=Header-item style=margin-right:0><a href=javascript:void(0) class="Header-link no-select" onclick=switchTheme()><svg style="fill:var(--color-profile-color-modes-toggle-moon)" class="no-select" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754c3.05612.0 5.53362-2.47748 5.53362-5.5336C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961 9.95801 1.07727 10.3495.771159 10.6474.99992c1.4679 1.12724 2.4141 2.90007 2.4141 4.89391.0 3.40575-2.7609 6.16667-6.16665 6.16667-2.94151.0-5.40199-2.0595-6.018122-4.81523C.794841 6.87902 1.23668 6.65289 1.55321 6.85451 2.41106 7.40095 3.4296 7.71754 4.52208 7.71754z"/></svg></a></div></header></div><div id=search-result class="container-lg px-3 new-discussion-timeline" style=display:none></div><div class=application-main><div><main><div class="gisthead pagehead bg-gray-light pb-0 pt-3 mb-4"><div class=px-0><div class="mb-3 d-flex px-3 px-md-3 px-lg-5"><div class="flex-auto min-width-0 width-fit mr-3"><div class=d-flex><div class="d-none d-md-block"><a class="avatar mr-2 flex-shrink-0" href=https://zhuzhulang.github.io/blog/><img class=avatar-user src=https://zhuzhulang.github.io/blog/images/avatar.jpg width=32 height=32></a></div><div class="d-flex flex-column"><h1 class="break-word f3 text-normal mb-md-0 mb-1"><span class=author><a href=https://zhuzhulang.github.io/blog/>码力全开</a>
</span><span class=path-divider>/</span>
<strong class="css-truncate css-truncate-target mr-1" style=max-width:410px><a href=https://zhuzhulang.github.io/blog/posts/from-unet-to-deeplab/>从UNet到DeepLab:语义分割网络的架构演进与技术突破</a></strong></h1><div class="note m-0">Created <relative-time datetime="Fri, 20 Jun 2025 10:58:39 +0800" class=no-wrap>Fri, 20 Jun 2025 10:58:39 +0800</relative-time>
<span class=file-info-divider></span>
Modified <relative-time datetime="Fri, 20 Jun 2025 12:51:21 +0800" class=no-wrap>Fri, 20 Jun 2025 12:51:21 +0800</relative-time></div></div></div></div></div></div></div><div class="container-lg px-3 new-discussion-timeline"><div class="repository-content gist-content"><div><div class="js-gist-file-update-container js-task-list-container file-box"><div id=file-pytest class="file my-2"><div id=post-header class="file-header d-flex flex-md-items-center flex-items-start sticky-header" style=z-index:2><div class="file-info d-flex flex-md-items-center flex-items-start flex-order-1 flex-auto"><div class="text-mono f6 flex-auto pr-3 flex-order-2 flex-md-order-1 mt-2 mt-md-0"><summary id=toc-toggle onclick=clickToc() class="btn btn-octicon m-0 mr-2 p-2"><svg aria-hidden="true" viewBox="0 0 16 16" height="16" width="16" class="octicon octicon-list-unordered"><path fill-rule="evenodd" d="M2 4a1 1 0 100-2 1 1 0 000 2zm3.75-1.5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zm0 5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zm0 5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zM3 8A1 1 0 111 8a1 1 0 012 0zm-1 6a1 1 0 100-2 1 1 0 000 2z"/></svg></summary><details-menu class=SelectMenu id=toc-details style="display: none;"><div class="SelectMenu-modal rounded-3 mt-1" style=max-height:340px><div class="SelectMenu-list SelectMenu-list--borderless p-2" style=overscroll-behavior:contain id=toc-list></div></div></details-menu>2205 Words
<span class=file-info-divider></span>
1 min
<span class=file-info-divider></span>
<svg fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" width="16" viewBox="0 0 24 20" height="20"><path d="M12 10c-7 0-11 7-11 7s3.5 7 11 7 11-7 11-7-4-7-11-7z"/><circle r="4" cy="16" cx="12"/></svg>
<span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span></div><div class="file-actions flex-order-2 pt-0"><a class="muted-link mr-3" href=https://zhuzhulang.github.io/blog/tags/ai><svg class="octicon octicon-tag" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M2.5 7.775V2.75a.25.25.0 01.25-.25h5.025a.25.25.0 01.177.073l6.25 6.25a.25.25.0 010 .354l-5.025 5.025a.25.25.0 01-.354.0l-6.25-6.25A.25.25.0 012.5 7.775zm-1.5.0V2.75C1 1.784 1.784 1 2.75 1h5.025c.464.0.91.184 1.238.513l6.25 6.25a1.75 1.75.0 010 2.474l-5.026 5.026a1.75 1.75.0 01-2.474.0l-6.25-6.25A1.75 1.75.0 011 7.775zM6 5a1 1 0 100 2 1 1 0 000-2z"/></svg>
AI</a></div></div></div><article class="markdown-body entry-content container-lg"><aside class=toc-wrapper><nav id=TableOfContents><ol><li><ol><li><a href=#简述>简述</a></li><li><a href=#unet网络框架>UNet网络框架</a><ol><li><a href=#u-net>U-Net</a></li><li><a href=#unet>UNet++</a></li><li><a href=#unet-1>UNet+++</a></li></ol></li><li><a href=#deeplab网络框架>DeepLab网络框架</a><ol><li><a href=#deeplab-v1>DeepLab v1</a></li><li><a href=#deeplab-v2>DeepLab v2</a></li><li><a href=#deeplab-v3>DeepLab v3</a></li><li><a href=#deeplab-v3-1>DeepLab v3+</a></li></ol></li><li><a href=#代码实现>代码实现</a></li></ol></li></ol></nav></aside><div class="Box-body px-5 pb-5" style=z-index:1><h3 id=简述>简述</h3><p>在正式介绍UNet与DeepLab网络架构之前,先对其整个过程进行简单的梳理。</p><table><thead><tr><th>网络架构</th><th>发布时间</th><th>初稿标题</th></tr></thead><tbody><tr><td>U-Net</td><td>2015.5</td><td><a href=https://arxiv.org/abs/1505.04597>U-Net: Convolutional Networks for Biomedical Image Segmentation</a></td></tr><tr><td>UNet++</td><td>2019.12</td><td><a href=https://arxiv.org/abs/1912.05074>UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation</a></td></tr><tr><td>UNet+++</td><td>2020.4</td><td><a href=https://arxiv.org/abs/2004.08790>UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation</a></td></tr><tr><td>DeepLab v1</td><td>2014.12</td><td><a href=https://arxiv.org/abs/1412.7062v3>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></td></tr><tr><td>DeepLab v2</td><td>2016.6</td><td><a href=https://arxiv.org/abs/1606.00915>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></td></tr><tr><td>DeepLab v3</td><td>2017.6</td><td><a href=https://arxiv.org/abs/1706.05587>Rethinking Atrous Convolution for Semantic Image Segmentation</a></td></tr><tr><td>DeepLab v3+</td><td>2018.2</td><td><a href=https://arxiv.org/abs/1802.02611>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></td></tr></tbody></table><p>需要注意的是,U-Net、UNet++、UNet+++分别由3个不同团队提出的,是基于前者网络结构上的改进。相比而言,DeepLab系列是Google核心团队主导的统一迭代演进模型。</p><p>另外UNet网络系列是纯正的Encoder-Decoder结构,而DeepLab v1-v3仅有Encoder,直到DeepLab v3+时才引入Decoder。</p><h3 id=unet网络框架>UNet网络框架</h3><h4 id=u-net>U-Net</h4><p>U-Net最初通过卷积网络解决医学中图像分割问题,由于该网络结构非常简单而得到广泛的应用。其通过基础卷积层+对称编解码+跳跃连接的设计,解决了医学图像分割中三个核心问题:</p><ol><li>小样本训练: 依靠数据增强弥补训练数据不足的问题</li><li>器官/病变边界模糊: 利用跳跃连接skip-connection融合多尺度特征,保留细节信息</li><li>计算资源受限: 无冗余设计,整个网络没有使用全连接层、批量归一化(BN)或Dropout等复杂技术,从而减少了参数量。</li></ol><p>其网络架构如下图所示:</p><p><img src=https://zhuzhulang.github.io/blog/img/20250620104113.jpg alt=image></p><p>从上图可以看出,使用的仅是3x3卷积与ReLU的基础块,而下采样通过2x2最大池化来实现,上采样通过2x2转置卷积或插值法进行实现。最后输出层使用1x1卷积将通道数映射为类别数。</p><h4 id=unet>UNet++</h4><p>U-Net虽然网络结构非常简单,但是存在如下一些问题:</p><ul><li>直接使用skip-connection将编码器的浅层特征与解码器深层特征进行拼接,而两者语义信息存在显著差异,从而导致直接融合效果受限</li><li>网络需要预先设定固定深度,但不同任务(如不同尺寸的病变分割)所需的最优深度不同,从而限制了模型的适应性</li></ul><p>而UNet++通过如下改进设计解决上述问题:</p><ul><li>嵌套密集跳跃连接:通过在编码器与解码器之间引入多级子网络,形成稠密连接路径。而每一层解码器接收所有同尺度及更浅层的编码器特征,从而实现渐进式特征融合,减少语义差异并增强尺度信息传递</li><li>自适应网络剪枝:可在推理阶段移除部分子网络分支,从而平衡精度与速度的要求</li></ul><p>其网络结构如下图所示:</p><p><img src=https://zhuzhulang.github.io/blog/img/202506-20111126.jpg alt=image></p><p>在上图中,每个节点代表一个卷积块,向下的箭头代表下采样,而向上的箭头代表上采样,点状箭头代表跳跃连接(skip connections)。
其借鉴了DenseNet密集连接的思想,在Encoder和Decoder之间跳跃路径上,跳跃路径上的每一个节点都接收前面所有更浅层节点的特征图,并将其拼接起来作为输入。</p><p>而在推理时可以根据业务需求进行剪枝:</p><p><img src=https://zhuzhulang.github.io/blog/img/20250620111839.jpg alt=image></p><h4 id=unet-1>UNet+++</h4><p>相比UNet++仅嵌套融合同层级特征,导致跨尺度信息交互不足的问题。在UNet 3+提出了全尺度跳跃连接(Full-Scale Skip Connections)的概念,通过Decoder每层直接融合Encoder所有层级(包括浅、中、深层)的特征图,通过跳跃连接实现跨分辨率特征融合,从而增强小目标检测能力。</p><p>其网络结构如下所示:</p><p><img src=https://zhuzhulang.github.io/blog/img/20250620112248.jpg alt=image></p><h3 id=deeplab网络框架>DeepLab网络框架</h3><p>相比U-Net系列,DeepLab系列引入空洞卷积,在不牺牲分辨率情况下扩大了感受野。本质上DeepLab是FCN(Fully Convolutional Network)架构改进的变体。</p><h4 id=deeplab-v1>DeepLab v1</h4><p>传统CNN网络中存在这样的问题,最后的全连接层会限制输入的尺寸并。而将全连接层FC移除后,将其全部替换为卷积层,从而使得网络可接受任意尺寸输入并可输出同尺寸热力图。</p><p>下面是其网络结构:</p><p><img src=https://zhuzhulang.github.io/blog/img/20250620114805.jpg alt=image></p><p>可以看到输入图片经过深度卷积神经网络后得到对应分数图,再经过双线性插值进行上采样,最后通过全连接CRF层改进其最终输出。</p><h4 id=deeplab-v2>DeepLab v2</h4><p>由于DeepLab v1存在如下多尺度分割问题:</p><ol><li>物体尺寸差异大,由于单一空洞卷积感受野固定,难以覆盖不同尺度目标</li><li>深层特征偏向大物体,丢失小目标。</li></ol><p>因此在DeepLab v2中引入ASPP(Atrous Spatial Pyramid Pooling)模块,通过并行使用不同空洞率(如rates=6,12,18,24)的卷积层捕捉多尺度上下文信息。如下图所示:</p><p><img src=https://zhuzhulang.github.io/blog/img/20250620121424.jpg alt=image></p><p>另外DeepLab v2中将主干网络由VGG16升级为ResNet101,提升高层语义表达能力并支持更高效训练。</p><h4 id=deeplab-v3>DeepLab v3</h4><p>由于DeepLab v2继承之前版本的CRF模块从而造成计算瓶颈,而在DeepLab v3中移除了该模块从而实现纯卷积架构。</p><p>另外由于ASPP不同空洞率分支感受野重叠严重问题,在v3中引入1x1卷积和全局平均池化分支,通过补充局部与全局上下文信息,缓解空洞卷积边界栅格效应。</p><p><img src=https://zhuzhulang.github.io/blog/img/20250620122442.jpg alt=image></p><h4 id=deeplab-v3-1>DeepLab v3+</h4><p>DeepLab v3+为了解决v3中直接8倍或16倍采样导致边界模糊导致的物体轮廓断裂、小目标分割精度低的问题,新增了轻量级Decoder模块。</p><p><img src=https://zhuzhulang.github.io/blog/img/20250620123339.jpg alt=image></p><p>另外将主干网络从ResNet升级为Xception。</p><p>可以看到整个DeepLab结构变动并不是很大。</p><h3 id=代码实现>代码实现</h3><p>最后说完理论上网络结构的变化,下面提供相应代码的实现:</p><ul><li>DeepLab官方基于TensorFlow的实现<a href=https://github.com/tensorflow/models/tree/master/research/deeplab>deeplab</a></li><li>DeepLab v3的PyTorch实现<a href=https://github.com/VainF/DeepLabV3Plus-Pytorch>DeepLabV3Plus-Pytorch</a>或直接使用TorchVision中的模型</li><li>U-Net的PyTorch实现<a href=github.com/fepegar/unet/>unet</a></li><li>UNet++的实现<a href=https://github.com/4uiiurz1/pytorch-nested-unet>pytorch-nested-unet</a>及<a href=https://smp.readthedocs.io/en/docs/install.html>segmentation-models-pytorch</a></li><li>UNet++的PyTorch实现<a href=https://github.com/russel0719/UNet-3-Plus-Pytorch>UNet-3-Plus-Pytorch</a></li></ul><div class=reward><span>如果喜欢这篇文章或对您有帮助，可以：[☕] 请我喝杯咖啡 | [💓] 小额赞助</span><br><img src=https://zhuzhulang.github.io/blog/img/qr_alipay.jpg>
<img src=https://zhuzhulang.github.io/blog/img/qr_wechat.png></div></article></div></div></div></div></div></div></main></div><script type=application/javascript src=https://zhuzhulang.github.io/blog/js/toc.js></script><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/toc.css></div><div class="footer container-xl width-full p-responsive"><div class="position-relative d-flex flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between flex-sm-items-center pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light"><a aria-label=Homepage title=GitHub class="footer-octicon d-none d-lg-block mr-lg-4" href=https://zhuzhulang.github.io/blog/><svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" width="24"><path fill-rule="evenodd" d="M8 0C3.58.0.0 3.58.0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38.0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95.0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12.0.0.67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15.0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48.0 1.07-.01 1.93-.01 2.2.0.21.15.46.55.38A8.013 8.013.0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a><ul class="list-style-none d-flex flex-wrap col-12 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0"><li class="mr-3 mr-lg-0">✨ 原创不易，转载请注明作者和链接哦~</li><li class="mr-3 mr-lg-0">GitHub and the Invertocat logo are trademarks of <a href=https://github.com/>GitHub, Inc.</a></li></ul></div><div class="d-flex flex-justify-center pb-6"><span class="f6 text-gray-light"></span></div></div></body><script type=application/javascript src=https://zhuzhulang.github.io/blog/js/github-style.js></script><script src=https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js></script><script type=application/javascript src=https://zhuzhulang.github.io/blog/js/search.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></html>