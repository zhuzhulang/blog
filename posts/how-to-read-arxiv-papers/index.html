<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script type=application/javascript src=https://zhuzhulang.github.io/blog/js/theme-mode.js></script><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/frameworks.min.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/github.min.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/github-style.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/light.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/dark.css><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/syntax.css><title>如何阅读arXiv的初稿 - 码力全开</title>
<link rel=icon type=image/x-icon href=https://zhuzhulang.github.io/blog/images/github-mark.png><meta name=theme-color content="#1e2327"><meta name=description content="介绍一些阅读arXiv中paper的方法"><meta name=keywords content='blog,ai,in action'><meta name=robots content="noodp"><link rel=canonical href=https://zhuzhulang.github.io/blog/posts/how-to-read-arxiv-papers/><meta name=twitter:card content="summary"><meta name=twitter:title content="如何阅读arXiv的初稿 - 码力全开"><meta name=twitter:description content="介绍一些阅读arXiv中paper的方法"><meta name=twitter:site content="https://zhuzhulang.github.io/blog/"><meta name=twitter:creator content><meta name=twitter:image content="https://zhuzhulang.github.io/blog/"><meta property="og:type" content="article"><meta property="og:title" content="如何阅读arXiv的初稿 - 码力全开"><meta property="og:description" content="介绍一些阅读arXiv中paper的方法"><meta property="og:url" content="https://zhuzhulang.github.io/blog/posts/how-to-read-arxiv-papers/"><meta property="og:site_name" content="如何阅读arXiv的初稿"><meta property="og:image" content="https://zhuzhulang.github.io/blog/"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2025-05-26 12:09:05 +0800 CST"></head><body><div style=position:relative><header class="Header js-details-container Details px-3 px-md-4 px-lg-5 flex-wrap flex-md-nowrap open Details--on"><div class="Header-item mobile-none" style=margin-top:-4px;margin-bottom:-4px><a class=Header-link href=https://zhuzhulang.github.io/blog/><img class=octicon height=32 width=32 src=https://zhuzhulang.github.io/blog/images/github-mark-white.png></a></div><div class="Header-item d-md-none"><button class="Header-link btn-link js-details-target" type=button onclick='document.querySelector("#header-search").style.display=document.querySelector("#header-search").style.display=="none"?"block":"none"'><svg height="24" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="24"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button></div><div style=display:none id=header-search class="Header-item Header-item--full flex-column flex-md-row width-full flex-order-2 flex-md-order-none mr-0 mr-md-3 mt-3 mt-md-0 Details-content--hidden-not-important d-md-flex"><div class="Header-search header-search flex-auto js-site-search position-relative flex-self-stretch flex-md-self-auto mb-3 mb-md-0 mr-0 mr-md-3 scoped-search site-scoped-search js-jump-to"><div class=position-relative><form target=_blank id=search-form accept-charset=UTF-8 method=get autocomplete=off><label class="Header-search-label form-control input-sm header-search-wrapper p-0 js-chromeless-input-container header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center"><input type=text class="Header-search-input form-control input-sm header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable" name=q placeholder=Search autocomplete=off></label></form></div></div></div><div class="Header-item Header-item--full flex-justify-center d-md-none position-relative"><a class=Header-link href=https://zhuzhulang.github.io/blog/><img class="octicon octicon-mark-github v-align-middle" height=32 width=32 src=https://zhuzhulang.github.io/blog/images/github-mark-white.png></a></div><div class=Header-item style=margin-right:0><a href=javascript:void(0) class="Header-link no-select" onclick=switchTheme()><svg style="fill:var(--color-profile-color-modes-toggle-moon)" class="no-select" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754c3.05612.0 5.53362-2.47748 5.53362-5.5336C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961 9.95801 1.07727 10.3495.771159 10.6474.99992c1.4679 1.12724 2.4141 2.90007 2.4141 4.89391.0 3.40575-2.7609 6.16667-6.16665 6.16667-2.94151.0-5.40199-2.0595-6.018122-4.81523C.794841 6.87902 1.23668 6.65289 1.55321 6.85451 2.41106 7.40095 3.4296 7.71754 4.52208 7.71754z"/></svg></a></div></header></div><div id=search-result class="container-lg px-3 new-discussion-timeline" style=display:none></div><div class=application-main><div><main><div class="gisthead pagehead bg-gray-light pb-0 pt-3 mb-4"><div class=px-0><div class="mb-3 d-flex px-3 px-md-3 px-lg-5"><div class="flex-auto min-width-0 width-fit mr-3"><div class=d-flex><div class="d-none d-md-block"><a class="avatar mr-2 flex-shrink-0" href=https://zhuzhulang.github.io/blog/><img class=avatar-user src=https://zhuzhulang.github.io/blog/images/avatar.jpg width=32 height=32></a></div><div class="d-flex flex-column"><h1 class="break-word f3 text-normal mb-md-0 mb-1"><span class=author><a href=https://zhuzhulang.github.io/blog/>码力全开</a>
</span><span class=path-divider>/</span>
<strong class="css-truncate css-truncate-target mr-1" style=max-width:410px><a href=https://zhuzhulang.github.io/blog/posts/how-to-read-arxiv-papers/>如何阅读arXiv的初稿</a></strong></h1><div class="note m-0">Created <relative-time datetime="Mon, 26 May 2025 12:09:05 +0800" class=no-wrap>Mon, 26 May 2025 12:09:05 +0800</relative-time>
<span class=file-info-divider></span>
Modified <relative-time datetime="Mon, 26 May 2025 15:51:22 +0800" class=no-wrap>Mon, 26 May 2025 15:51:22 +0800</relative-time></div></div></div></div></div></div></div><div class="container-lg px-3 new-discussion-timeline"><div class="repository-content gist-content"><div><div class="js-gist-file-update-container js-task-list-container file-box"><div id=file-pytest class="file my-2"><div id=post-header class="file-header d-flex flex-md-items-center flex-items-start sticky-header" style=z-index:2><div class="file-info d-flex flex-md-items-center flex-items-start flex-order-1 flex-auto"><div class="text-mono f6 flex-auto pr-3 flex-order-2 flex-md-order-1 mt-2 mt-md-0"><summary id=toc-toggle onclick=clickToc() class="btn btn-octicon m-0 mr-2 p-2"><svg aria-hidden="true" viewBox="0 0 16 16" height="16" width="16" class="octicon octicon-list-unordered"><path fill-rule="evenodd" d="M2 4a1 1 0 100-2 1 1 0 000 2zm3.75-1.5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zm0 5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zm0 5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zM3 8A1 1 0 111 8a1 1 0 012 0zm-1 6a1 1 0 100-2 1 1 0 000 2z"/></svg></summary><details-menu class=SelectMenu id=toc-details style="display: none;"><div class="SelectMenu-modal rounded-3 mt-1" style=max-height:340px><div class="SelectMenu-list SelectMenu-list--borderless p-2" style=overscroll-behavior:contain id=toc-list></div></div></details-menu>1871 Words
<span class=file-info-divider></span>
1 min
<span class=file-info-divider></span>&nbsp;&nbsp;<span id=busuanzi_container_page_pv>Total Page Views: <span id=busuanzi_value_page_pv></span></span></div><div class="file-actions flex-order-2 pt-0"><a class="muted-link mr-3" href=https://zhuzhulang.github.io/blog/tags/other><svg class="octicon octicon-tag" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M2.5 7.775V2.75a.25.25.0 01.25-.25h5.025a.25.25.0 01.177.073l6.25 6.25a.25.25.0 010 .354l-5.025 5.025a.25.25.0 01-.354.0l-6.25-6.25A.25.25.0 012.5 7.775zm-1.5.0V2.75C1 1.784 1.784 1 2.75 1h5.025c.464.0.91.184 1.238.513l6.25 6.25a1.75 1.75.0 010 2.474l-5.026 5.026a1.75 1.75.0 01-2.474.0l-6.25-6.25A1.75 1.75.0 011 7.775zM6 5a1 1 0 100 2 1 1 0 000-2z"/></svg>
other</a></div></div></div><article class="markdown-body entry-content container-lg"><aside class=toc-wrapper><nav id=TableOfContents></nav></aside><div class="Box-body px-5 pb-5" style=z-index:1><p>实际上arXiv平台上论文只是初稿,并没有经过同行的评审,因此其可信度需要结合实验复现、论文引用量及作者团队背景综合判断其可信度。不过这种深度一般需要研究生学历才会接触到,但实际上本科生稍加学习也是可以应对的。</p><p>一般来说,这些论文单词量并不会很大,CET-4基本可以应付。重要的是信心,不要胆怯,而且论文中很多想法实际上你也是可以想到的。在阅读论文时,要带着问题,即本次你想从这篇论文中学习到什么。比如想看下它说的是什么,或者想看下它的网络结构是怎样设计的。每次阅读一部分,逐次蚕食吸收,自然有所成就。
一般而言,每篇论文开头都会介绍相关的背景,比如之前有什么类似的技术,而这些技术得到了怎样的效果或有什么缺点。之后就开始进入自己内容的介绍,介绍的想法、网络结构,有什么优势,解决怎样的问题。最后自然是使用的数据集、训练参数、训练结果的展现以及细节的叙述。就比如这段内容:</p><pre>
TTS is a typical one-to-many mapping problem, since multiple possible speech sequences can correspond to a text sequence due to different variations in speech audio, such as pitch, duration,sound volume and prosody. In autoregressive TTS, the decoder can condition on the text sequence and the previous mel-spectrograms to predict next mel-spectrograms, where the previous mel-spectrograms can provide some variation information and thus alleviate this problem to a certain degree. While in non-autoregressive TTS, the only input information is text which is not enough to fully predict the variance in speech. In this case, the model is prone to overfit to the variations of the target speech in the training set, resulting in poor generalization ability.
</pre><p>自己的翻译是:</p><pre>
TTS是典型的一对多映射问题,因为一个文本序列由于不同变化(音高、时长、音量和韵律)可以对应多种可能语音序列。在自回归TTS中,解码器可以以文本序列作为条件,通过之前的梅尔频谱预测下一个梅尔频谱,其中之前的梅尔频谱可以提供一些变体信息从而在一定程序上缓解这一问题。而在非自回归TTS中,输入的信息只有文本,它是不足以完全预测语音中的变化。在这种情况下,模型容易拟合训练集目标语音的变化,结构泛化较差。
</pre><p>而DeepSeek的翻译如下:</p><pre>
文本到语音（TTS）是一个典型的一对多映射问题。由于语音音频中存在音高、时长、音量和韵律等多种可变因素，同一文本序列可能对应多个不同的语音序列。在自回归TTS模型中，解码器可以通过文本序列和前一时间步的梅尔频谱图来预测后续频谱，其中历史梅尔频谱提供了部分变化信息，从而在一定程度上缓解了这一问题。而在非自回归TTS模型中，仅有文本输入信息不足以完整预测语音的所有动态变化。这种情况下，模型容易过度拟合训练数据中的语音变化特征，导致泛化能力显著下降。
</pre><p>阅读论文没必要做到100%都翻译正确,掌握大概的意思即可。毕竟语言之间的鸿沟,在深度神经网络盛行的今天,已经区别不是那么大了。如果可以话,可以借助一些翻译工具。</p><p>因此有一些插件可以同步翻译,如<a href=https://immersivetranslate.com/>沉浸式翻译</a>及<a href="https://fanyi.youdao.com/trans/?keyfrom=academic#/home">有道翻译</a>。</p><p>这里以FastSpeech作为例子进行说明,其paper分别为:</p><ul><li><a href=https://arxiv.org/abs/1905.09263>FastSpeech: Fast, Robust and Controllable Text to Speech</a></li><li><a href=https://arxiv.org/abs/2006.04558v8>FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a></li></ul><p>首先从其编号<code>1905</code>和<code>2006</code>可以看出这两篇文章分别发布在19年5月和20年6月。从其标题可以看出,FastSpeech是一种快速、鲁棒和可控的文本到语音合成。而FastSpeech2是快速高质量端到端文本到语音合成。</p><p>其中FastSpeech有5个版本,而FastSpeech2有8个版本,一般我们选择最新版本进行阅读即可。在这里主要是为了获取其网络结构,然后编写代码进行实现。</p><p>首先对比其网络结构,下面是FastSpeech的结构:</p><p><img src=https://zhuzhulang.github.io/blog/img/7294323231-122611.jpg alt=FastSpeech></p><p>之后是FastSpeech2的结构:</p><p><img src=https://zhuzhulang.github.io/blog/img/74342646826-122634.jpg alt=FastSpeech2></p><p>在FastSpeech中可以看到音素(Phoneme)经过音素嵌入层,再经过位置编码进入N层的前馈变换器块。之后经过长度调节器(Length Regulator)后,与位置编码一起再经过N层的前馈变换器块(FFT Block)后,经过线性层输出梅尔频谱图。</p><p>而FastSpeech2中的变化是将第1个FFT Block修改为编码器(Encoder),经过方差适配器(Variance Adaptor),叠加位置编码后输出到梅尔频谱解码器(Mel-spectrogram Decoder)和声波解码器(Waveform Decoder)中。</p><p>对于代码的复现,可以参考<a href=https://paperswithcode.com/>Paperswithcode</a>,我们可以搜索相关的paper,查看其是否有一些代码、数据集及结果的测评。</p><p><img src=https://zhuzhulang.github.io/blog/img/271827182_122327.jpg alt=img></p><p>从上图可以看到,FastSpeech有多个实现,根据框架不同选择对应的实现。而数据集只有LJSpeech。</p><p>在FastSpeech2的实现<a href=https://github.com/ming024/FastSpeech2/blob/master/model/fastspeech2.py>ming024/FastSpeech2</a>中有如下的代码:</p><pre tabindex=0><code>
class FastSpeech2(nn.Module):
    &#34;&#34;&#34; FastSpeech2 &#34;&#34;&#34;

    def __init__(self, preprocess_config, model_config):
        super(FastSpeech2, self).__init__()
        self.model_config = model_config

        self.encoder = Encoder(model_config)
        self.variance_adaptor = VarianceAdaptor(preprocess_config, model_config)
        self.decoder = Decoder(model_config)
        self.mel_linear = nn.Linear(
            model_config[&#34;transformer&#34;][&#34;decoder_hidden&#34;],
            preprocess_config[&#34;preprocessing&#34;][&#34;mel&#34;][&#34;n_mel_channels&#34;],
        )
        self.postnet = PostNet()
</code></pre><p>正好对应之前的网络结构。</p><p>总体而言,对于论文的阅读,需要对基础有一定的了解,能够举一反三。论文中可能会提出一些很新颖的名词,而实际上就是你熟悉的某些方法。不要被他吓到了。</p><div class=reward><span>如果喜欢这篇文章或对您有帮助，可以：[☕] 请我喝杯咖啡 | [💓] 小额赞助</span><br><img src=https://zhuzhulang.github.io/blog/img/qr_alipay.jpg>
<img src=https://zhuzhulang.github.io/blog/img/qr_wechat.png></div></article></div></div></div></div></div></div></main></div><script type=application/javascript src=https://zhuzhulang.github.io/blog/js/toc.js></script><link rel=stylesheet href=https://zhuzhulang.github.io/blog/css/toc.css></div><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><div class="footer container-xl width-full p-responsive"><div class="position-relative d-flex flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between flex-sm-items-center pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light"><a aria-label=Homepage title=GitHub class="footer-octicon d-none d-lg-block mr-lg-4" href=https://zhuzhulang.github.io/blog/><svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" width="24"><path fill-rule="evenodd" d="M8 0C3.58.0.0 3.58.0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38.0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95.0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12.0.0.67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15.0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48.0 1.07-.01 1.93-.01 2.2.0.21.15.46.55.38A8.013 8.013.0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a><ul class="list-style-none d-flex flex-wrap col-12 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0"><li class="mr-3 mr-lg-0">✨ 原创不易，转载请注明作者和链接哦~</li><li><span id=busuanzi_container_site_pv>本站总访问量<span id=busuanzi_value_site_pv></span>次</span></li><li class="mr-3 mr-lg-0">GitHub and the Invertocat logo are trademarks of <a href=https://github.com/>GitHub, Inc.</a></li></ul></div><div class="d-flex flex-justify-center pb-6"><span class="f6 text-gray-light"></span></div></div></body><script type=application/javascript src=https://zhuzhulang.github.io/blog/js/github-style.js></script><script src=https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js></script><script type=application/javascript src=https://zhuzhulang.github.io/blog/js/search.js></script></html>