<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on 码力全开</title><link>https://zhuzhulang.github.io/blog/posts/</link><description>Recent content in Posts on 码力全开</description><generator>Hugo</generator><language>zh-cn</language><copyright>✨ 原创不易，转载请注明作者和链接哦~</copyright><lastBuildDate>Wed, 14 Jan 2026 21:28:52 +0800</lastBuildDate><atom:link href="https://zhuzhulang.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Llama3.2视觉模型</title><link>https://zhuzhulang.github.io/blog/posts/llama3.2-vision/</link><pubDate>Wed, 14 Jan 2026 11:26:56 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/llama3.2-vision/</guid><description>对Llama3.2视觉模型进行简单的介绍</description></item><item><title>LLaVA网络简述</title><link>https://zhuzhulang.github.io/blog/posts/llava-network/</link><pubDate>Wed, 14 Jan 2026 09:18:59 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/llava-network/</guid><description>对LLaVA模型进行简单的介绍</description></item><item><title>BLIP网络简述</title><link>https://zhuzhulang.github.io/blog/posts/blip-network/</link><pubDate>Tue, 13 Jan 2026 15:28:43 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/blip-network/</guid><description>对BLIP网络进行简单的介绍</description></item><item><title>ALBEF网络简述</title><link>https://zhuzhulang.github.io/blog/posts/albef-network/</link><pubDate>Mon, 12 Jan 2026 21:30:04 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/albef-network/</guid><description>对ALBEF网络进行简单的介绍</description></item><item><title>视觉Transformer网络ViT简述</title><link>https://zhuzhulang.github.io/blog/posts/vision-transformer/</link><pubDate>Mon, 12 Jan 2026 15:59:06 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/vision-transformer/</guid><description>&lt;p>Vision Transformer是用Transformer架构解决视觉问题。其可以在不做改动来解决计算机视觉问题。&lt;/p>
&lt;p>其在小规模数据上略输卷积神经网络,而在中等或者大规模数据集上,表现相当或者优于卷积神经网络。在计算效率上,训练同等精度的模型,Transformer模型比卷积神经网络模型更有优势。&lt;/p>
&lt;p>首先将图片按照固定大小分为一个个patch作为token。一个patch如何作为Embedding向量呢?&lt;/p>
&lt;p>可以将这个图像的长乘以宽再乘以通道数的多维矩阵表示展平,再通过一个共享线性层投射到Transformer模型特征维度就完成1个图片转换为向量序列。&lt;/p>
&lt;p>图片切片相当于文本中分词,而线性层相当于Embedding。接下来需要考虑位置编码。&lt;/p>
&lt;p>在ViT中把位置编码为可学习向量,直接加在图像Token向量上。其参考BERT模型,在前面添加1个可学习的分类头。后面用的是Transformer Encoder,因此每个Token都可以看到所有Token。&lt;/p>
&lt;p>其网络结构如下图所示:&lt;/p>
&lt;p>&lt;img src="https://i-blog.csdnimg.cn/blog_migrate/03a4604f09cc96daed1fc66cd768c9ea.png" alt="ViT" />&lt;/p>
&lt;p>其模型类型可分为3种:&lt;/p>
&lt;ul>
&lt;li>ViT-Base&lt;/li>
&lt;li>ViT-Large&lt;/li>
&lt;li>ViT-Huge&lt;/li>
&lt;/ul>
&lt;p>其中ViT-L/16表示使用的是ViT-Large模型,patch尺寸为16x16。其中patch size越小,序列越长,计算代价越大。&lt;/p>
&lt;p>下面介绍下图像转化为Embedding序列的两种实现方式:&lt;/p>
&lt;p>训练图片大小为224x224,patch大小为16x16,patch数量为14x14。而Transformer里的特征维度(Hidden Size)为1024。&lt;/p>
&lt;p>第一种方式是采用线性映射,将原始图片拆分为多个patch,对于每个patch,shape为(16,16,3),展开为1个长度为768的一维向量,然后通过一个共享的(768,1024)的线性层进行编码。&lt;/p>
&lt;p>第二种方式是卷积操作,直接对原始图片,定义1024个卷积核,每个卷积核大小为patch大小(16,16),步长为16,padding为valid。&lt;/p>
&lt;p>这两种操作是完全等价的。&lt;/p>
&lt;p>另外Encoder输出分类也有两种方法:&lt;/p>
&lt;ol>
&lt;li>增加一个分类头[cls]token,用最后的Encoder对这个token的输出提取全局信息&lt;/li>
&lt;li>不增加token,用最后Encoder的所有token的向量的全局平均池化GAP提取全局信息&lt;/li>
&lt;/ol>
&lt;p>最后,原作者也在该网络上尝试自监督学习。其借鉴BERT,对50%的patch进行标记,其中的80%标记为可学习的[mask]标签,10%替换为其他patch的Embedding,剩余10%维持不变。&lt;/p>
&lt;p>而预测像素值时,原来是RGB数量255x255x255=16581375色转换为预测8x8x8=256色,效果非常不错。&lt;/p>
&lt;p>参考视频:&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV15RDtYqE4r">https://www.bilibili.com/video/BV15RDtYqE4r&lt;/a>&lt;/p>&lt;/blockquote></description></item><item><title>MoCo网络简述</title><link>https://zhuzhulang.github.io/blog/posts/moco-network/</link><pubDate>Mon, 12 Jan 2026 11:03:49 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/moco-network/</guid><description>对MoCo网络进行简单的介绍</description></item><item><title>CLIP网络简述</title><link>https://zhuzhulang.github.io/blog/posts/clip-network/</link><pubDate>Mon, 12 Jan 2026 10:17:18 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/clip-network/</guid><description>对CLIP网络进行简单的介绍</description></item><item><title>视觉基础模型下游任务迁移</title><link>https://zhuzhulang.github.io/blog/posts/downstream-task-migration/</link><pubDate>Sat, 10 Jan 2026 22:38:37 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/downstream-task-migration/</guid><description>对视觉大模型下游任务迁移进行简单的介绍</description></item><item><title>多模态网络的要素</title><link>https://zhuzhulang.github.io/blog/posts/multimodel-elements/</link><pubDate>Fri, 09 Jan 2026 22:01:49 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/multimodel-elements/</guid><description>对多模态网络的要素进行简单的介绍</description></item><item><title>自监督学习的方法</title><link>https://zhuzhulang.github.io/blog/posts/self-supervised-learning-method/</link><pubDate>Fri, 09 Jan 2026 11:03:26 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/self-supervised-learning-method/</guid><description>对自监督学习的方法进行简单的介绍</description></item><item><title>卷积神经网络特征提取</title><link>https://zhuzhulang.github.io/blog/posts/convolution-network-introduction/</link><pubDate>Wed, 07 Jan 2026 15:07:01 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/convolution-network-introduction/</guid><description>将介绍卷积神经网络是如何从图像中提取特征</description></item><item><title>自动驾驶与传感器融合</title><link>https://zhuzhulang.github.io/blog/posts/autonomous-driving-sensor-fusion/</link><pubDate>Wed, 07 Jan 2026 09:56:24 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/autonomous-driving-sensor-fusion/</guid><description>对自动驾驶中传感器融合进行简单介绍</description></item><item><title>基于YOLO的车速检测</title><link>https://zhuzhulang.github.io/blog/posts/yolo-speed-estimate/</link><pubDate>Sun, 04 Jan 2026 16:07:46 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/yolo-speed-estimate/</guid><description>&lt;p>这里来看一个实例,基于YOLO实现对高速公路的车速检测。&lt;/p>
&lt;p>整个过程可以说非常简单,主要用到YOLO、ByteTrack及OpenCV。通过ByteTrack进行车辆目标跟踪,从而方便获取其位置信息。&lt;/p>
&lt;p>要实现车速计算,最简单的方法自然是根据识别框的像素位置进行估计,但是这种方式存在一种问题,无法从图像的像素值得到其实际物理的距离。为此,需要通过一种映射关系建立与真实世界的联系。&lt;/p>
&lt;p>在OpenCV中提供了getPerspectiveTransform函数,我们可以将图像中的四边形变换为一个矩形,从而建立其关系。因为龙门架的高度、车道线的距离一般是规定不变的,从而借助这些隐含信息作为参考,得到其真实的距离度量。&lt;/p>
&lt;p>问题在于如何得到图像中的坐标点,假设我们知道其坐标点信息,那么就可以建立联系了,从而解决车速检测的问题。&lt;/p>
&lt;p>最后为了避免车速出现一些异常值,可以对一秒钟内的速度取平均值,这样有助于系统更加稳健。&lt;/p>
&lt;p>最后我们还可以使用YOLO制造热力图,将其进行显示,详情可以参考&lt;a href="https://cloud.tencent.com/developer/article/2493342">使用YOLOv8创建交通热力图&lt;/a>。&lt;/p>
&lt;p>参考文章:&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://cloud.tencent.com/developer/article/2389959?policyId=1004">https://cloud.tencent.com/developer/article/2389959?policyId=1004&lt;/a>&lt;/p>&lt;/blockquote></description></item><item><title>大模型并行策略简述</title><link>https://zhuzhulang.github.io/blog/posts/parallelism-strategy-introduction/</link><pubDate>Sat, 03 Jan 2026 21:33:07 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/parallelism-strategy-introduction/</guid><description>&lt;p>这里对大模型并行计算中的策略进行简单的介绍,主要涵盖如下一些策略的内容:&lt;/p>
&lt;ul>
&lt;li>数据并行&lt;/li>
&lt;li>模型并行&lt;/li>
&lt;li>张量并行&lt;/li>
&lt;li>流水线并行&lt;/li>
&lt;/ul>
&lt;p>其中数据并行(DP,Data Parallelism)是最简单的,就是将训练的数据集拆分为大小相同的若干份,模型对不同的数据子集进行训练,在各个分组训练完成后需要进行全局的参数同步。&lt;/p>
&lt;p>而张量并行(TP,Tensor Parallelism)通过将模型中的张量(如权重矩阵)按行或列切分到多个计算设备上,以解决单设备内存不足问题并提升大模型训练效率。其属于模型并行中的一种,核心思想是将单个模型层的参数或计算任务拆分到不同设备上执行,如矩阵乘法拆分到不同的GPU上运行。&lt;/p>
&lt;p>模型并行(Model Parallelism)是通过将模型不同的部分分配给多个设备上进行计算,比如将不同的层分配给不同的设备上,但这种朴素策略存在GPU利用率低的问题。与之类似的是流水线并行(PP,Pipeline Parallelism),通过将模型的不同层按顺序分配到不同设备上的方法。&lt;/p>
&lt;p>流水线并行通过将输入数据切分成多个微批次,使得每个设备可以在处理完当前批次后立即处理下一个批次,从而提高设备利用率。主要策略有Gpipe和PipeDream流水线并行。&lt;/p>
&lt;p>参考文章:&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://developer.aliyun.com/article/1257832">https://developer.aliyun.com/article/1257832&lt;/a>
&lt;a href="https://blog.csdn.net/shizheng_Li/article/details/144138542">https://blog.csdn.net/shizheng_Li/article/details/144138542&lt;/a>
&lt;a href="https://blog.csdn.net/m0_59164520/article/details/142731435">https://blog.csdn.net/m0_59164520/article/details/142731435&lt;/a>
&lt;a href="https://www.cnblogs.com/khronos0206/p/18606724">https://www.cnblogs.com/khronos0206/p/18606724&lt;/a>
&lt;a href="https://zhuanlan.zhihu.com/p/613196255">https://zhuanlan.zhihu.com/p/613196255&lt;/a>&lt;/p>&lt;/blockquote></description></item><item><title>Ring算法简述</title><link>https://zhuzhulang.github.io/blog/posts/ring-algorithm-introduction/</link><pubDate>Sat, 03 Jan 2026 16:01:12 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/ring-algorithm-introduction/</guid><description>对分布式并行中Ring算法进行简单的介绍</description></item><item><title>分布式并行通信原语简述</title><link>https://zhuzhulang.github.io/blog/posts/parallel-primitives-introduction/</link><pubDate>Sat, 03 Jan 2026 09:51:41 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/parallel-primitives-introduction/</guid><description>对大模型分布式并行中各种通信原语进行简单的介绍</description></item><item><title>Switch Transformer简介</title><link>https://zhuzhulang.github.io/blog/posts/switch-transformer-introduction/</link><pubDate>Fri, 02 Jan 2026 21:48:29 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/switch-transformer-introduction/</guid><description>对Switch Transformer进行简单的介绍</description></item><item><title>Transformer各类位置编码技术简述</title><link>https://zhuzhulang.github.io/blog/posts/transformer-position-encoder-introduction/</link><pubDate>Wed, 31 Dec 2025 15:24:15 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/transformer-position-encoder-introduction/</guid><description>对Transformer各类位置编码技术进行简单的叙述</description></item><item><title>MQA与GQA简述</title><link>https://zhuzhulang.github.io/blog/posts/mqa-and-gqa-introduction/</link><pubDate>Wed, 31 Dec 2025 09:44:26 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/mqa-and-gqa-introduction/</guid><description>对MHA、MQA及GQA进行简单的介绍</description></item><item><title>PageAttention简述</title><link>https://zhuzhulang.github.io/blog/posts/pageattention-introduction/</link><pubDate>Tue, 30 Dec 2025 21:07:00 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/pageattention-introduction/</guid><description>关于PageAttention的简单介绍</description></item><item><title>FlashAttention简述</title><link>https://zhuzhulang.github.io/blog/posts/flashattention-introduction/</link><pubDate>Tue, 30 Dec 2025 15:46:57 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/flashattention-introduction/</guid><description>对FlashAttention进行简单的介绍</description></item><item><title>int8量化概述</title><link>https://zhuzhulang.github.io/blog/posts/how-to-int8-quantization/</link><pubDate>Thu, 18 Dec 2025 16:29:26 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-int8-quantization/</guid><description>如何实现int8量化</description></item><item><title>MobileViT简述</title><link>https://zhuzhulang.github.io/blog/posts/mobilevit-introduction/</link><pubDate>Sun, 07 Dec 2025 16:05:33 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/mobilevit-introduction/</guid><description>对MobileViT网络进行简单的介绍</description></item><item><title>YOLO双Backbone架构设计</title><link>https://zhuzhulang.github.io/blog/posts/double-backbone-in-yolo/</link><pubDate>Sun, 07 Dec 2025 10:25:40 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/double-backbone-in-yolo/</guid><description>介绍YOLO中双Backbone架构的相关设计</description></item><item><title>YOLO11中替换Backbone为MobileNetV3-Small</title><link>https://zhuzhulang.github.io/blog/posts/how-to-replace-backbone-to-mobilenetv3-small-in-yolo/</link><pubDate>Thu, 04 Dec 2025 15:37:06 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-replace-backbone-to-mobilenetv3-small-in-yolo/</guid><description>将YOLO 11中的Backbone网络替换为MobileNet V3 Small</description></item><item><title>YOLO11更换Backbone为ShuffleNetV2</title><link>https://zhuzhulang.github.io/blog/posts/how-to-replace-backbone-to-shufflenetv2-in-yolo11/</link><pubDate>Fri, 28 Nov 2025 20:00:51 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-replace-backbone-to-shufflenetv2-in-yolo11/</guid><description>如何替换YOLO11中Backbone为ShuffleNetV2</description></item><item><title>GhostNet网络简述</title><link>https://zhuzhulang.github.io/blog/posts/ghostnet-introduction/</link><pubDate>Thu, 27 Nov 2025 09:31:49 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/ghostnet-introduction/</guid><description>关于GhostNet网络简单的叙述</description></item><item><title>YOLOv5在边缘设备上的优化</title><link>https://zhuzhulang.github.io/blog/posts/yolov5-in-edge-device/</link><pubDate>Sat, 15 Nov 2025 10:22:02 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/yolov5-in-edge-device/</guid><description>如何让YOLOv5可以在边缘设备上较好的运行</description></item><item><title>YOLO性别与年龄估计</title><link>https://zhuzhulang.github.io/blog/posts/use-yolo-for-humman-age-and-gender-estimiation/</link><pubDate>Sat, 08 Nov 2025 11:39:17 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/use-yolo-for-humman-age-and-gender-estimiation/</guid><description>如何使用YOLO模型进行性别和年龄估计</description></item><item><title>在RK3588主板上使用Docker进行推理</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-rk3588-npu-in-docker/</link><pubDate>Thu, 23 Oct 2025 21:19:25 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-rk3588-npu-in-docker/</guid><description>如何在Docker中调用主板上的NPU</description></item><item><title>视频插帧方案简述</title><link>https://zhuzhulang.github.io/blog/posts/something-about-interpolate-frame/</link><pubDate>Sun, 21 Sep 2025 16:34:01 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/something-about-interpolate-frame/</guid><description>关于视频插帧的一些常用方法</description></item><item><title>YOLO添加注意力机制</title><link>https://zhuzhulang.github.io/blog/posts/add-attention-module-in-yolo/</link><pubDate>Sun, 14 Sep 2025 21:16:01 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/add-attention-module-in-yolo/</guid><description>如何为YOLO添加注意力机制</description></item><item><title>YOLOv11网络结构简述</title><link>https://zhuzhulang.github.io/blog/posts/yolov11-architecture-introduction/</link><pubDate>Sun, 07 Sep 2025 09:49:18 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/yolov11-architecture-introduction/</guid><description>对YOLO11网络结构进行简单的介绍</description></item><item><title>YOLO物体跟踪</title><link>https://zhuzhulang.github.io/blog/posts/how-to-track-object-in-yolo/</link><pubDate>Sat, 06 Sep 2025 09:35:32 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-track-object-in-yolo/</guid><description>如何在YOLO中进行物体跟踪,特别是使用onnxruntime</description></item><item><title>MoE简述</title><link>https://zhuzhulang.github.io/blog/posts/introduction-of-moe/</link><pubDate>Fri, 15 Aug 2025 08:57:51 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/introduction-of-moe/</guid><description>关于大模型中MoE的简单学习</description></item><item><title>使用法向量夹角算法提取特征点</title><link>https://zhuzhulang.github.io/blog/posts/pointcloud-extract-feature-point/</link><pubDate>Wed, 06 Aug 2025 15:11:46 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/pointcloud-extract-feature-point/</guid><description>如何利益法向量夹角算法提取点云中的特征点</description></item><item><title>PyTorch分布式数据并行入门</title><link>https://zhuzhulang.github.io/blog/posts/pytorch-distributed-data-parallel/</link><pubDate>Mon, 04 Aug 2025 10:45:50 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/pytorch-distributed-data-parallel/</guid><description>如何使用DDP实现分布式数据并行训练</description></item><item><title>Vue3快速应用工程化</title><link>https://zhuzhulang.github.io/blog/posts/rapid-engineering-application-of-vue/</link><pubDate>Sat, 26 Jul 2025 16:19:13 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/rapid-engineering-application-of-vue/</guid><description>如何使用Vue3快速进行单页面应用工程化</description></item><item><title>Vue3快速入门</title><link>https://zhuzhulang.github.io/blog/posts/getting-started-with-vue3/</link><pubDate>Sat, 26 Jul 2025 08:57:56 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/getting-started-with-vue3/</guid><description>如何快速入门Vue3,并在项目中进行应用</description></item><item><title>预训练大模型内存占用估计</title><link>https://zhuzhulang.github.io/blog/posts/how-to-compute-pretrained-model-parameter-and-memory-usage/</link><pubDate>Tue, 22 Jul 2025 08:30:57 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-compute-pretrained-model-parameter-and-memory-usage/</guid><description>如何对预训练大模型训练占用的内存进行估算</description></item><item><title>Transformer模型新解</title><link>https://zhuzhulang.github.io/blog/posts/a-new-perspective-of-transformer/</link><pubDate>Tue, 15 Jul 2025 22:17:14 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/a-new-perspective-of-transformer/</guid><description>从一种新的视角看Transformer模型</description></item><item><title>使用YOLO-obb检测旋转物体</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-yolo-obb-for-rotate-object/</link><pubDate>Thu, 03 Jul 2025 21:19:34 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-yolo-obb-for-rotate-object/</guid><description>如何使用YOLO模型检测旋转的物体</description></item><item><title>点云图可视化</title><link>https://zhuzhulang.github.io/blog/posts/how-to-visualization-point-cloud/</link><pubDate>Wed, 02 Jul 2025 19:53:38 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-visualization-point-cloud/</guid><description>如何对点云图进行可视化</description></item><item><title>YOLO + DeepSort 解决目标追踪问题</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-deepsort-track-object/</link><pubDate>Tue, 01 Jul 2025 16:56:43 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-deepsort-track-object/</guid><description>通过使用YOLOv5与DeepSort算法解决一般的目标追踪问题</description></item><item><title>YOLOv5训练自定义数据集</title><link>https://zhuzhulang.github.io/blog/posts/how-to-train-custom-dataset-in-yolov5/</link><pubDate>Sun, 29 Jun 2025 22:03:31 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-train-custom-dataset-in-yolov5/</guid><description>如何通过YOLOv5对自定义数据集进行训练,完成项目中一些实战的工作</description></item><item><title>YOLO数据集格式简述</title><link>https://zhuzhulang.github.io/blog/posts/yolo-dataset-format/</link><pubDate>Sat, 28 Jun 2025 16:21:52 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/yolo-dataset-format/</guid><description>对YOLO数据集格式进行简单的介绍,说明其计算过程</description></item><item><title>YOLOv5目标检测代码精解</title><link>https://zhuzhulang.github.io/blog/posts/in-depth-code-explaination-of-yolov5/</link><pubDate>Fri, 27 Jun 2025 16:29:48 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/in-depth-code-explaination-of-yolov5/</guid><description>手把手从YOLOv3代码入手,为你详细讲解其训练整个实现过程</description></item><item><title>YOLOv3目标检测代码精解</title><link>https://zhuzhulang.github.io/blog/posts/in-depth-code-explaination-of-yolov3/</link><pubDate>Fri, 27 Jun 2025 10:26:11 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/in-depth-code-explaination-of-yolov3/</guid><description>手把手从YOLOv3代码入手,为你详细讲解其推理整个实现过程</description></item><item><title>从"一眼万年"到"万物可识"：YOLO系列目标检测技术的进化之路</title><link>https://zhuzhulang.github.io/blog/posts/unfolding-the-evolution-of-yolo-series-in-object-detection/</link><pubDate>Wed, 25 Jun 2025 09:07:10 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/unfolding-the-evolution-of-yolo-series-in-object-detection/</guid><description>对YOLO整个系列进行简单的介绍</description></item><item><title>从UNet到DeepLab:语义分割网络的架构演进与技术突破</title><link>https://zhuzhulang.github.io/blog/posts/from-unet-to-deeplab/</link><pubDate>Fri, 20 Jun 2025 10:58:39 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/from-unet-to-deeplab/</guid><description>如何通过UNet和DeepLab网络架构的演进逐步解决语义分割中出现的问题</description></item><item><title>PostgreSQL + 向量搜索:解锁关系型数据库的AI潜能</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-postgresql-as-vector-database/</link><pubDate>Tue, 17 Jun 2025 16:04:15 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-postgresql-as-vector-database/</guid><description>如何将PostgreSQL摇身一变为向量数据库,并在项目中进行使用</description></item><item><title>Dify二次开发-新增图表类型</title><link>https://zhuzhulang.github.io/blog/posts/how-to-add-new-chart-in-plugins/</link><pubDate>Mon, 16 Jun 2025 07:20:24 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-add-new-chart-in-plugins/</guid><description>如何根据实际需求新增图表</description></item><item><title>Dify二次开发-生成图表</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-dify-generate-graph/</link><pubDate>Sun, 15 Jun 2025 15:09:13 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-dify-generate-graph/</guid><description>如何在Dify中根据数据库中的数据生成图表</description></item><item><title>Dify二次开发-使用MCP协议查询数据库数据</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-mcp-protocol-query-data/</link><pubDate>Fri, 13 Jun 2025 20:27:30 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-mcp-protocol-query-data/</guid><description>如何通过MCP协议对数据库中数据进行查询</description></item><item><title>如何阅读arXiv的初稿</title><link>https://zhuzhulang.github.io/blog/posts/how-to-read-arxiv-papers/</link><pubDate>Mon, 26 May 2025 12:09:05 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-read-arxiv-papers/</guid><description>介绍一些阅读arXiv中paper的方法</description></item><item><title>Dify二次开发-新增工具</title><link>https://zhuzhulang.github.io/blog/posts/how-to-add-custom-tool/</link><pubDate>Sat, 24 May 2025 22:55:27 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-add-custom-tool/</guid><description>在Dify上新增内置工具</description></item><item><title>Dify二次开发-重置管理员邮箱</title><link>https://zhuzhulang.github.io/blog/posts/how-to-reset-email-in-dify/</link><pubDate>Thu, 22 May 2025 15:16:54 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-reset-email-in-dify/</guid><description>如何重置Dify的管理员邮箱</description></item><item><title>Dify二次开发-新增模型提供商</title><link>https://zhuzhulang.github.io/blog/posts/how-to-add-custom-providers/</link><pubDate>Tue, 20 May 2025 15:16:54 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-add-custom-providers/</guid><description>如何在Dify上新增模型提供商</description></item><item><title>Dify二次开发-环境搭建</title><link>https://zhuzhulang.github.io/blog/posts/how-to-build-dify-environment/</link><pubDate>Sat, 17 May 2025 15:16:54 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-build-dify-environment/</guid><description>如何搭建Dify环境</description></item><item><title>使用FastSpeech2进行语音合成</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-fastspeech2-for-speech-synthesis/</link><pubDate>Tue, 29 Apr 2025 09:01:47 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-fastspeech2-for-speech-synthesis/</guid><description>如何使用FastSpeech2的模型实现中文语音合成</description></item><item><title>使用VITS进行语音合成</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-vits-for-speech-synthesis/</link><pubDate>Sat, 26 Apr 2025 09:13:13 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-vits-for-speech-synthesis/</guid><description>如何使用VITS模型进行中文语音合成</description></item><item><title>使用PaddleOCR进行表格识别</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-paddleocr-recognition-table/</link><pubDate>Tue, 22 Apr 2025 10:16:54 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-paddleocr-recognition-table/</guid><description>将介绍如何通过PaddleOCR实现表格识别的需求</description></item><item><title>使用AI提取歌词</title><link>https://zhuzhulang.github.io/blog/posts/how-to-use-ai-extract-lyrics/</link><pubDate>Wed, 12 Mar 2025 10:16:54 +0800</pubDate><guid>https://zhuzhulang.github.io/blog/posts/how-to-use-ai-extract-lyrics/</guid><description>如何使用AI智能地从网页中提取歌词</description></item></channel></rss>